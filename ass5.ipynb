{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet20\n",
      "Total number of params 269722\n",
      "Total layers 20\n",
      "\n",
      "resnet32\n",
      "Total number of params 464154\n",
      "Total layers 32\n",
      "\n",
      "resnet44\n",
      "Total number of params 658586\n",
      "Total layers 44\n",
      "\n",
      "resnet56\n",
      "Total number of params 853018\n",
      "Total layers 56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56']\n",
    " \n",
    "def _weights_init(m):\n",
    "    \"\"\"\n",
    "        Initialization of CNN weights\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
    "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
    "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
    "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 experiment, ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
    "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
    "# The subsampling is performed by convolutions with a stride of 2.\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for net_name in __all__:\n",
    "        if net_name.startswith('resnet'):\n",
    "            print(net_name)\n",
    "            test(globals()[net_name]())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNetArgs:\n",
    "       \"\"\"Passing the hyperparameters to the model\"\"\"\n",
    "       def __init__(self, arch='resnet20' ,epochs=50, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
    "                 evaluate=0, pretrained=0, half=0, save_dir='save_temp', save_every=10):\n",
    "          \n",
    "          self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "          self.save_dir = save_dir #The directory used to save the trained models\n",
    "          self.half = half #use half-precision(16-bit)\n",
    "          self.evaluate = evaluate #evaluate model on the validation set\n",
    "          self.pretrained = pretrained #evaluate the pretrained model on the validation set\n",
    "          self.print_freq = print_freq #print frequency \n",
    "          self.weight_decay = weight_decay\n",
    "          self.momentum = momentum \n",
    "          self.lr = lr #Learning rate\n",
    "          self.batch_size = batch_size \n",
    "          self.start_epoch = start_epoch\n",
    "          self.epochs = epochs\n",
    "          self.arch = arch #ResNet model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-18                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-20            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-23                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-25             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-28                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-30             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-7                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-33                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-35            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-8                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-38                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-40             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-9                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-43                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-45             [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 269,722\n",
      "Trainable params: 269,722\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 41.09\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.88\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 3.92\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet20',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-28                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-30            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-33                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-35             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-38                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-40             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-43                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-45             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-50             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-11                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-53                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-55            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-12                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-58                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-60             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-13                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-63                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-65             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-14                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-68                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-70             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-15                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-73                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-75             [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 464,154\n",
      "Trainable params: 464,154\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 69.79\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.63\n",
      "Params size (MB): 1.77\n",
      "Estimated Total Size (MB): 6.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet32',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        if args.half:\n",
    "            input_var = input_var.half()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            if args.half:\n",
    "                input_var = input_var.half()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.th'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           frog             cat            bird            bird           horse            ship             car           truck\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 179,
       "width": 1150
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=128, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=128, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "plt.figure(figsize=(20,10)) \n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
    "# print labels\n",
    "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    \n",
    "    # Check the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model =  globals()[args.arch]().to(device)\n",
    "    model.cuda()\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    if args.half:\n",
    "        print('half persicion is used.')\n",
    "        model.half()\n",
    "        criterion.half()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    if args.arch in ['resnet1202']:\n",
    "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
    "        # then switch back. In this setup it will correspond for first epoch.\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr*0.1\n",
    "\n",
    "\n",
    "    if args.evaluate:\n",
    "        print('evalution mode')\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    if args.pretrained:\n",
    "        print('evalution of pretrained model')\n",
    "        args.save_dir='pretrained_models'\n",
    "        pretrained_model= args.arch +'.th'\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        # train for one epoch\n",
    "        print('Training {} model'.format(args.arch))\n",
    "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
    "        if is_best:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
    "\n",
    "    return best_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 3.5689 (3.5689)\tPrec@1 7.031 (7.031)\n",
      "Epoch: [0][55/391]\tLoss 1.8030 (2.1498)\tPrec@1 36.719 (23.521)\n",
      "Epoch: [0][110/391]\tLoss 1.7471 (1.9687)\tPrec@1 37.500 (27.717)\n",
      "Epoch: [0][165/391]\tLoss 1.4042 (1.8601)\tPrec@1 42.188 (31.208)\n",
      "Epoch: [0][220/391]\tLoss 1.4470 (1.7783)\tPrec@1 48.438 (34.159)\n",
      "Epoch: [0][275/391]\tLoss 1.3513 (1.7172)\tPrec@1 50.000 (36.450)\n",
      "Epoch: [0][330/391]\tLoss 1.4397 (1.6599)\tPrec@1 46.875 (38.687)\n",
      "Epoch: [0][385/391]\tLoss 1.1727 (1.6140)\tPrec@1 57.812 (40.459)\n",
      "Test\t  Prec@1: 46.470 (Err: 53.530 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.2773 (1.2773)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [1][55/391]\tLoss 1.3059 (1.2579)\tPrec@1 47.656 (53.697)\n",
      "Epoch: [1][110/391]\tLoss 1.1017 (1.2219)\tPrec@1 59.375 (55.279)\n",
      "Epoch: [1][165/391]\tLoss 1.0425 (1.1914)\tPrec@1 57.812 (56.255)\n",
      "Epoch: [1][220/391]\tLoss 1.1828 (1.1725)\tPrec@1 59.375 (56.999)\n",
      "Epoch: [1][275/391]\tLoss 1.2741 (1.1544)\tPrec@1 55.469 (57.801)\n",
      "Epoch: [1][330/391]\tLoss 1.0492 (1.1360)\tPrec@1 62.500 (58.665)\n",
      "Epoch: [1][385/391]\tLoss 0.8574 (1.1174)\tPrec@1 67.969 (59.505)\n",
      "Test\t  Prec@1: 63.240 (Err: 36.760 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 0.9489 (0.9489)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [2][55/391]\tLoss 0.9210 (0.9865)\tPrec@1 64.844 (64.774)\n",
      "Epoch: [2][110/391]\tLoss 0.7821 (0.9646)\tPrec@1 73.438 (65.407)\n",
      "Epoch: [2][165/391]\tLoss 0.8591 (0.9522)\tPrec@1 68.750 (65.983)\n",
      "Epoch: [2][220/391]\tLoss 0.8963 (0.9489)\tPrec@1 68.750 (66.184)\n",
      "Epoch: [2][275/391]\tLoss 0.8503 (0.9389)\tPrec@1 66.406 (66.585)\n",
      "Epoch: [2][330/391]\tLoss 0.9140 (0.9231)\tPrec@1 67.969 (67.119)\n",
      "Epoch: [2][385/391]\tLoss 0.7370 (0.9105)\tPrec@1 68.750 (67.542)\n",
      "Test\t  Prec@1: 68.570 (Err: 31.430 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 0.7664 (0.7664)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [3][55/391]\tLoss 0.7484 (0.8153)\tPrec@1 75.000 (71.247)\n",
      "Epoch: [3][110/391]\tLoss 0.8691 (0.7918)\tPrec@1 71.875 (72.438)\n",
      "Epoch: [3][165/391]\tLoss 0.7135 (0.7897)\tPrec@1 72.656 (72.581)\n",
      "Epoch: [3][220/391]\tLoss 0.8458 (0.7824)\tPrec@1 67.969 (72.663)\n",
      "Epoch: [3][275/391]\tLoss 0.6493 (0.7752)\tPrec@1 75.000 (72.866)\n",
      "Epoch: [3][330/391]\tLoss 0.7229 (0.7689)\tPrec@1 73.438 (73.043)\n",
      "Epoch: [3][385/391]\tLoss 0.7051 (0.7656)\tPrec@1 74.219 (73.189)\n",
      "Test\t  Prec@1: 68.810 (Err: 31.190 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.7984 (0.7984)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [4][55/391]\tLoss 0.6394 (0.7086)\tPrec@1 77.344 (75.167)\n",
      "Epoch: [4][110/391]\tLoss 0.7483 (0.7013)\tPrec@1 71.875 (75.436)\n",
      "Epoch: [4][165/391]\tLoss 0.5359 (0.6985)\tPrec@1 78.125 (75.485)\n",
      "Epoch: [4][220/391]\tLoss 0.6947 (0.6898)\tPrec@1 73.438 (76.004)\n",
      "Epoch: [4][275/391]\tLoss 0.7542 (0.6840)\tPrec@1 69.531 (76.084)\n",
      "Epoch: [4][330/391]\tLoss 0.8857 (0.6812)\tPrec@1 67.969 (76.095)\n",
      "Epoch: [4][385/391]\tLoss 0.5195 (0.6789)\tPrec@1 80.469 (76.158)\n",
      "Test\t  Prec@1: 74.740 (Err: 25.260 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.6025 (0.6025)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [5][55/391]\tLoss 0.6719 (0.6487)\tPrec@1 70.312 (77.706)\n",
      "Epoch: [5][110/391]\tLoss 0.8590 (0.6427)\tPrec@1 72.656 (77.956)\n",
      "Epoch: [5][165/391]\tLoss 0.5009 (0.6349)\tPrec@1 82.812 (78.064)\n",
      "Epoch: [5][220/391]\tLoss 0.5237 (0.6276)\tPrec@1 79.688 (78.330)\n",
      "Epoch: [5][275/391]\tLoss 0.6653 (0.6202)\tPrec@1 77.344 (78.595)\n",
      "Epoch: [5][330/391]\tLoss 0.5887 (0.6149)\tPrec@1 82.812 (78.722)\n",
      "Epoch: [5][385/391]\tLoss 0.5683 (0.6149)\tPrec@1 78.906 (78.773)\n",
      "Test\t  Prec@1: 73.660 (Err: 26.340 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.8142 (0.8142)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [6][55/391]\tLoss 0.5107 (0.5741)\tPrec@1 81.250 (79.980)\n",
      "Epoch: [6][110/391]\tLoss 0.7281 (0.5805)\tPrec@1 77.344 (79.913)\n",
      "Epoch: [6][165/391]\tLoss 0.5412 (0.5773)\tPrec@1 77.344 (80.045)\n",
      "Epoch: [6][220/391]\tLoss 0.5707 (0.5746)\tPrec@1 81.250 (80.045)\n",
      "Epoch: [6][275/391]\tLoss 0.4418 (0.5758)\tPrec@1 83.594 (79.911)\n",
      "Epoch: [6][330/391]\tLoss 0.4816 (0.5755)\tPrec@1 82.812 (79.919)\n",
      "Epoch: [6][385/391]\tLoss 0.5632 (0.5761)\tPrec@1 77.344 (79.868)\n",
      "Test\t  Prec@1: 74.130 (Err: 25.870 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.5797 (0.5797)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [7][55/391]\tLoss 0.5231 (0.5270)\tPrec@1 81.250 (81.627)\n",
      "Epoch: [7][110/391]\tLoss 0.5902 (0.5403)\tPrec@1 75.781 (81.356)\n",
      "Epoch: [7][165/391]\tLoss 0.6219 (0.5409)\tPrec@1 75.781 (81.151)\n",
      "Epoch: [7][220/391]\tLoss 0.4974 (0.5400)\tPrec@1 82.812 (81.331)\n",
      "Epoch: [7][275/391]\tLoss 0.4991 (0.5385)\tPrec@1 82.031 (81.428)\n",
      "Epoch: [7][330/391]\tLoss 0.5362 (0.5370)\tPrec@1 76.562 (81.432)\n",
      "Epoch: [7][385/391]\tLoss 0.4777 (0.5374)\tPrec@1 80.469 (81.432)\n",
      "Test\t  Prec@1: 77.930 (Err: 22.070 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5622 (0.5622)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [8][55/391]\tLoss 0.5844 (0.4986)\tPrec@1 82.031 (82.743)\n",
      "Epoch: [8][110/391]\tLoss 0.4246 (0.4954)\tPrec@1 82.812 (83.017)\n",
      "Epoch: [8][165/391]\tLoss 0.4946 (0.5020)\tPrec@1 82.812 (82.784)\n",
      "Epoch: [8][220/391]\tLoss 0.5272 (0.5028)\tPrec@1 82.031 (82.728)\n",
      "Epoch: [8][275/391]\tLoss 0.4017 (0.5022)\tPrec@1 87.500 (82.801)\n",
      "Epoch: [8][330/391]\tLoss 0.5002 (0.5053)\tPrec@1 83.594 (82.761)\n",
      "Epoch: [8][385/391]\tLoss 0.6581 (0.5058)\tPrec@1 78.125 (82.636)\n",
      "Test\t  Prec@1: 82.140 (Err: 17.860 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.3994 (0.3994)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [9][55/391]\tLoss 0.4892 (0.4692)\tPrec@1 82.031 (83.789)\n",
      "Epoch: [9][110/391]\tLoss 0.4655 (0.4773)\tPrec@1 81.250 (83.326)\n",
      "Epoch: [9][165/391]\tLoss 0.5915 (0.4885)\tPrec@1 80.469 (83.104)\n",
      "Epoch: [9][220/391]\tLoss 0.4470 (0.4871)\tPrec@1 83.594 (83.187)\n",
      "Epoch: [9][275/391]\tLoss 0.4344 (0.4838)\tPrec@1 85.156 (83.263)\n",
      "Epoch: [9][330/391]\tLoss 0.4399 (0.4858)\tPrec@1 82.812 (83.171)\n",
      "Epoch: [9][385/391]\tLoss 0.4604 (0.4849)\tPrec@1 82.031 (83.159)\n",
      "Test\t  Prec@1: 76.610 (Err: 23.390 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.3568 (0.3568)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.5295 (0.4687)\tPrec@1 81.250 (83.608)\n",
      "Epoch: [10][110/391]\tLoss 0.6595 (0.4715)\tPrec@1 79.688 (83.404)\n",
      "Epoch: [10][165/391]\tLoss 0.4585 (0.4663)\tPrec@1 84.375 (83.697)\n",
      "Epoch: [10][220/391]\tLoss 0.4157 (0.4677)\tPrec@1 85.156 (83.710)\n",
      "Epoch: [10][275/391]\tLoss 0.4001 (0.4683)\tPrec@1 87.500 (83.659)\n",
      "Epoch: [10][330/391]\tLoss 0.5180 (0.4691)\tPrec@1 85.156 (83.613)\n",
      "Epoch: [10][385/391]\tLoss 0.4549 (0.4674)\tPrec@1 80.469 (83.737)\n",
      "Test\t  Prec@1: 83.450 (Err: 16.550 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.3987 (0.3987)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [11][55/391]\tLoss 0.3767 (0.4284)\tPrec@1 86.719 (84.710)\n",
      "Epoch: [11][110/391]\tLoss 0.3893 (0.4373)\tPrec@1 85.938 (84.657)\n",
      "Epoch: [11][165/391]\tLoss 0.3808 (0.4366)\tPrec@1 88.281 (84.752)\n",
      "Epoch: [11][220/391]\tLoss 0.3655 (0.4369)\tPrec@1 89.062 (84.721)\n",
      "Epoch: [11][275/391]\tLoss 0.5566 (0.4404)\tPrec@1 78.125 (84.624)\n",
      "Epoch: [11][330/391]\tLoss 0.4883 (0.4411)\tPrec@1 84.375 (84.729)\n",
      "Epoch: [11][385/391]\tLoss 0.4295 (0.4432)\tPrec@1 85.156 (84.683)\n",
      "Test\t  Prec@1: 81.240 (Err: 18.760 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.3807 (0.3807)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [12][55/391]\tLoss 0.6148 (0.4244)\tPrec@1 81.250 (85.226)\n",
      "Epoch: [12][110/391]\tLoss 0.5343 (0.4123)\tPrec@1 82.812 (85.762)\n",
      "Epoch: [12][165/391]\tLoss 0.3424 (0.4202)\tPrec@1 88.281 (85.561)\n",
      "Epoch: [12][220/391]\tLoss 0.5015 (0.4251)\tPrec@1 80.469 (85.333)\n",
      "Epoch: [12][275/391]\tLoss 0.4952 (0.4234)\tPrec@1 85.938 (85.374)\n",
      "Epoch: [12][330/391]\tLoss 0.4658 (0.4285)\tPrec@1 82.031 (85.156)\n",
      "Epoch: [12][385/391]\tLoss 0.4675 (0.4298)\tPrec@1 82.812 (85.094)\n",
      "Test\t  Prec@1: 81.780 (Err: 18.220 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.4428 (0.4428)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [13][55/391]\tLoss 0.3838 (0.4078)\tPrec@1 86.719 (86.119)\n",
      "Epoch: [13][110/391]\tLoss 0.3502 (0.4117)\tPrec@1 86.719 (85.909)\n",
      "Epoch: [13][165/391]\tLoss 0.4770 (0.4141)\tPrec@1 87.500 (85.853)\n",
      "Epoch: [13][220/391]\tLoss 0.4510 (0.4150)\tPrec@1 81.250 (85.750)\n",
      "Epoch: [13][275/391]\tLoss 0.4052 (0.4132)\tPrec@1 86.719 (85.813)\n",
      "Epoch: [13][330/391]\tLoss 0.5383 (0.4142)\tPrec@1 82.031 (85.753)\n",
      "Epoch: [13][385/391]\tLoss 0.6368 (0.4160)\tPrec@1 80.469 (85.711)\n",
      "Test\t  Prec@1: 77.770 (Err: 22.230 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3677 (0.3677)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [14][55/391]\tLoss 0.4127 (0.4051)\tPrec@1 83.594 (85.993)\n",
      "Epoch: [14][110/391]\tLoss 0.4185 (0.4117)\tPrec@1 87.500 (85.797)\n",
      "Epoch: [14][165/391]\tLoss 0.4011 (0.4083)\tPrec@1 86.719 (85.952)\n",
      "Epoch: [14][220/391]\tLoss 0.3408 (0.4104)\tPrec@1 85.938 (85.757)\n",
      "Epoch: [14][275/391]\tLoss 0.5280 (0.4116)\tPrec@1 81.250 (85.728)\n",
      "Epoch: [14][330/391]\tLoss 0.4579 (0.4108)\tPrec@1 81.250 (85.772)\n",
      "Epoch: [14][385/391]\tLoss 0.3558 (0.4108)\tPrec@1 87.500 (85.753)\n",
      "Test\t  Prec@1: 81.710 (Err: 18.290 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.4395 (0.4395)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [15][55/391]\tLoss 0.4930 (0.4226)\tPrec@1 85.156 (85.491)\n",
      "Epoch: [15][110/391]\tLoss 0.3738 (0.4039)\tPrec@1 84.375 (85.923)\n",
      "Epoch: [15][165/391]\tLoss 0.5745 (0.4086)\tPrec@1 84.375 (85.721)\n",
      "Epoch: [15][220/391]\tLoss 0.4040 (0.4040)\tPrec@1 84.375 (86.022)\n",
      "Epoch: [15][275/391]\tLoss 0.3452 (0.4027)\tPrec@1 84.375 (86.020)\n",
      "Epoch: [15][330/391]\tLoss 0.3901 (0.3994)\tPrec@1 86.719 (86.218)\n",
      "Epoch: [15][385/391]\tLoss 0.2796 (0.3973)\tPrec@1 89.844 (86.247)\n",
      "Test\t  Prec@1: 82.700 (Err: 17.300 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.4757 (0.4757)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [16][55/391]\tLoss 0.3757 (0.3904)\tPrec@1 84.375 (86.063)\n",
      "Epoch: [16][110/391]\tLoss 0.3666 (0.3832)\tPrec@1 85.938 (86.677)\n",
      "Epoch: [16][165/391]\tLoss 0.4184 (0.3832)\tPrec@1 85.156 (86.672)\n",
      "Epoch: [16][220/391]\tLoss 0.3525 (0.3833)\tPrec@1 86.719 (86.698)\n",
      "Epoch: [16][275/391]\tLoss 0.2559 (0.3834)\tPrec@1 92.188 (86.671)\n",
      "Epoch: [16][330/391]\tLoss 0.2999 (0.3826)\tPrec@1 91.406 (86.709)\n",
      "Epoch: [16][385/391]\tLoss 0.4011 (0.3865)\tPrec@1 85.938 (86.601)\n",
      "Test\t  Prec@1: 82.910 (Err: 17.090 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.2835 (0.2835)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [17][55/391]\tLoss 0.3373 (0.3613)\tPrec@1 88.281 (87.374)\n",
      "Epoch: [17][110/391]\tLoss 0.3461 (0.3655)\tPrec@1 85.938 (87.310)\n",
      "Epoch: [17][165/391]\tLoss 0.3945 (0.3665)\tPrec@1 89.062 (87.293)\n",
      "Epoch: [17][220/391]\tLoss 0.3269 (0.3691)\tPrec@1 89.844 (87.175)\n",
      "Epoch: [17][275/391]\tLoss 0.2431 (0.3672)\tPrec@1 91.406 (87.291)\n",
      "Epoch: [17][330/391]\tLoss 0.4153 (0.3736)\tPrec@1 86.719 (87.137)\n",
      "Epoch: [17][385/391]\tLoss 0.3621 (0.3751)\tPrec@1 85.156 (87.047)\n",
      "Test\t  Prec@1: 84.350 (Err: 15.650 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3107 (0.3107)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [18][55/391]\tLoss 0.4292 (0.3574)\tPrec@1 86.719 (87.779)\n",
      "Epoch: [18][110/391]\tLoss 0.3607 (0.3588)\tPrec@1 85.938 (87.613)\n",
      "Epoch: [18][165/391]\tLoss 0.3895 (0.3609)\tPrec@1 85.156 (87.472)\n",
      "Epoch: [18][220/391]\tLoss 0.3360 (0.3611)\tPrec@1 88.281 (87.557)\n",
      "Epoch: [18][275/391]\tLoss 0.2975 (0.3603)\tPrec@1 89.062 (87.528)\n",
      "Epoch: [18][330/391]\tLoss 0.4372 (0.3614)\tPrec@1 84.375 (87.491)\n",
      "Epoch: [18][385/391]\tLoss 0.3984 (0.3657)\tPrec@1 86.719 (87.340)\n",
      "Test\t  Prec@1: 83.950 (Err: 16.050 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3073 (0.3073)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [19][55/391]\tLoss 0.3183 (0.3441)\tPrec@1 89.844 (88.128)\n",
      "Epoch: [19][110/391]\tLoss 0.3024 (0.3439)\tPrec@1 89.844 (87.922)\n",
      "Epoch: [19][165/391]\tLoss 0.2869 (0.3442)\tPrec@1 92.188 (87.914)\n",
      "Epoch: [19][220/391]\tLoss 0.3201 (0.3480)\tPrec@1 86.719 (87.723)\n",
      "Epoch: [19][275/391]\tLoss 0.4453 (0.3546)\tPrec@1 83.594 (87.531)\n",
      "Epoch: [19][330/391]\tLoss 0.3360 (0.3591)\tPrec@1 89.844 (87.441)\n",
      "Epoch: [19][385/391]\tLoss 0.3225 (0.3629)\tPrec@1 90.625 (87.362)\n",
      "Test\t  Prec@1: 75.220 (Err: 24.780 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.4031 (0.4031)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [20][55/391]\tLoss 0.3010 (0.3444)\tPrec@1 89.844 (88.030)\n",
      "Epoch: [20][110/391]\tLoss 0.2723 (0.3437)\tPrec@1 89.062 (88.042)\n",
      "Epoch: [20][165/391]\tLoss 0.4638 (0.3425)\tPrec@1 84.375 (88.093)\n",
      "Epoch: [20][220/391]\tLoss 0.4811 (0.3493)\tPrec@1 86.719 (87.892)\n",
      "Epoch: [20][275/391]\tLoss 0.2438 (0.3489)\tPrec@1 92.188 (87.843)\n",
      "Epoch: [20][330/391]\tLoss 0.3735 (0.3489)\tPrec@1 84.375 (87.795)\n",
      "Epoch: [20][385/391]\tLoss 0.3382 (0.3538)\tPrec@1 88.281 (87.674)\n",
      "Test\t  Prec@1: 81.890 (Err: 18.110 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.2055 (0.2055)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [21][55/391]\tLoss 0.3908 (0.3503)\tPrec@1 88.281 (87.849)\n",
      "Epoch: [21][110/391]\tLoss 0.2945 (0.3430)\tPrec@1 89.844 (87.950)\n",
      "Epoch: [21][165/391]\tLoss 0.4068 (0.3408)\tPrec@1 85.938 (88.074)\n",
      "Epoch: [21][220/391]\tLoss 0.3528 (0.3446)\tPrec@1 86.719 (87.917)\n",
      "Epoch: [21][275/391]\tLoss 0.3164 (0.3467)\tPrec@1 87.500 (87.780)\n",
      "Epoch: [21][330/391]\tLoss 0.3493 (0.3474)\tPrec@1 91.406 (87.856)\n",
      "Epoch: [21][385/391]\tLoss 0.3860 (0.3452)\tPrec@1 86.719 (87.945)\n",
      "Test\t  Prec@1: 82.440 (Err: 17.560 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.3873 (0.3873)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [22][55/391]\tLoss 0.4480 (0.3437)\tPrec@1 86.719 (88.100)\n",
      "Epoch: [22][110/391]\tLoss 0.3564 (0.3397)\tPrec@1 87.500 (88.112)\n",
      "Epoch: [22][165/391]\tLoss 0.3387 (0.3399)\tPrec@1 89.062 (88.145)\n",
      "Epoch: [22][220/391]\tLoss 0.3005 (0.3395)\tPrec@1 87.500 (88.136)\n",
      "Epoch: [22][275/391]\tLoss 0.4456 (0.3401)\tPrec@1 85.938 (88.131)\n",
      "Epoch: [22][330/391]\tLoss 0.4902 (0.3406)\tPrec@1 82.031 (88.050)\n",
      "Epoch: [22][385/391]\tLoss 0.2498 (0.3411)\tPrec@1 90.625 (88.107)\n",
      "Test\t  Prec@1: 85.170 (Err: 14.830 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2417 (0.2417)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [23][55/391]\tLoss 0.3186 (0.3182)\tPrec@1 88.281 (88.937)\n",
      "Epoch: [23][110/391]\tLoss 0.2955 (0.3165)\tPrec@1 92.188 (89.168)\n",
      "Epoch: [23][165/391]\tLoss 0.5401 (0.3257)\tPrec@1 82.812 (88.766)\n",
      "Epoch: [23][220/391]\tLoss 0.4309 (0.3292)\tPrec@1 86.719 (88.652)\n",
      "Epoch: [23][275/391]\tLoss 0.3749 (0.3310)\tPrec@1 86.719 (88.519)\n",
      "Epoch: [23][330/391]\tLoss 0.4910 (0.3358)\tPrec@1 83.594 (88.281)\n",
      "Epoch: [23][385/391]\tLoss 0.3150 (0.3372)\tPrec@1 89.844 (88.281)\n",
      "Test\t  Prec@1: 82.520 (Err: 17.480 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.3067 (0.3067)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [24][55/391]\tLoss 0.3308 (0.3250)\tPrec@1 88.281 (88.881)\n",
      "Epoch: [24][110/391]\tLoss 0.3420 (0.3256)\tPrec@1 86.719 (88.929)\n",
      "Epoch: [24][165/391]\tLoss 0.3027 (0.3274)\tPrec@1 90.625 (88.837)\n",
      "Epoch: [24][220/391]\tLoss 0.3665 (0.3257)\tPrec@1 88.281 (88.879)\n",
      "Epoch: [24][275/391]\tLoss 0.2296 (0.3274)\tPrec@1 92.969 (88.697)\n",
      "Epoch: [24][330/391]\tLoss 0.2829 (0.3259)\tPrec@1 88.281 (88.742)\n",
      "Epoch: [24][385/391]\tLoss 0.5488 (0.3292)\tPrec@1 82.812 (88.670)\n",
      "Test\t  Prec@1: 80.440 (Err: 19.560 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.3675 (0.3675)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [25][55/391]\tLoss 0.3133 (0.3211)\tPrec@1 86.719 (88.825)\n",
      "Epoch: [25][110/391]\tLoss 0.3875 (0.3223)\tPrec@1 86.719 (88.795)\n",
      "Epoch: [25][165/391]\tLoss 0.4008 (0.3261)\tPrec@1 85.938 (88.535)\n",
      "Epoch: [25][220/391]\tLoss 0.3932 (0.3256)\tPrec@1 87.500 (88.631)\n",
      "Epoch: [25][275/391]\tLoss 0.3408 (0.3262)\tPrec@1 88.281 (88.638)\n",
      "Epoch: [25][330/391]\tLoss 0.3999 (0.3288)\tPrec@1 84.375 (88.508)\n",
      "Epoch: [25][385/391]\tLoss 0.3615 (0.3310)\tPrec@1 88.281 (88.453)\n",
      "Test\t  Prec@1: 86.120 (Err: 13.880 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.3158 (0.3158)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [26][55/391]\tLoss 0.4130 (0.2820)\tPrec@1 86.719 (90.234)\n",
      "Epoch: [26][110/391]\tLoss 0.2709 (0.3036)\tPrec@1 90.625 (89.358)\n",
      "Epoch: [26][165/391]\tLoss 0.3091 (0.3080)\tPrec@1 88.281 (89.180)\n",
      "Epoch: [26][220/391]\tLoss 0.2806 (0.3084)\tPrec@1 90.625 (89.186)\n",
      "Epoch: [26][275/391]\tLoss 0.3651 (0.3111)\tPrec@1 86.719 (89.062)\n",
      "Epoch: [26][330/391]\tLoss 0.3532 (0.3134)\tPrec@1 85.938 (88.980)\n",
      "Epoch: [26][385/391]\tLoss 0.3429 (0.3152)\tPrec@1 89.844 (88.892)\n",
      "Test\t  Prec@1: 83.130 (Err: 16.870 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.2888 (0.2888)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [27][55/391]\tLoss 0.3214 (0.2960)\tPrec@1 88.281 (89.886)\n",
      "Epoch: [27][110/391]\tLoss 0.3418 (0.3044)\tPrec@1 88.281 (89.562)\n",
      "Epoch: [27][165/391]\tLoss 0.2159 (0.3082)\tPrec@1 92.188 (89.284)\n",
      "Epoch: [27][220/391]\tLoss 0.2813 (0.3113)\tPrec@1 91.406 (89.169)\n",
      "Epoch: [27][275/391]\tLoss 0.2023 (0.3102)\tPrec@1 93.750 (89.252)\n",
      "Epoch: [27][330/391]\tLoss 0.3516 (0.3118)\tPrec@1 87.500 (89.178)\n",
      "Epoch: [27][385/391]\tLoss 0.3942 (0.3164)\tPrec@1 89.844 (88.998)\n",
      "Test\t  Prec@1: 83.300 (Err: 16.700 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.3009 (0.3009)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [28][55/391]\tLoss 0.2045 (0.2964)\tPrec@1 91.406 (89.495)\n",
      "Epoch: [28][110/391]\tLoss 0.3458 (0.2990)\tPrec@1 86.719 (89.316)\n",
      "Epoch: [28][165/391]\tLoss 0.2541 (0.3034)\tPrec@1 90.625 (89.270)\n",
      "Epoch: [28][220/391]\tLoss 0.2524 (0.3040)\tPrec@1 89.844 (89.345)\n",
      "Epoch: [28][275/391]\tLoss 0.2298 (0.3067)\tPrec@1 92.188 (89.323)\n",
      "Epoch: [28][330/391]\tLoss 0.2150 (0.3105)\tPrec@1 92.188 (89.256)\n",
      "Epoch: [28][385/391]\tLoss 0.2933 (0.3132)\tPrec@1 90.625 (89.186)\n",
      "Test\t  Prec@1: 84.100 (Err: 15.900 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2677 (0.2677)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [29][55/391]\tLoss 0.3773 (0.2900)\tPrec@1 87.500 (89.914)\n",
      "Epoch: [29][110/391]\tLoss 0.2626 (0.2975)\tPrec@1 91.406 (89.780)\n",
      "Epoch: [29][165/391]\tLoss 0.3762 (0.3011)\tPrec@1 87.500 (89.623)\n",
      "Epoch: [29][220/391]\tLoss 0.4282 (0.3003)\tPrec@1 83.594 (89.540)\n",
      "Epoch: [29][275/391]\tLoss 0.2745 (0.3048)\tPrec@1 92.188 (89.334)\n",
      "Epoch: [29][330/391]\tLoss 0.2011 (0.3093)\tPrec@1 92.188 (89.114)\n",
      "Epoch: [29][385/391]\tLoss 0.2742 (0.3105)\tPrec@1 89.844 (89.081)\n",
      "Test\t  Prec@1: 85.050 (Err: 14.950 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2548 (0.2548)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [30][55/391]\tLoss 0.1422 (0.2766)\tPrec@1 94.531 (90.430)\n",
      "Epoch: [30][110/391]\tLoss 0.3146 (0.2905)\tPrec@1 89.062 (89.999)\n",
      "Epoch: [30][165/391]\tLoss 0.3877 (0.2944)\tPrec@1 85.938 (89.839)\n",
      "Epoch: [30][220/391]\tLoss 0.2397 (0.2946)\tPrec@1 92.188 (89.840)\n",
      "Epoch: [30][275/391]\tLoss 0.4618 (0.2987)\tPrec@1 82.812 (89.671)\n",
      "Epoch: [30][330/391]\tLoss 0.2343 (0.3007)\tPrec@1 91.406 (89.631)\n",
      "Epoch: [30][385/391]\tLoss 0.3444 (0.3063)\tPrec@1 88.281 (89.429)\n",
      "Test\t  Prec@1: 85.940 (Err: 14.060 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.3689 (0.3689)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [31][55/391]\tLoss 0.1947 (0.2950)\tPrec@1 92.188 (89.858)\n",
      "Epoch: [31][110/391]\tLoss 0.3254 (0.2988)\tPrec@1 89.844 (89.604)\n",
      "Epoch: [31][165/391]\tLoss 0.2158 (0.2935)\tPrec@1 91.406 (89.811)\n",
      "Epoch: [31][220/391]\tLoss 0.4515 (0.2962)\tPrec@1 84.375 (89.646)\n",
      "Epoch: [31][275/391]\tLoss 0.3743 (0.2992)\tPrec@1 87.500 (89.518)\n",
      "Epoch: [31][330/391]\tLoss 0.3167 (0.3010)\tPrec@1 89.844 (89.499)\n",
      "Epoch: [31][385/391]\tLoss 0.2668 (0.3010)\tPrec@1 91.406 (89.520)\n",
      "Test\t  Prec@1: 87.260 (Err: 12.740 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.2647 (0.2647)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [32][55/391]\tLoss 0.3998 (0.2815)\tPrec@1 83.594 (90.025)\n",
      "Epoch: [32][110/391]\tLoss 0.1999 (0.2938)\tPrec@1 92.969 (89.661)\n",
      "Epoch: [32][165/391]\tLoss 0.3356 (0.2966)\tPrec@1 89.844 (89.641)\n",
      "Epoch: [32][220/391]\tLoss 0.2504 (0.3018)\tPrec@1 91.406 (89.384)\n",
      "Epoch: [32][275/391]\tLoss 0.2584 (0.2970)\tPrec@1 91.406 (89.589)\n",
      "Epoch: [32][330/391]\tLoss 0.2337 (0.2953)\tPrec@1 92.188 (89.641)\n",
      "Epoch: [32][385/391]\tLoss 0.4041 (0.2949)\tPrec@1 83.594 (89.664)\n",
      "Test\t  Prec@1: 86.410 (Err: 13.590 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.3303 (0.3303)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [33][55/391]\tLoss 0.3430 (0.2902)\tPrec@1 88.281 (89.634)\n",
      "Epoch: [33][110/391]\tLoss 0.2337 (0.2980)\tPrec@1 91.406 (89.633)\n",
      "Epoch: [33][165/391]\tLoss 0.3210 (0.2988)\tPrec@1 86.719 (89.575)\n",
      "Epoch: [33][220/391]\tLoss 0.3998 (0.2999)\tPrec@1 88.281 (89.494)\n",
      "Epoch: [33][275/391]\tLoss 0.2206 (0.2997)\tPrec@1 92.969 (89.558)\n",
      "Epoch: [33][330/391]\tLoss 0.3468 (0.2989)\tPrec@1 89.844 (89.594)\n",
      "Epoch: [33][385/391]\tLoss 0.3262 (0.2982)\tPrec@1 86.719 (89.603)\n",
      "Test\t  Prec@1: 83.480 (Err: 16.520 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.3326 (0.3326)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [34][55/391]\tLoss 0.3934 (0.2680)\tPrec@1 82.031 (90.695)\n",
      "Epoch: [34][110/391]\tLoss 0.2438 (0.2749)\tPrec@1 92.188 (90.358)\n",
      "Epoch: [34][165/391]\tLoss 0.2857 (0.2840)\tPrec@1 89.844 (90.140)\n",
      "Epoch: [34][220/391]\tLoss 0.3011 (0.2857)\tPrec@1 88.281 (90.024)\n",
      "Epoch: [34][275/391]\tLoss 0.2749 (0.2853)\tPrec@1 90.625 (90.090)\n",
      "Epoch: [34][330/391]\tLoss 0.3788 (0.2831)\tPrec@1 87.500 (90.210)\n",
      "Epoch: [34][385/391]\tLoss 0.2728 (0.2866)\tPrec@1 90.625 (90.097)\n",
      "Test\t  Prec@1: 83.160 (Err: 16.840 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.1987 (0.1987)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [35][55/391]\tLoss 0.1520 (0.2916)\tPrec@1 96.094 (89.579)\n",
      "Epoch: [35][110/391]\tLoss 0.2863 (0.2833)\tPrec@1 89.062 (89.865)\n",
      "Epoch: [35][165/391]\tLoss 0.3673 (0.2831)\tPrec@1 88.281 (89.914)\n",
      "Epoch: [35][220/391]\tLoss 0.1778 (0.2857)\tPrec@1 92.969 (89.833)\n",
      "Epoch: [35][275/391]\tLoss 0.3048 (0.2835)\tPrec@1 91.406 (89.948)\n",
      "Epoch: [35][330/391]\tLoss 0.2017 (0.2863)\tPrec@1 94.531 (89.889)\n",
      "Epoch: [35][385/391]\tLoss 0.2605 (0.2894)\tPrec@1 91.406 (89.819)\n",
      "Test\t  Prec@1: 81.000 (Err: 19.000 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2734 (0.2734)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [36][55/391]\tLoss 0.2620 (0.2664)\tPrec@1 87.500 (90.653)\n",
      "Epoch: [36][110/391]\tLoss 0.3891 (0.2756)\tPrec@1 85.938 (90.210)\n",
      "Epoch: [36][165/391]\tLoss 0.3283 (0.2781)\tPrec@1 89.844 (90.239)\n",
      "Epoch: [36][220/391]\tLoss 0.2738 (0.2776)\tPrec@1 88.281 (90.233)\n",
      "Epoch: [36][275/391]\tLoss 0.4013 (0.2781)\tPrec@1 83.594 (90.135)\n",
      "Epoch: [36][330/391]\tLoss 0.3685 (0.2811)\tPrec@1 85.156 (90.092)\n",
      "Epoch: [36][385/391]\tLoss 0.4142 (0.2832)\tPrec@1 83.594 (89.998)\n",
      "Test\t  Prec@1: 82.630 (Err: 17.370 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.3453 (0.3453)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [37][55/391]\tLoss 0.2686 (0.2727)\tPrec@1 90.625 (90.318)\n",
      "Epoch: [37][110/391]\tLoss 0.3350 (0.2729)\tPrec@1 87.500 (90.329)\n",
      "Epoch: [37][165/391]\tLoss 0.2703 (0.2768)\tPrec@1 90.625 (90.154)\n",
      "Epoch: [37][220/391]\tLoss 0.1696 (0.2780)\tPrec@1 93.750 (90.144)\n",
      "Epoch: [37][275/391]\tLoss 0.4113 (0.2800)\tPrec@1 86.719 (90.164)\n",
      "Epoch: [37][330/391]\tLoss 0.3527 (0.2847)\tPrec@1 88.281 (90.077)\n",
      "Epoch: [37][385/391]\tLoss 0.2027 (0.2856)\tPrec@1 93.750 (90.066)\n",
      "Test\t  Prec@1: 86.190 (Err: 13.810 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.3085 (0.3085)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.1438 (0.2812)\tPrec@1 96.094 (90.290)\n",
      "Epoch: [38][110/391]\tLoss 0.3418 (0.2750)\tPrec@1 89.844 (90.569)\n",
      "Epoch: [38][165/391]\tLoss 0.2674 (0.2760)\tPrec@1 89.844 (90.465)\n",
      "Epoch: [38][220/391]\tLoss 0.1836 (0.2743)\tPrec@1 92.969 (90.455)\n",
      "Epoch: [38][275/391]\tLoss 0.2268 (0.2754)\tPrec@1 91.406 (90.427)\n",
      "Epoch: [38][330/391]\tLoss 0.3263 (0.2773)\tPrec@1 89.062 (90.295)\n",
      "Epoch: [38][385/391]\tLoss 0.3144 (0.2811)\tPrec@1 88.281 (90.168)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2527 (0.2527)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [39][55/391]\tLoss 0.2892 (0.2715)\tPrec@1 91.406 (90.332)\n",
      "Epoch: [39][110/391]\tLoss 0.2308 (0.2706)\tPrec@1 92.969 (90.435)\n",
      "Epoch: [39][165/391]\tLoss 0.1669 (0.2696)\tPrec@1 93.750 (90.559)\n",
      "Epoch: [39][220/391]\tLoss 0.4453 (0.2738)\tPrec@1 82.031 (90.332)\n",
      "Epoch: [39][275/391]\tLoss 0.2237 (0.2774)\tPrec@1 92.188 (90.220)\n",
      "Epoch: [39][330/391]\tLoss 0.3421 (0.2789)\tPrec@1 86.719 (90.238)\n",
      "Epoch: [39][385/391]\tLoss 0.2236 (0.2764)\tPrec@1 92.969 (90.344)\n",
      "Test\t  Prec@1: 85.560 (Err: 14.440 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.4054 (0.4054)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [40][55/391]\tLoss 0.3599 (0.2686)\tPrec@1 88.281 (90.527)\n",
      "Epoch: [40][110/391]\tLoss 0.2875 (0.2725)\tPrec@1 89.844 (90.456)\n",
      "Epoch: [40][165/391]\tLoss 0.2276 (0.2656)\tPrec@1 93.750 (90.667)\n",
      "Epoch: [40][220/391]\tLoss 0.3266 (0.2683)\tPrec@1 86.719 (90.544)\n",
      "Epoch: [40][275/391]\tLoss 0.3427 (0.2717)\tPrec@1 85.938 (90.441)\n",
      "Epoch: [40][330/391]\tLoss 0.3148 (0.2712)\tPrec@1 89.062 (90.446)\n",
      "Epoch: [40][385/391]\tLoss 0.2254 (0.2737)\tPrec@1 89.844 (90.330)\n",
      "Test\t  Prec@1: 85.830 (Err: 14.170 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2654 (0.2654)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [41][55/391]\tLoss 0.2290 (0.2610)\tPrec@1 91.406 (91.309)\n",
      "Epoch: [41][110/391]\tLoss 0.2183 (0.2582)\tPrec@1 91.406 (91.181)\n",
      "Epoch: [41][165/391]\tLoss 0.2112 (0.2647)\tPrec@1 90.625 (90.954)\n",
      "Epoch: [41][220/391]\tLoss 0.2830 (0.2662)\tPrec@1 89.062 (90.777)\n",
      "Epoch: [41][275/391]\tLoss 0.3432 (0.2706)\tPrec@1 85.938 (90.588)\n",
      "Epoch: [41][330/391]\tLoss 0.3344 (0.2710)\tPrec@1 88.281 (90.526)\n",
      "Epoch: [41][385/391]\tLoss 0.3709 (0.2754)\tPrec@1 85.938 (90.374)\n",
      "Test\t  Prec@1: 82.280 (Err: 17.720 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.3190 (0.3190)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [42][55/391]\tLoss 0.4283 (0.2817)\tPrec@1 84.375 (90.416)\n",
      "Epoch: [42][110/391]\tLoss 0.2294 (0.2671)\tPrec@1 90.625 (90.892)\n",
      "Epoch: [42][165/391]\tLoss 0.2626 (0.2706)\tPrec@1 92.188 (90.672)\n",
      "Epoch: [42][220/391]\tLoss 0.2926 (0.2723)\tPrec@1 89.062 (90.593)\n",
      "Epoch: [42][275/391]\tLoss 0.2404 (0.2723)\tPrec@1 90.625 (90.577)\n",
      "Epoch: [42][330/391]\tLoss 0.2785 (0.2715)\tPrec@1 90.625 (90.594)\n",
      "Epoch: [42][385/391]\tLoss 0.2708 (0.2730)\tPrec@1 92.188 (90.518)\n",
      "Test\t  Prec@1: 85.320 (Err: 14.680 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.1439 (0.1439)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [43][55/391]\tLoss 0.2210 (0.2634)\tPrec@1 93.750 (90.778)\n",
      "Epoch: [43][110/391]\tLoss 0.2801 (0.2643)\tPrec@1 89.844 (90.731)\n",
      "Epoch: [43][165/391]\tLoss 0.2711 (0.2667)\tPrec@1 90.625 (90.729)\n",
      "Epoch: [43][220/391]\tLoss 0.1890 (0.2688)\tPrec@1 93.750 (90.770)\n",
      "Epoch: [43][275/391]\tLoss 0.2519 (0.2710)\tPrec@1 91.406 (90.707)\n",
      "Epoch: [43][330/391]\tLoss 0.3431 (0.2708)\tPrec@1 88.281 (90.691)\n",
      "Epoch: [43][385/391]\tLoss 0.1796 (0.2709)\tPrec@1 94.531 (90.670)\n",
      "Test\t  Prec@1: 85.550 (Err: 14.450 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2111 (0.2111)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [44][55/391]\tLoss 0.3024 (0.2318)\tPrec@1 89.062 (92.034)\n",
      "Epoch: [44][110/391]\tLoss 0.2772 (0.2388)\tPrec@1 88.281 (91.624)\n",
      "Epoch: [44][165/391]\tLoss 0.2293 (0.2447)\tPrec@1 91.406 (91.491)\n",
      "Epoch: [44][220/391]\tLoss 0.3806 (0.2571)\tPrec@1 90.625 (91.067)\n",
      "Epoch: [44][275/391]\tLoss 0.2053 (0.2621)\tPrec@1 90.625 (90.863)\n",
      "Epoch: [44][330/391]\tLoss 0.3181 (0.2633)\tPrec@1 86.719 (90.752)\n",
      "Epoch: [44][385/391]\tLoss 0.3750 (0.2676)\tPrec@1 85.156 (90.635)\n",
      "Test\t  Prec@1: 85.530 (Err: 14.470 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.4227 (0.4227)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [45][55/391]\tLoss 0.2472 (0.2454)\tPrec@1 87.500 (90.932)\n",
      "Epoch: [45][110/391]\tLoss 0.2870 (0.2564)\tPrec@1 87.500 (90.759)\n",
      "Epoch: [45][165/391]\tLoss 0.2308 (0.2602)\tPrec@1 89.844 (90.653)\n",
      "Epoch: [45][220/391]\tLoss 0.2970 (0.2654)\tPrec@1 90.625 (90.558)\n",
      "Epoch: [45][275/391]\tLoss 0.2825 (0.2653)\tPrec@1 91.406 (90.614)\n",
      "Epoch: [45][330/391]\tLoss 0.2552 (0.2656)\tPrec@1 93.750 (90.625)\n",
      "Epoch: [45][385/391]\tLoss 0.2417 (0.2660)\tPrec@1 89.062 (90.647)\n",
      "Test\t  Prec@1: 84.230 (Err: 15.770 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2240 (0.2240)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [46][55/391]\tLoss 0.1927 (0.2509)\tPrec@1 92.188 (91.071)\n",
      "Epoch: [46][110/391]\tLoss 0.2645 (0.2584)\tPrec@1 90.625 (90.949)\n",
      "Epoch: [46][165/391]\tLoss 0.2087 (0.2605)\tPrec@1 92.188 (90.874)\n",
      "Epoch: [46][220/391]\tLoss 0.4249 (0.2617)\tPrec@1 87.500 (90.925)\n",
      "Epoch: [46][275/391]\tLoss 0.3751 (0.2643)\tPrec@1 88.281 (90.815)\n",
      "Epoch: [46][330/391]\tLoss 0.2034 (0.2654)\tPrec@1 93.750 (90.710)\n",
      "Epoch: [46][385/391]\tLoss 0.2014 (0.2700)\tPrec@1 92.969 (90.506)\n",
      "Test\t  Prec@1: 85.320 (Err: 14.680 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.2246 (0.2246)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [47][55/391]\tLoss 0.1906 (0.2339)\tPrec@1 96.094 (91.755)\n",
      "Epoch: [47][110/391]\tLoss 0.2062 (0.2437)\tPrec@1 92.969 (91.456)\n",
      "Epoch: [47][165/391]\tLoss 0.3185 (0.2437)\tPrec@1 89.062 (91.416)\n",
      "Epoch: [47][220/391]\tLoss 0.3483 (0.2491)\tPrec@1 87.500 (91.307)\n",
      "Epoch: [47][275/391]\tLoss 0.2415 (0.2531)\tPrec@1 92.188 (91.140)\n",
      "Epoch: [47][330/391]\tLoss 0.2821 (0.2543)\tPrec@1 90.625 (91.118)\n",
      "Epoch: [47][385/391]\tLoss 0.2325 (0.2564)\tPrec@1 91.406 (91.062)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.2797 (0.2797)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [48][55/391]\tLoss 0.3074 (0.2589)\tPrec@1 89.844 (91.295)\n",
      "Epoch: [48][110/391]\tLoss 0.3188 (0.2553)\tPrec@1 89.844 (91.202)\n",
      "Epoch: [48][165/391]\tLoss 0.2203 (0.2530)\tPrec@1 93.750 (91.237)\n",
      "Epoch: [48][220/391]\tLoss 0.2161 (0.2525)\tPrec@1 92.969 (91.208)\n",
      "Epoch: [48][275/391]\tLoss 0.2372 (0.2540)\tPrec@1 94.531 (91.126)\n",
      "Epoch: [48][330/391]\tLoss 0.3137 (0.2538)\tPrec@1 86.719 (91.104)\n",
      "Epoch: [48][385/391]\tLoss 0.2614 (0.2565)\tPrec@1 89.844 (90.983)\n",
      "Test\t  Prec@1: 84.500 (Err: 15.500 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2033 (0.2033)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [49][55/391]\tLoss 0.2537 (0.2507)\tPrec@1 91.406 (91.532)\n",
      "Epoch: [49][110/391]\tLoss 0.1372 (0.2515)\tPrec@1 96.094 (91.322)\n",
      "Epoch: [49][165/391]\tLoss 0.2633 (0.2591)\tPrec@1 89.062 (91.002)\n",
      "Epoch: [49][220/391]\tLoss 0.2551 (0.2608)\tPrec@1 88.281 (90.876)\n",
      "Epoch: [49][275/391]\tLoss 0.2148 (0.2612)\tPrec@1 92.188 (90.823)\n",
      "Epoch: [49][330/391]\tLoss 0.2165 (0.2630)\tPrec@1 92.969 (90.729)\n",
      "Epoch: [49][385/391]\tLoss 0.3681 (0.2668)\tPrec@1 85.938 (90.631)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "The lowest error from resnet20 model after 50 epochs is 12.740\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 4.8126 (4.8126)\tPrec@1 10.938 (10.938)\n",
      "Epoch: [0][55/391]\tLoss 2.0015 (3.2928)\tPrec@1 22.656 (13.560)\n",
      "Epoch: [0][110/391]\tLoss 1.8453 (2.6238)\tPrec@1 31.250 (19.264)\n",
      "Epoch: [0][165/391]\tLoss 1.7414 (2.3557)\tPrec@1 35.938 (22.948)\n",
      "Epoch: [0][220/391]\tLoss 1.6716 (2.1945)\tPrec@1 37.500 (26.057)\n",
      "Epoch: [0][275/391]\tLoss 1.7556 (2.0908)\tPrec@1 33.594 (28.054)\n",
      "Epoch: [0][330/391]\tLoss 1.6340 (2.0123)\tPrec@1 39.062 (30.060)\n",
      "Epoch: [0][385/391]\tLoss 1.5781 (1.9461)\tPrec@1 40.625 (31.835)\n",
      "Test\t  Prec@1: 40.840 (Err: 59.160 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.4036 (1.4036)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [1][55/391]\tLoss 1.3088 (1.4894)\tPrec@1 57.031 (44.587)\n",
      "Epoch: [1][110/391]\tLoss 1.4552 (1.4871)\tPrec@1 46.875 (44.728)\n",
      "Epoch: [1][165/391]\tLoss 1.4664 (1.4694)\tPrec@1 42.188 (45.628)\n",
      "Epoch: [1][220/391]\tLoss 1.5268 (1.4522)\tPrec@1 46.094 (46.394)\n",
      "Epoch: [1][275/391]\tLoss 1.3794 (1.4371)\tPrec@1 50.781 (47.169)\n",
      "Epoch: [1][330/391]\tLoss 1.3241 (1.4179)\tPrec@1 50.000 (47.935)\n",
      "Epoch: [1][385/391]\tLoss 1.2386 (1.3965)\tPrec@1 53.125 (48.725)\n",
      "Test\t  Prec@1: 49.320 (Err: 50.680 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.2987 (1.2987)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [2][55/391]\tLoss 1.0876 (1.2454)\tPrec@1 65.625 (54.925)\n",
      "Epoch: [2][110/391]\tLoss 1.2100 (1.2182)\tPrec@1 52.344 (56.046)\n",
      "Epoch: [2][165/391]\tLoss 1.1827 (1.2003)\tPrec@1 59.375 (56.631)\n",
      "Epoch: [2][220/391]\tLoss 1.1834 (1.1936)\tPrec@1 54.688 (56.890)\n",
      "Epoch: [2][275/391]\tLoss 1.2743 (1.1758)\tPrec@1 55.469 (57.657)\n",
      "Epoch: [2][330/391]\tLoss 1.2347 (1.1608)\tPrec@1 58.594 (58.405)\n",
      "Epoch: [2][385/391]\tLoss 1.0890 (1.1431)\tPrec@1 63.281 (59.092)\n",
      "Test\t  Prec@1: 61.340 (Err: 38.660 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.1142 (1.1142)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [3][55/391]\tLoss 1.0879 (1.0154)\tPrec@1 61.719 (64.216)\n",
      "Epoch: [3][110/391]\tLoss 1.1128 (0.9979)\tPrec@1 60.938 (64.773)\n",
      "Epoch: [3][165/391]\tLoss 1.0278 (0.9848)\tPrec@1 64.844 (65.281)\n",
      "Epoch: [3][220/391]\tLoss 1.0935 (0.9792)\tPrec@1 59.375 (65.480)\n",
      "Epoch: [3][275/391]\tLoss 0.8038 (0.9701)\tPrec@1 68.750 (65.789)\n",
      "Epoch: [3][330/391]\tLoss 1.0201 (0.9609)\tPrec@1 63.281 (66.121)\n",
      "Epoch: [3][385/391]\tLoss 1.0441 (0.9524)\tPrec@1 60.938 (66.420)\n",
      "Test\t  Prec@1: 60.890 (Err: 39.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.7176 (0.7176)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [4][55/391]\tLoss 0.8368 (0.8382)\tPrec@1 71.875 (70.187)\n",
      "Epoch: [4][110/391]\tLoss 0.8790 (0.8329)\tPrec@1 67.969 (70.601)\n",
      "Epoch: [4][165/391]\tLoss 0.9154 (0.8316)\tPrec@1 67.969 (70.731)\n",
      "Epoch: [4][220/391]\tLoss 0.8406 (0.8258)\tPrec@1 69.531 (70.885)\n",
      "Epoch: [4][275/391]\tLoss 0.8671 (0.8214)\tPrec@1 64.844 (71.071)\n",
      "Epoch: [4][330/391]\tLoss 0.7385 (0.8134)\tPrec@1 72.656 (71.403)\n",
      "Epoch: [4][385/391]\tLoss 0.7401 (0.8057)\tPrec@1 75.781 (71.701)\n",
      "Test\t  Prec@1: 72.180 (Err: 27.820 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.8160 (0.8160)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [5][55/391]\tLoss 0.7483 (0.7196)\tPrec@1 74.219 (74.874)\n",
      "Epoch: [5][110/391]\tLoss 0.6509 (0.7300)\tPrec@1 73.438 (75.014)\n",
      "Epoch: [5][165/391]\tLoss 0.7174 (0.7269)\tPrec@1 74.219 (74.976)\n",
      "Epoch: [5][220/391]\tLoss 0.8339 (0.7257)\tPrec@1 66.406 (74.982)\n",
      "Epoch: [5][275/391]\tLoss 0.6924 (0.7204)\tPrec@1 74.219 (75.187)\n",
      "Epoch: [5][330/391]\tLoss 0.5433 (0.7194)\tPrec@1 82.031 (75.220)\n",
      "Epoch: [5][385/391]\tLoss 0.6450 (0.7137)\tPrec@1 77.344 (75.393)\n",
      "Test\t  Prec@1: 76.110 (Err: 23.890 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.7274 (0.7274)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [6][55/391]\tLoss 0.6281 (0.6571)\tPrec@1 77.344 (77.218)\n",
      "Epoch: [6][110/391]\tLoss 0.7972 (0.6554)\tPrec@1 71.094 (77.280)\n",
      "Epoch: [6][165/391]\tLoss 0.6072 (0.6542)\tPrec@1 80.469 (77.367)\n",
      "Epoch: [6][220/391]\tLoss 0.5183 (0.6486)\tPrec@1 84.375 (77.669)\n",
      "Epoch: [6][275/391]\tLoss 0.6642 (0.6515)\tPrec@1 75.781 (77.505)\n",
      "Epoch: [6][330/391]\tLoss 0.7471 (0.6513)\tPrec@1 71.094 (77.511)\n",
      "Epoch: [6][385/391]\tLoss 0.6755 (0.6443)\tPrec@1 78.906 (77.769)\n",
      "Test\t  Prec@1: 75.540 (Err: 24.460 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.5567 (0.5567)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [7][55/391]\tLoss 0.5100 (0.6094)\tPrec@1 87.500 (78.823)\n",
      "Epoch: [7][110/391]\tLoss 0.6434 (0.6018)\tPrec@1 78.125 (78.836)\n",
      "Epoch: [7][165/391]\tLoss 0.7396 (0.6067)\tPrec@1 75.000 (78.789)\n",
      "Epoch: [7][220/391]\tLoss 0.4108 (0.6043)\tPrec@1 84.375 (78.959)\n",
      "Epoch: [7][275/391]\tLoss 0.4062 (0.6008)\tPrec@1 85.156 (79.178)\n",
      "Epoch: [7][330/391]\tLoss 0.6229 (0.5963)\tPrec@1 78.906 (79.315)\n",
      "Epoch: [7][385/391]\tLoss 0.5523 (0.5940)\tPrec@1 85.156 (79.453)\n",
      "Test\t  Prec@1: 76.830 (Err: 23.170 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.4507 (0.4507)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [8][55/391]\tLoss 0.5130 (0.5541)\tPrec@1 79.688 (80.706)\n",
      "Epoch: [8][110/391]\tLoss 0.6295 (0.5531)\tPrec@1 77.344 (80.652)\n",
      "Epoch: [8][165/391]\tLoss 0.4232 (0.5525)\tPrec@1 82.812 (80.681)\n",
      "Epoch: [8][220/391]\tLoss 0.5395 (0.5513)\tPrec@1 82.812 (80.677)\n",
      "Epoch: [8][275/391]\tLoss 0.3687 (0.5528)\tPrec@1 85.938 (80.675)\n",
      "Epoch: [8][330/391]\tLoss 0.6759 (0.5521)\tPrec@1 76.562 (80.776)\n",
      "Epoch: [8][385/391]\tLoss 0.4308 (0.5498)\tPrec@1 82.031 (80.855)\n",
      "Test\t  Prec@1: 79.900 (Err: 20.100 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.4559 (0.4559)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [9][55/391]\tLoss 0.5552 (0.5066)\tPrec@1 79.688 (82.617)\n",
      "Epoch: [9][110/391]\tLoss 0.5377 (0.5162)\tPrec@1 79.688 (82.320)\n",
      "Epoch: [9][165/391]\tLoss 0.4313 (0.5187)\tPrec@1 84.375 (82.154)\n",
      "Epoch: [9][220/391]\tLoss 0.6368 (0.5145)\tPrec@1 75.000 (82.293)\n",
      "Epoch: [9][275/391]\tLoss 0.5262 (0.5163)\tPrec@1 81.250 (82.167)\n",
      "Epoch: [9][330/391]\tLoss 0.5620 (0.5168)\tPrec@1 82.031 (82.156)\n",
      "Epoch: [9][385/391]\tLoss 0.4808 (0.5153)\tPrec@1 82.812 (82.222)\n",
      "Test\t  Prec@1: 77.130 (Err: 22.870 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4580 (0.4580)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.4131 (0.4766)\tPrec@1 87.500 (83.733)\n",
      "Epoch: [10][110/391]\tLoss 0.4089 (0.4833)\tPrec@1 85.938 (83.249)\n",
      "Epoch: [10][165/391]\tLoss 0.5146 (0.4871)\tPrec@1 83.594 (82.973)\n",
      "Epoch: [10][220/391]\tLoss 0.5676 (0.4873)\tPrec@1 78.125 (82.855)\n",
      "Epoch: [10][275/391]\tLoss 0.5460 (0.4855)\tPrec@1 82.812 (82.963)\n",
      "Epoch: [10][330/391]\tLoss 0.4916 (0.4877)\tPrec@1 82.812 (82.940)\n",
      "Epoch: [10][385/391]\tLoss 0.4909 (0.4876)\tPrec@1 82.031 (82.976)\n",
      "Test\t  Prec@1: 78.330 (Err: 21.670 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.5866 (0.5866)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [11][55/391]\tLoss 0.6363 (0.4541)\tPrec@1 78.906 (84.124)\n",
      "Epoch: [11][110/391]\tLoss 0.5787 (0.4550)\tPrec@1 81.250 (84.347)\n",
      "Epoch: [11][165/391]\tLoss 0.4992 (0.4619)\tPrec@1 85.938 (84.055)\n",
      "Epoch: [11][220/391]\tLoss 0.4627 (0.4630)\tPrec@1 79.688 (83.947)\n",
      "Epoch: [11][275/391]\tLoss 0.4798 (0.4665)\tPrec@1 83.594 (83.812)\n",
      "Epoch: [11][330/391]\tLoss 0.3549 (0.4640)\tPrec@1 88.281 (83.903)\n",
      "Epoch: [11][385/391]\tLoss 0.5278 (0.4631)\tPrec@1 81.250 (83.934)\n",
      "Test\t  Prec@1: 79.890 (Err: 20.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.4564 (0.4564)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [12][55/391]\tLoss 0.3342 (0.4441)\tPrec@1 87.500 (84.124)\n",
      "Epoch: [12][110/391]\tLoss 0.4611 (0.4480)\tPrec@1 85.938 (84.213)\n",
      "Epoch: [12][165/391]\tLoss 0.4350 (0.4546)\tPrec@1 82.812 (84.003)\n",
      "Epoch: [12][220/391]\tLoss 0.5660 (0.4594)\tPrec@1 78.125 (83.958)\n",
      "Epoch: [12][275/391]\tLoss 0.5048 (0.4574)\tPrec@1 81.250 (84.047)\n",
      "Epoch: [12][330/391]\tLoss 0.4104 (0.4538)\tPrec@1 85.938 (84.215)\n",
      "Epoch: [12][385/391]\tLoss 0.4093 (0.4503)\tPrec@1 85.938 (84.349)\n",
      "Test\t  Prec@1: 81.220 (Err: 18.780 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.4098 (0.4098)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [13][55/391]\tLoss 0.4210 (0.4245)\tPrec@1 85.938 (85.407)\n",
      "Epoch: [13][110/391]\tLoss 0.3823 (0.4246)\tPrec@1 86.719 (85.410)\n",
      "Epoch: [13][165/391]\tLoss 0.4994 (0.4217)\tPrec@1 85.156 (85.472)\n",
      "Epoch: [13][220/391]\tLoss 0.5407 (0.4209)\tPrec@1 82.031 (85.393)\n",
      "Epoch: [13][275/391]\tLoss 0.4742 (0.4208)\tPrec@1 80.469 (85.422)\n",
      "Epoch: [13][330/391]\tLoss 0.4703 (0.4229)\tPrec@1 82.812 (85.350)\n",
      "Epoch: [13][385/391]\tLoss 0.3737 (0.4257)\tPrec@1 85.156 (85.209)\n",
      "Test\t  Prec@1: 79.910 (Err: 20.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3278 (0.3278)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [14][55/391]\tLoss 0.3389 (0.4046)\tPrec@1 88.281 (86.147)\n",
      "Epoch: [14][110/391]\tLoss 0.5695 (0.4007)\tPrec@1 82.812 (86.289)\n",
      "Epoch: [14][165/391]\tLoss 0.5230 (0.4034)\tPrec@1 81.250 (86.069)\n",
      "Epoch: [14][220/391]\tLoss 0.4928 (0.4064)\tPrec@1 81.250 (85.938)\n",
      "Epoch: [14][275/391]\tLoss 0.4870 (0.4095)\tPrec@1 81.250 (85.804)\n",
      "Epoch: [14][330/391]\tLoss 0.3981 (0.4096)\tPrec@1 83.594 (85.784)\n",
      "Epoch: [14][385/391]\tLoss 0.4779 (0.4123)\tPrec@1 79.688 (85.689)\n",
      "Test\t  Prec@1: 83.040 (Err: 16.960 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3665 (0.3665)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [15][55/391]\tLoss 0.2819 (0.3882)\tPrec@1 88.281 (86.663)\n",
      "Epoch: [15][110/391]\tLoss 0.4127 (0.3871)\tPrec@1 85.156 (86.719)\n",
      "Epoch: [15][165/391]\tLoss 0.5504 (0.3855)\tPrec@1 78.125 (86.611)\n",
      "Epoch: [15][220/391]\tLoss 0.6283 (0.3910)\tPrec@1 78.125 (86.362)\n",
      "Epoch: [15][275/391]\tLoss 0.3656 (0.3934)\tPrec@1 86.719 (86.280)\n",
      "Epoch: [15][330/391]\tLoss 0.3560 (0.3939)\tPrec@1 86.719 (86.303)\n",
      "Epoch: [15][385/391]\tLoss 0.3565 (0.3986)\tPrec@1 86.719 (86.207)\n",
      "Test\t  Prec@1: 82.480 (Err: 17.520 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.2887 (0.2887)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [16][55/391]\tLoss 0.3306 (0.3697)\tPrec@1 85.938 (86.956)\n",
      "Epoch: [16][110/391]\tLoss 0.4107 (0.3710)\tPrec@1 85.938 (86.937)\n",
      "Epoch: [16][165/391]\tLoss 0.2606 (0.3700)\tPrec@1 91.406 (86.963)\n",
      "Epoch: [16][220/391]\tLoss 0.5280 (0.3737)\tPrec@1 82.031 (86.860)\n",
      "Epoch: [16][275/391]\tLoss 0.4083 (0.3779)\tPrec@1 87.500 (86.741)\n",
      "Epoch: [16][330/391]\tLoss 0.4773 (0.3817)\tPrec@1 86.719 (86.648)\n",
      "Epoch: [16][385/391]\tLoss 0.3000 (0.3844)\tPrec@1 92.188 (86.551)\n",
      "Test\t  Prec@1: 81.590 (Err: 18.410 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.3533 (0.3533)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [17][55/391]\tLoss 0.4820 (0.3742)\tPrec@1 83.594 (87.040)\n",
      "Epoch: [17][110/391]\tLoss 0.3640 (0.3586)\tPrec@1 88.281 (87.507)\n",
      "Epoch: [17][165/391]\tLoss 0.2846 (0.3649)\tPrec@1 89.062 (87.232)\n",
      "Epoch: [17][220/391]\tLoss 0.4575 (0.3724)\tPrec@1 82.812 (86.949)\n",
      "Epoch: [17][275/391]\tLoss 0.5135 (0.3742)\tPrec@1 82.031 (86.920)\n",
      "Epoch: [17][330/391]\tLoss 0.2747 (0.3744)\tPrec@1 92.188 (86.934)\n",
      "Epoch: [17][385/391]\tLoss 0.4132 (0.3760)\tPrec@1 88.281 (86.907)\n",
      "Test\t  Prec@1: 80.690 (Err: 19.310 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3336 (0.3336)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [18][55/391]\tLoss 0.4274 (0.3660)\tPrec@1 87.500 (87.333)\n",
      "Epoch: [18][110/391]\tLoss 0.3925 (0.3681)\tPrec@1 88.281 (87.190)\n",
      "Epoch: [18][165/391]\tLoss 0.4131 (0.3663)\tPrec@1 82.812 (87.307)\n",
      "Epoch: [18][220/391]\tLoss 0.3891 (0.3658)\tPrec@1 83.594 (87.203)\n",
      "Epoch: [18][275/391]\tLoss 0.3550 (0.3663)\tPrec@1 88.281 (87.225)\n",
      "Epoch: [18][330/391]\tLoss 0.3345 (0.3663)\tPrec@1 86.719 (87.198)\n",
      "Epoch: [18][385/391]\tLoss 0.5526 (0.3691)\tPrec@1 80.469 (87.182)\n",
      "Test\t  Prec@1: 84.460 (Err: 15.540 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3528 (0.3528)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [19][55/391]\tLoss 0.4343 (0.3373)\tPrec@1 88.281 (88.421)\n",
      "Epoch: [19][110/391]\tLoss 0.4140 (0.3472)\tPrec@1 86.719 (87.986)\n",
      "Epoch: [19][165/391]\tLoss 0.3150 (0.3456)\tPrec@1 85.938 (88.065)\n",
      "Epoch: [19][220/391]\tLoss 0.4725 (0.3462)\tPrec@1 82.031 (88.002)\n",
      "Epoch: [19][275/391]\tLoss 0.3387 (0.3470)\tPrec@1 89.062 (88.029)\n",
      "Epoch: [19][330/391]\tLoss 0.3307 (0.3487)\tPrec@1 88.281 (87.939)\n",
      "Epoch: [19][385/391]\tLoss 0.5151 (0.3527)\tPrec@1 82.812 (87.775)\n",
      "Test\t  Prec@1: 82.790 (Err: 17.210 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.3050 (0.3050)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [20][55/391]\tLoss 0.3422 (0.3405)\tPrec@1 85.938 (88.267)\n",
      "Epoch: [20][110/391]\tLoss 0.3407 (0.3353)\tPrec@1 88.281 (88.471)\n",
      "Epoch: [20][165/391]\tLoss 0.4035 (0.3337)\tPrec@1 85.156 (88.432)\n",
      "Epoch: [20][220/391]\tLoss 0.3023 (0.3364)\tPrec@1 87.500 (88.218)\n",
      "Epoch: [20][275/391]\tLoss 0.3773 (0.3399)\tPrec@1 84.375 (88.134)\n",
      "Epoch: [20][330/391]\tLoss 0.3168 (0.3409)\tPrec@1 88.281 (88.085)\n",
      "Epoch: [20][385/391]\tLoss 0.5271 (0.3426)\tPrec@1 79.688 (88.024)\n",
      "Test\t  Prec@1: 84.480 (Err: 15.520 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.2837 (0.2837)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [21][55/391]\tLoss 0.3296 (0.3289)\tPrec@1 86.719 (88.351)\n",
      "Epoch: [21][110/391]\tLoss 0.2607 (0.3390)\tPrec@1 93.750 (88.309)\n",
      "Epoch: [21][165/391]\tLoss 0.3776 (0.3362)\tPrec@1 85.156 (88.352)\n",
      "Epoch: [21][220/391]\tLoss 0.4336 (0.3324)\tPrec@1 85.156 (88.529)\n",
      "Epoch: [21][275/391]\tLoss 0.2853 (0.3336)\tPrec@1 91.406 (88.434)\n",
      "Epoch: [21][330/391]\tLoss 0.4111 (0.3351)\tPrec@1 83.594 (88.319)\n",
      "Epoch: [21][385/391]\tLoss 0.4387 (0.3388)\tPrec@1 85.938 (88.216)\n",
      "Test\t  Prec@1: 85.160 (Err: 14.840 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.2355 (0.2355)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [22][55/391]\tLoss 0.2654 (0.2996)\tPrec@1 90.625 (89.593)\n",
      "Epoch: [22][110/391]\tLoss 0.4556 (0.3090)\tPrec@1 82.812 (89.070)\n",
      "Epoch: [22][165/391]\tLoss 0.3823 (0.3140)\tPrec@1 85.156 (89.001)\n",
      "Epoch: [22][220/391]\tLoss 0.3570 (0.3242)\tPrec@1 89.062 (88.755)\n",
      "Epoch: [22][275/391]\tLoss 0.2721 (0.3273)\tPrec@1 91.406 (88.578)\n",
      "Epoch: [22][330/391]\tLoss 0.2607 (0.3308)\tPrec@1 92.188 (88.432)\n",
      "Epoch: [22][385/391]\tLoss 0.2564 (0.3301)\tPrec@1 89.062 (88.469)\n",
      "Test\t  Prec@1: 83.170 (Err: 16.830 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2444 (0.2444)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [23][55/391]\tLoss 0.2255 (0.3132)\tPrec@1 92.969 (89.369)\n",
      "Epoch: [23][110/391]\tLoss 0.2375 (0.3151)\tPrec@1 91.406 (89.084)\n",
      "Epoch: [23][165/391]\tLoss 0.2702 (0.3128)\tPrec@1 89.844 (89.147)\n",
      "Epoch: [23][220/391]\tLoss 0.3446 (0.3168)\tPrec@1 85.938 (89.020)\n",
      "Epoch: [23][275/391]\tLoss 0.3189 (0.3182)\tPrec@1 85.156 (88.918)\n",
      "Epoch: [23][330/391]\tLoss 0.3107 (0.3187)\tPrec@1 91.406 (88.954)\n",
      "Epoch: [23][385/391]\tLoss 0.4165 (0.3171)\tPrec@1 85.156 (88.907)\n",
      "Test\t  Prec@1: 83.910 (Err: 16.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.4183 (0.4183)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [24][55/391]\tLoss 0.2707 (0.3234)\tPrec@1 89.062 (88.574)\n",
      "Epoch: [24][110/391]\tLoss 0.3493 (0.3214)\tPrec@1 88.281 (88.718)\n",
      "Epoch: [24][165/391]\tLoss 0.3356 (0.3190)\tPrec@1 89.062 (89.034)\n",
      "Epoch: [24][220/391]\tLoss 0.3152 (0.3199)\tPrec@1 89.844 (89.045)\n",
      "Epoch: [24][275/391]\tLoss 0.3269 (0.3200)\tPrec@1 89.062 (88.935)\n",
      "Epoch: [24][330/391]\tLoss 0.2824 (0.3196)\tPrec@1 93.750 (88.923)\n",
      "Epoch: [24][385/391]\tLoss 0.3513 (0.3219)\tPrec@1 85.938 (88.828)\n",
      "Test\t  Prec@1: 85.760 (Err: 14.240 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2743 (0.2743)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [25][55/391]\tLoss 0.2968 (0.2961)\tPrec@1 89.844 (89.704)\n",
      "Epoch: [25][110/391]\tLoss 0.5021 (0.2986)\tPrec@1 76.562 (89.583)\n",
      "Epoch: [25][165/391]\tLoss 0.2730 (0.3024)\tPrec@1 91.406 (89.575)\n",
      "Epoch: [25][220/391]\tLoss 0.3772 (0.3050)\tPrec@1 85.938 (89.359)\n",
      "Epoch: [25][275/391]\tLoss 0.2961 (0.3081)\tPrec@1 89.844 (89.275)\n",
      "Epoch: [25][330/391]\tLoss 0.2972 (0.3098)\tPrec@1 91.406 (89.195)\n",
      "Epoch: [25][385/391]\tLoss 0.3666 (0.3108)\tPrec@1 88.281 (89.176)\n",
      "Test\t  Prec@1: 82.870 (Err: 17.130 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.3593 (0.3593)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [26][55/391]\tLoss 0.2150 (0.2808)\tPrec@1 92.969 (90.109)\n",
      "Epoch: [26][110/391]\tLoss 0.3442 (0.2926)\tPrec@1 89.844 (89.569)\n",
      "Epoch: [26][165/391]\tLoss 0.2720 (0.3011)\tPrec@1 91.406 (89.406)\n",
      "Epoch: [26][220/391]\tLoss 0.2443 (0.3016)\tPrec@1 90.625 (89.412)\n",
      "Epoch: [26][275/391]\tLoss 0.2732 (0.3024)\tPrec@1 89.062 (89.365)\n",
      "Epoch: [26][330/391]\tLoss 0.2852 (0.3051)\tPrec@1 89.062 (89.313)\n",
      "Epoch: [26][385/391]\tLoss 0.4476 (0.3043)\tPrec@1 85.156 (89.328)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.2404 (0.2404)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [27][55/391]\tLoss 0.3229 (0.2820)\tPrec@1 90.625 (90.137)\n",
      "Epoch: [27][110/391]\tLoss 0.3757 (0.2844)\tPrec@1 84.375 (90.020)\n",
      "Epoch: [27][165/391]\tLoss 0.2538 (0.2871)\tPrec@1 90.625 (89.867)\n",
      "Epoch: [27][220/391]\tLoss 0.3616 (0.2921)\tPrec@1 85.156 (89.671)\n",
      "Epoch: [27][275/391]\tLoss 0.2944 (0.2969)\tPrec@1 90.625 (89.524)\n",
      "Epoch: [27][330/391]\tLoss 0.3420 (0.2972)\tPrec@1 89.062 (89.577)\n",
      "Epoch: [27][385/391]\tLoss 0.2880 (0.2969)\tPrec@1 91.406 (89.591)\n",
      "Test\t  Prec@1: 84.580 (Err: 15.420 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.1668 (0.1668)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [28][55/391]\tLoss 0.3427 (0.2851)\tPrec@1 87.500 (90.179)\n",
      "Epoch: [28][110/391]\tLoss 0.2398 (0.2829)\tPrec@1 92.969 (90.259)\n",
      "Epoch: [28][165/391]\tLoss 0.2818 (0.2812)\tPrec@1 90.625 (90.310)\n",
      "Epoch: [28][220/391]\tLoss 0.1845 (0.2867)\tPrec@1 93.750 (90.084)\n",
      "Epoch: [28][275/391]\tLoss 0.2826 (0.2881)\tPrec@1 89.844 (90.056)\n",
      "Epoch: [28][330/391]\tLoss 0.3952 (0.2916)\tPrec@1 87.500 (89.974)\n",
      "Epoch: [28][385/391]\tLoss 0.3115 (0.2935)\tPrec@1 88.281 (89.890)\n",
      "Test\t  Prec@1: 84.980 (Err: 15.020 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2833 (0.2833)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [29][55/391]\tLoss 0.3609 (0.2804)\tPrec@1 84.375 (90.318)\n",
      "Epoch: [29][110/391]\tLoss 0.2890 (0.2812)\tPrec@1 89.844 (90.132)\n",
      "Epoch: [29][165/391]\tLoss 0.3362 (0.2783)\tPrec@1 89.062 (90.239)\n",
      "Epoch: [29][220/391]\tLoss 0.3588 (0.2856)\tPrec@1 85.938 (90.059)\n",
      "Epoch: [29][275/391]\tLoss 0.2723 (0.2901)\tPrec@1 92.188 (90.002)\n",
      "Epoch: [29][330/391]\tLoss 0.3303 (0.2886)\tPrec@1 87.500 (90.040)\n",
      "Epoch: [29][385/391]\tLoss 0.4446 (0.2905)\tPrec@1 85.156 (89.967)\n",
      "Test\t  Prec@1: 84.120 (Err: 15.880 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2660 (0.2660)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [30][55/391]\tLoss 0.2787 (0.2518)\tPrec@1 89.062 (91.309)\n",
      "Epoch: [30][110/391]\tLoss 0.2638 (0.2657)\tPrec@1 92.188 (90.653)\n",
      "Epoch: [30][165/391]\tLoss 0.2418 (0.2706)\tPrec@1 89.844 (90.550)\n",
      "Epoch: [30][220/391]\tLoss 0.3204 (0.2730)\tPrec@1 89.844 (90.487)\n",
      "Epoch: [30][275/391]\tLoss 0.1999 (0.2777)\tPrec@1 93.750 (90.285)\n",
      "Epoch: [30][330/391]\tLoss 0.3177 (0.2782)\tPrec@1 86.719 (90.287)\n",
      "Epoch: [30][385/391]\tLoss 0.2560 (0.2792)\tPrec@1 92.188 (90.279)\n",
      "Test\t  Prec@1: 84.470 (Err: 15.530 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.3828 (0.3828)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [31][55/391]\tLoss 0.3352 (0.2556)\tPrec@1 88.281 (91.030)\n",
      "Epoch: [31][110/391]\tLoss 0.2698 (0.2599)\tPrec@1 92.969 (90.759)\n",
      "Epoch: [31][165/391]\tLoss 0.4100 (0.2667)\tPrec@1 85.938 (90.644)\n",
      "Epoch: [31][220/391]\tLoss 0.2630 (0.2674)\tPrec@1 89.062 (90.568)\n",
      "Epoch: [31][275/391]\tLoss 0.2953 (0.2715)\tPrec@1 89.844 (90.492)\n",
      "Epoch: [31][330/391]\tLoss 0.1450 (0.2766)\tPrec@1 96.094 (90.370)\n",
      "Epoch: [31][385/391]\tLoss 0.4297 (0.2779)\tPrec@1 82.031 (90.384)\n",
      "Test\t  Prec@1: 84.040 (Err: 15.960 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.2916 (0.2916)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [32][55/391]\tLoss 0.1985 (0.2570)\tPrec@1 92.188 (90.834)\n",
      "Epoch: [32][110/391]\tLoss 0.3156 (0.2735)\tPrec@1 87.500 (90.484)\n",
      "Epoch: [32][165/391]\tLoss 0.3249 (0.2786)\tPrec@1 89.844 (90.258)\n",
      "Epoch: [32][220/391]\tLoss 0.2175 (0.2775)\tPrec@1 93.750 (90.339)\n",
      "Epoch: [32][275/391]\tLoss 0.2457 (0.2776)\tPrec@1 92.188 (90.243)\n",
      "Epoch: [32][330/391]\tLoss 0.3579 (0.2775)\tPrec@1 86.719 (90.245)\n",
      "Epoch: [32][385/391]\tLoss 0.2162 (0.2788)\tPrec@1 92.969 (90.180)\n",
      "Test\t  Prec@1: 83.870 (Err: 16.130 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.1720 (0.1720)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [33][55/391]\tLoss 0.2362 (0.2658)\tPrec@1 91.406 (90.639)\n",
      "Epoch: [33][110/391]\tLoss 0.2795 (0.2699)\tPrec@1 89.062 (90.393)\n",
      "Epoch: [33][165/391]\tLoss 0.2216 (0.2648)\tPrec@1 91.406 (90.606)\n",
      "Epoch: [33][220/391]\tLoss 0.2972 (0.2685)\tPrec@1 89.844 (90.586)\n",
      "Epoch: [33][275/391]\tLoss 0.2439 (0.2739)\tPrec@1 90.625 (90.481)\n",
      "Epoch: [33][330/391]\tLoss 0.2340 (0.2770)\tPrec@1 90.625 (90.365)\n",
      "Epoch: [33][385/391]\tLoss 0.3742 (0.2756)\tPrec@1 89.844 (90.459)\n",
      "Test\t  Prec@1: 83.530 (Err: 16.470 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.1902 (0.1902)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [34][55/391]\tLoss 0.2408 (0.2632)\tPrec@1 88.281 (90.569)\n",
      "Epoch: [34][110/391]\tLoss 0.2308 (0.2614)\tPrec@1 95.312 (90.738)\n",
      "Epoch: [34][165/391]\tLoss 0.2392 (0.2674)\tPrec@1 90.625 (90.536)\n",
      "Epoch: [34][220/391]\tLoss 0.2846 (0.2749)\tPrec@1 86.719 (90.296)\n",
      "Epoch: [34][275/391]\tLoss 0.2598 (0.2728)\tPrec@1 89.844 (90.384)\n",
      "Epoch: [34][330/391]\tLoss 0.3095 (0.2740)\tPrec@1 89.062 (90.417)\n",
      "Epoch: [34][385/391]\tLoss 0.2931 (0.2729)\tPrec@1 88.281 (90.487)\n",
      "Test\t  Prec@1: 83.910 (Err: 16.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.1858 (0.1858)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [35][55/391]\tLoss 0.3438 (0.2499)\tPrec@1 91.406 (91.420)\n",
      "Epoch: [35][110/391]\tLoss 0.1806 (0.2426)\tPrec@1 93.750 (91.716)\n",
      "Epoch: [35][165/391]\tLoss 0.2293 (0.2426)\tPrec@1 92.969 (91.665)\n",
      "Epoch: [35][220/391]\tLoss 0.1717 (0.2494)\tPrec@1 94.531 (91.374)\n",
      "Epoch: [35][275/391]\tLoss 0.2433 (0.2533)\tPrec@1 92.188 (91.168)\n",
      "Epoch: [35][330/391]\tLoss 0.2417 (0.2571)\tPrec@1 87.500 (91.010)\n",
      "Epoch: [35][385/391]\tLoss 0.1561 (0.2591)\tPrec@1 96.094 (90.941)\n",
      "Test\t  Prec@1: 85.590 (Err: 14.410 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2007 (0.2007)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [36][55/391]\tLoss 0.2534 (0.2391)\tPrec@1 89.062 (91.741)\n",
      "Epoch: [36][110/391]\tLoss 0.1648 (0.2402)\tPrec@1 94.531 (91.681)\n",
      "Epoch: [36][165/391]\tLoss 0.2590 (0.2491)\tPrec@1 91.406 (91.350)\n",
      "Epoch: [36][220/391]\tLoss 0.2312 (0.2514)\tPrec@1 92.969 (91.332)\n",
      "Epoch: [36][275/391]\tLoss 0.3297 (0.2539)\tPrec@1 85.938 (91.217)\n",
      "Epoch: [36][330/391]\tLoss 0.3065 (0.2570)\tPrec@1 91.406 (91.090)\n",
      "Epoch: [36][385/391]\tLoss 0.3012 (0.2599)\tPrec@1 89.062 (90.985)\n",
      "Test\t  Prec@1: 86.750 (Err: 13.250 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.2990 (0.2990)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [37][55/391]\tLoss 0.2792 (0.2474)\tPrec@1 89.062 (91.546)\n",
      "Epoch: [37][110/391]\tLoss 0.2289 (0.2457)\tPrec@1 92.188 (91.484)\n",
      "Epoch: [37][165/391]\tLoss 0.2309 (0.2513)\tPrec@1 90.625 (91.331)\n",
      "Epoch: [37][220/391]\tLoss 0.2618 (0.2540)\tPrec@1 89.062 (91.251)\n",
      "Epoch: [37][275/391]\tLoss 0.2606 (0.2544)\tPrec@1 90.625 (91.177)\n",
      "Epoch: [37][330/391]\tLoss 0.2255 (0.2548)\tPrec@1 90.625 (91.196)\n",
      "Epoch: [37][385/391]\tLoss 0.2016 (0.2545)\tPrec@1 93.750 (91.182)\n",
      "Test\t  Prec@1: 82.360 (Err: 17.640 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.3108 (0.3108)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.2647 (0.2416)\tPrec@1 90.625 (91.546)\n",
      "Epoch: [38][110/391]\tLoss 0.2728 (0.2589)\tPrec@1 88.281 (90.970)\n",
      "Epoch: [38][165/391]\tLoss 0.1842 (0.2606)\tPrec@1 95.312 (90.945)\n",
      "Epoch: [38][220/391]\tLoss 0.2224 (0.2575)\tPrec@1 93.750 (91.067)\n",
      "Epoch: [38][275/391]\tLoss 0.2391 (0.2586)\tPrec@1 92.188 (91.098)\n",
      "Epoch: [38][330/391]\tLoss 0.2548 (0.2609)\tPrec@1 91.406 (90.996)\n",
      "Epoch: [38][385/391]\tLoss 0.3158 (0.2621)\tPrec@1 84.375 (90.933)\n",
      "Test\t  Prec@1: 86.930 (Err: 13.070 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2072 (0.2072)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [39][55/391]\tLoss 0.1355 (0.2352)\tPrec@1 96.875 (91.685)\n",
      "Epoch: [39][110/391]\tLoss 0.2806 (0.2412)\tPrec@1 89.062 (91.498)\n",
      "Epoch: [39][165/391]\tLoss 0.3172 (0.2445)\tPrec@1 89.844 (91.467)\n",
      "Epoch: [39][220/391]\tLoss 0.2318 (0.2511)\tPrec@1 90.625 (91.268)\n",
      "Epoch: [39][275/391]\tLoss 0.1982 (0.2492)\tPrec@1 92.188 (91.319)\n",
      "Epoch: [39][330/391]\tLoss 0.1809 (0.2500)\tPrec@1 93.750 (91.302)\n",
      "Epoch: [39][385/391]\tLoss 0.2133 (0.2501)\tPrec@1 92.969 (91.283)\n",
      "Test\t  Prec@1: 84.180 (Err: 15.820 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.3044 (0.3044)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [40][55/391]\tLoss 0.2094 (0.2322)\tPrec@1 92.969 (91.895)\n",
      "Epoch: [40][110/391]\tLoss 0.2516 (0.2277)\tPrec@1 92.188 (92.033)\n",
      "Epoch: [40][165/391]\tLoss 0.2230 (0.2310)\tPrec@1 90.625 (91.839)\n",
      "Epoch: [40][220/391]\tLoss 0.1951 (0.2392)\tPrec@1 92.969 (91.583)\n",
      "Epoch: [40][275/391]\tLoss 0.2129 (0.2408)\tPrec@1 92.188 (91.502)\n",
      "Epoch: [40][330/391]\tLoss 0.3048 (0.2452)\tPrec@1 87.500 (91.449)\n",
      "Epoch: [40][385/391]\tLoss 0.2698 (0.2488)\tPrec@1 90.625 (91.315)\n",
      "Test\t  Prec@1: 84.800 (Err: 15.200 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2236 (0.2236)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [41][55/391]\tLoss 0.3017 (0.2338)\tPrec@1 92.188 (91.950)\n",
      "Epoch: [41][110/391]\tLoss 0.3068 (0.2397)\tPrec@1 88.281 (91.512)\n",
      "Epoch: [41][165/391]\tLoss 0.2561 (0.2449)\tPrec@1 92.188 (91.307)\n",
      "Epoch: [41][220/391]\tLoss 0.2031 (0.2409)\tPrec@1 93.750 (91.484)\n",
      "Epoch: [41][275/391]\tLoss 0.3320 (0.2416)\tPrec@1 87.500 (91.460)\n",
      "Epoch: [41][330/391]\tLoss 0.2945 (0.2438)\tPrec@1 91.406 (91.409)\n",
      "Epoch: [41][385/391]\tLoss 0.2903 (0.2495)\tPrec@1 90.625 (91.250)\n",
      "Test\t  Prec@1: 85.910 (Err: 14.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2072 (0.2072)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [42][55/391]\tLoss 0.3465 (0.2213)\tPrec@1 89.844 (92.550)\n",
      "Epoch: [42][110/391]\tLoss 0.1676 (0.2217)\tPrec@1 94.531 (92.490)\n",
      "Epoch: [42][165/391]\tLoss 0.2569 (0.2321)\tPrec@1 89.844 (92.037)\n",
      "Epoch: [42][220/391]\tLoss 0.2057 (0.2426)\tPrec@1 92.969 (91.678)\n",
      "Epoch: [42][275/391]\tLoss 0.1595 (0.2413)\tPrec@1 92.188 (91.689)\n",
      "Epoch: [42][330/391]\tLoss 0.2607 (0.2432)\tPrec@1 89.062 (91.548)\n",
      "Epoch: [42][385/391]\tLoss 0.2274 (0.2464)\tPrec@1 93.750 (91.410)\n",
      "Test\t  Prec@1: 84.680 (Err: 15.320 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.2664 (0.2664)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [43][55/391]\tLoss 0.2508 (0.2341)\tPrec@1 89.844 (91.867)\n",
      "Epoch: [43][110/391]\tLoss 0.1900 (0.2355)\tPrec@1 93.750 (91.857)\n",
      "Epoch: [43][165/391]\tLoss 0.3290 (0.2349)\tPrec@1 89.062 (91.783)\n",
      "Epoch: [43][220/391]\tLoss 0.1675 (0.2400)\tPrec@1 92.969 (91.632)\n",
      "Epoch: [43][275/391]\tLoss 0.1968 (0.2444)\tPrec@1 92.969 (91.449)\n",
      "Epoch: [43][330/391]\tLoss 0.1969 (0.2443)\tPrec@1 93.750 (91.449)\n",
      "Epoch: [43][385/391]\tLoss 0.2923 (0.2440)\tPrec@1 89.844 (91.475)\n",
      "Test\t  Prec@1: 86.380 (Err: 13.620 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2677 (0.2677)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [44][55/391]\tLoss 0.1824 (0.2211)\tPrec@1 92.969 (92.118)\n",
      "Epoch: [44][110/391]\tLoss 0.2484 (0.2271)\tPrec@1 92.188 (92.033)\n",
      "Epoch: [44][165/391]\tLoss 0.3183 (0.2266)\tPrec@1 87.500 (92.051)\n",
      "Epoch: [44][220/391]\tLoss 0.2638 (0.2284)\tPrec@1 93.750 (91.972)\n",
      "Epoch: [44][275/391]\tLoss 0.2220 (0.2309)\tPrec@1 93.750 (91.927)\n",
      "Epoch: [44][330/391]\tLoss 0.2546 (0.2311)\tPrec@1 91.406 (91.897)\n",
      "Epoch: [44][385/391]\tLoss 0.2876 (0.2368)\tPrec@1 89.062 (91.698)\n",
      "Test\t  Prec@1: 81.620 (Err: 18.380 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.2217 (0.2217)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [45][55/391]\tLoss 0.1266 (0.2225)\tPrec@1 95.312 (92.076)\n",
      "Epoch: [45][110/391]\tLoss 0.2137 (0.2326)\tPrec@1 92.969 (91.695)\n",
      "Epoch: [45][165/391]\tLoss 0.1988 (0.2343)\tPrec@1 92.969 (91.656)\n",
      "Epoch: [45][220/391]\tLoss 0.3000 (0.2323)\tPrec@1 88.281 (91.717)\n",
      "Epoch: [45][275/391]\tLoss 0.2673 (0.2360)\tPrec@1 89.062 (91.599)\n",
      "Epoch: [45][330/391]\tLoss 0.2324 (0.2388)\tPrec@1 91.406 (91.522)\n",
      "Epoch: [45][385/391]\tLoss 0.2630 (0.2423)\tPrec@1 90.625 (91.408)\n",
      "Test\t  Prec@1: 86.890 (Err: 13.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.1203 (0.1203)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [46][55/391]\tLoss 0.1982 (0.2355)\tPrec@1 96.094 (91.992)\n",
      "Epoch: [46][110/391]\tLoss 0.2415 (0.2286)\tPrec@1 91.406 (91.927)\n",
      "Epoch: [46][165/391]\tLoss 0.3533 (0.2346)\tPrec@1 88.281 (91.675)\n",
      "Epoch: [46][220/391]\tLoss 0.2089 (0.2379)\tPrec@1 93.750 (91.558)\n",
      "Epoch: [46][275/391]\tLoss 0.1431 (0.2381)\tPrec@1 94.531 (91.585)\n",
      "Epoch: [46][330/391]\tLoss 0.2135 (0.2402)\tPrec@1 92.969 (91.503)\n",
      "Epoch: [46][385/391]\tLoss 0.3752 (0.2408)\tPrec@1 87.500 (91.469)\n",
      "Test\t  Prec@1: 84.600 (Err: 15.400 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.1969 (0.1969)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [47][55/391]\tLoss 0.1866 (0.2273)\tPrec@1 92.188 (92.229)\n",
      "Epoch: [47][110/391]\tLoss 0.2881 (0.2145)\tPrec@1 87.500 (92.575)\n",
      "Epoch: [47][165/391]\tLoss 0.2783 (0.2113)\tPrec@1 88.281 (92.578)\n",
      "Epoch: [47][220/391]\tLoss 0.2750 (0.2172)\tPrec@1 92.188 (92.431)\n",
      "Epoch: [47][275/391]\tLoss 0.1707 (0.2247)\tPrec@1 94.531 (92.159)\n",
      "Epoch: [47][330/391]\tLoss 0.2911 (0.2291)\tPrec@1 91.406 (91.982)\n",
      "Epoch: [47][385/391]\tLoss 0.3029 (0.2347)\tPrec@1 89.062 (91.744)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1869 (0.1869)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [48][55/391]\tLoss 0.2704 (0.2177)\tPrec@1 90.625 (92.467)\n",
      "Epoch: [48][110/391]\tLoss 0.3262 (0.2244)\tPrec@1 88.281 (92.216)\n",
      "Epoch: [48][165/391]\tLoss 0.2648 (0.2283)\tPrec@1 90.625 (92.046)\n",
      "Epoch: [48][220/391]\tLoss 0.1558 (0.2293)\tPrec@1 94.531 (92.011)\n",
      "Epoch: [48][275/391]\tLoss 0.2287 (0.2323)\tPrec@1 92.969 (91.921)\n",
      "Epoch: [48][330/391]\tLoss 0.1961 (0.2329)\tPrec@1 89.062 (91.859)\n",
      "Epoch: [48][385/391]\tLoss 0.2742 (0.2326)\tPrec@1 89.062 (91.892)\n",
      "Test\t  Prec@1: 86.020 (Err: 13.980 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.1959 (0.1959)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [49][55/391]\tLoss 0.1866 (0.1995)\tPrec@1 90.625 (93.164)\n",
      "Epoch: [49][110/391]\tLoss 0.1301 (0.2152)\tPrec@1 96.094 (92.469)\n",
      "Epoch: [49][165/391]\tLoss 0.2097 (0.2207)\tPrec@1 92.188 (92.376)\n",
      "Epoch: [49][220/391]\tLoss 0.1557 (0.2272)\tPrec@1 94.531 (92.173)\n",
      "Epoch: [49][275/391]\tLoss 0.2441 (0.2266)\tPrec@1 89.844 (92.193)\n",
      "Epoch: [49][330/391]\tLoss 0.2388 (0.2257)\tPrec@1 92.188 (92.225)\n",
      "Epoch: [49][385/391]\tLoss 0.2812 (0.2265)\tPrec@1 92.188 (92.198)\n",
      "Test\t  Prec@1: 86.660 (Err: 13.340 )\n",
      "\n",
      "The lowest error from resnet32 model after 50 epochs is 13.070\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet32';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-28                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-30             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-33                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-35             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-38                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-40            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-43                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-45             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-50             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-11                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-53                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-55             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-12                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-58                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-60             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-13                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-63                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-65             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-14                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-68                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-70             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-15                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-73                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-75            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-16                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-76                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-77            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-78                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-79            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-80             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-17                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-81                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-82            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-83                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-84            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-85             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-18                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-86                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-87            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-88                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-89            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-90             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-19                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-91                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-92            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-93                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-94            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-95             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-20                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-96                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-97            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-98                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-99            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-100            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-21                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-101                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-102           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-103                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-104           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-105            [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 658,586\n",
      "Trainable params: 658,586\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 98.49\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.38\n",
      "Params size (MB): 2.51\n",
      "Estimated Total Size (MB): 8.90\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet44',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 4.8021 (4.8021)\tPrec@1 10.156 (10.156)\n",
      "Epoch: [0][55/391]\tLoss 2.2239 (2.8684)\tPrec@1 10.156 (13.742)\n",
      "Epoch: [0][110/391]\tLoss 2.0390 (2.4984)\tPrec@1 23.438 (16.990)\n",
      "Epoch: [0][165/391]\tLoss 1.9911 (2.3236)\tPrec@1 24.219 (19.658)\n",
      "Epoch: [0][220/391]\tLoss 1.7376 (2.2007)\tPrec@1 32.812 (22.607)\n",
      "Epoch: [0][275/391]\tLoss 1.8931 (2.1164)\tPrec@1 32.031 (24.791)\n",
      "Epoch: [0][330/391]\tLoss 1.7547 (2.0475)\tPrec@1 32.812 (26.770)\n",
      "Epoch: [0][385/391]\tLoss 1.7116 (1.9911)\tPrec@1 43.750 (28.366)\n",
      "Test\t  Prec@1: 42.030 (Err: 57.970 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.6105 (1.6105)\tPrec@1 34.375 (34.375)\n",
      "Epoch: [1][55/391]\tLoss 1.5858 (1.5602)\tPrec@1 42.188 (42.983)\n",
      "Epoch: [1][110/391]\tLoss 1.3609 (1.5442)\tPrec@1 53.906 (43.117)\n",
      "Epoch: [1][165/391]\tLoss 1.3607 (1.5230)\tPrec@1 53.906 (43.901)\n",
      "Epoch: [1][220/391]\tLoss 1.3761 (1.4953)\tPrec@1 49.219 (45.065)\n",
      "Epoch: [1][275/391]\tLoss 1.0935 (1.4653)\tPrec@1 61.719 (46.224)\n",
      "Epoch: [1][330/391]\tLoss 1.3556 (1.4402)\tPrec@1 53.125 (47.364)\n",
      "Epoch: [1][385/391]\tLoss 1.3262 (1.4182)\tPrec@1 50.000 (48.106)\n",
      "Test\t  Prec@1: 53.790 (Err: 46.210 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.4915 (1.4915)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [2][55/391]\tLoss 1.2442 (1.2504)\tPrec@1 57.031 (54.590)\n",
      "Epoch: [2][110/391]\tLoss 0.9827 (1.2131)\tPrec@1 65.625 (56.018)\n",
      "Epoch: [2][165/391]\tLoss 1.1296 (1.1903)\tPrec@1 57.812 (57.271)\n",
      "Epoch: [2][220/391]\tLoss 0.9224 (1.1770)\tPrec@1 71.875 (57.770)\n",
      "Epoch: [2][275/391]\tLoss 1.0451 (1.1586)\tPrec@1 66.406 (58.387)\n",
      "Epoch: [2][330/391]\tLoss 0.9998 (1.1420)\tPrec@1 67.188 (58.948)\n",
      "Epoch: [2][385/391]\tLoss 1.2194 (1.1268)\tPrec@1 56.250 (59.537)\n",
      "Test\t  Prec@1: 62.230 (Err: 37.770 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.0802 (1.0802)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [3][55/391]\tLoss 0.8908 (0.9834)\tPrec@1 67.969 (64.955)\n",
      "Epoch: [3][110/391]\tLoss 0.9630 (0.9604)\tPrec@1 67.188 (66.019)\n",
      "Epoch: [3][165/391]\tLoss 0.8075 (0.9567)\tPrec@1 70.312 (66.171)\n",
      "Epoch: [3][220/391]\tLoss 0.9327 (0.9507)\tPrec@1 63.281 (66.374)\n",
      "Epoch: [3][275/391]\tLoss 0.6512 (0.9298)\tPrec@1 77.344 (67.105)\n",
      "Epoch: [3][330/391]\tLoss 0.9026 (0.9208)\tPrec@1 69.531 (67.405)\n",
      "Epoch: [3][385/391]\tLoss 0.8253 (0.9072)\tPrec@1 75.000 (67.946)\n",
      "Test\t  Prec@1: 70.500 (Err: 29.500 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.8535 (0.8535)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [4][55/391]\tLoss 0.7412 (0.8112)\tPrec@1 71.094 (71.680)\n",
      "Epoch: [4][110/391]\tLoss 0.8383 (0.7939)\tPrec@1 70.312 (72.206)\n",
      "Epoch: [4][165/391]\tLoss 0.8803 (0.7902)\tPrec@1 68.750 (72.426)\n",
      "Epoch: [4][220/391]\tLoss 0.9111 (0.7840)\tPrec@1 65.625 (72.688)\n",
      "Epoch: [4][275/391]\tLoss 0.7084 (0.7801)\tPrec@1 74.219 (72.781)\n",
      "Epoch: [4][330/391]\tLoss 0.7545 (0.7756)\tPrec@1 73.438 (72.793)\n",
      "Epoch: [4][385/391]\tLoss 0.7549 (0.7705)\tPrec@1 72.656 (73.079)\n",
      "Test\t  Prec@1: 73.470 (Err: 26.530 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.6686 (0.6686)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [5][55/391]\tLoss 0.7871 (0.7242)\tPrec@1 71.094 (74.400)\n",
      "Epoch: [5][110/391]\tLoss 0.8794 (0.7072)\tPrec@1 66.406 (75.127)\n",
      "Epoch: [5][165/391]\tLoss 0.6950 (0.7021)\tPrec@1 76.562 (75.513)\n",
      "Epoch: [5][220/391]\tLoss 0.7191 (0.6984)\tPrec@1 71.875 (75.629)\n",
      "Epoch: [5][275/391]\tLoss 0.7643 (0.6994)\tPrec@1 71.875 (75.620)\n",
      "Epoch: [5][330/391]\tLoss 0.6645 (0.6944)\tPrec@1 72.656 (75.810)\n",
      "Epoch: [5][385/391]\tLoss 0.7480 (0.6909)\tPrec@1 74.219 (75.976)\n",
      "Test\t  Prec@1: 73.700 (Err: 26.300 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.5433 (0.5433)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [6][55/391]\tLoss 0.7246 (0.6331)\tPrec@1 74.219 (78.041)\n",
      "Epoch: [6][110/391]\tLoss 0.7662 (0.6280)\tPrec@1 69.531 (78.026)\n",
      "Epoch: [6][165/391]\tLoss 0.7491 (0.6269)\tPrec@1 70.312 (78.003)\n",
      "Epoch: [6][220/391]\tLoss 0.5569 (0.6215)\tPrec@1 80.469 (78.235)\n",
      "Epoch: [6][275/391]\tLoss 0.5248 (0.6180)\tPrec@1 82.031 (78.397)\n",
      "Epoch: [6][330/391]\tLoss 0.6027 (0.6140)\tPrec@1 79.688 (78.533)\n",
      "Epoch: [6][385/391]\tLoss 0.6268 (0.6122)\tPrec@1 78.125 (78.647)\n",
      "Test\t  Prec@1: 74.810 (Err: 25.190 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.7311 (0.7311)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [7][55/391]\tLoss 0.5391 (0.5401)\tPrec@1 80.469 (81.445)\n",
      "Epoch: [7][110/391]\tLoss 0.6313 (0.5671)\tPrec@1 78.125 (80.384)\n",
      "Epoch: [7][165/391]\tLoss 0.6267 (0.5660)\tPrec@1 76.562 (80.243)\n",
      "Epoch: [7][220/391]\tLoss 0.6873 (0.5626)\tPrec@1 69.531 (80.380)\n",
      "Epoch: [7][275/391]\tLoss 0.5607 (0.5643)\tPrec@1 79.688 (80.412)\n",
      "Epoch: [7][330/391]\tLoss 0.5933 (0.5607)\tPrec@1 77.344 (80.599)\n",
      "Epoch: [7][385/391]\tLoss 0.5147 (0.5607)\tPrec@1 84.375 (80.594)\n",
      "Test\t  Prec@1: 78.540 (Err: 21.460 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5235 (0.5235)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [8][55/391]\tLoss 0.5453 (0.5286)\tPrec@1 82.812 (81.627)\n",
      "Epoch: [8][110/391]\tLoss 0.5270 (0.5246)\tPrec@1 82.031 (81.827)\n",
      "Epoch: [8][165/391]\tLoss 0.6518 (0.5237)\tPrec@1 77.344 (81.909)\n",
      "Epoch: [8][220/391]\tLoss 0.4718 (0.5213)\tPrec@1 78.906 (81.943)\n",
      "Epoch: [8][275/391]\tLoss 0.5061 (0.5195)\tPrec@1 82.812 (82.003)\n",
      "Epoch: [8][330/391]\tLoss 0.4842 (0.5184)\tPrec@1 83.594 (82.017)\n",
      "Epoch: [8][385/391]\tLoss 0.5547 (0.5174)\tPrec@1 80.469 (82.068)\n",
      "Test\t  Prec@1: 81.370 (Err: 18.630 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.4233 (0.4233)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [9][55/391]\tLoss 0.5588 (0.4997)\tPrec@1 76.562 (82.617)\n",
      "Epoch: [9][110/391]\tLoss 0.5436 (0.4884)\tPrec@1 78.906 (82.932)\n",
      "Epoch: [9][165/391]\tLoss 0.4423 (0.4822)\tPrec@1 86.719 (83.180)\n",
      "Epoch: [9][220/391]\tLoss 0.5535 (0.4843)\tPrec@1 82.031 (83.063)\n",
      "Epoch: [9][275/391]\tLoss 0.4431 (0.4879)\tPrec@1 85.156 (83.039)\n",
      "Epoch: [9][330/391]\tLoss 0.5799 (0.4904)\tPrec@1 78.125 (83.027)\n",
      "Epoch: [9][385/391]\tLoss 0.3519 (0.4886)\tPrec@1 89.062 (83.140)\n",
      "Test\t  Prec@1: 81.450 (Err: 18.550 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4608 (0.4608)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [10][55/391]\tLoss 0.5817 (0.4682)\tPrec@1 77.344 (83.315)\n",
      "Epoch: [10][110/391]\tLoss 0.4547 (0.4591)\tPrec@1 83.594 (83.685)\n",
      "Epoch: [10][165/391]\tLoss 0.4956 (0.4504)\tPrec@1 79.688 (83.989)\n",
      "Epoch: [10][220/391]\tLoss 0.3983 (0.4540)\tPrec@1 89.062 (83.933)\n",
      "Epoch: [10][275/391]\tLoss 0.4408 (0.4534)\tPrec@1 85.938 (84.041)\n",
      "Epoch: [10][330/391]\tLoss 0.5211 (0.4574)\tPrec@1 84.375 (83.964)\n",
      "Epoch: [10][385/391]\tLoss 0.5707 (0.4580)\tPrec@1 78.125 (83.912)\n",
      "Test\t  Prec@1: 80.640 (Err: 19.360 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.4506 (0.4506)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [11][55/391]\tLoss 0.3336 (0.4238)\tPrec@1 89.062 (85.686)\n",
      "Epoch: [11][110/391]\tLoss 0.3340 (0.4290)\tPrec@1 84.375 (85.353)\n",
      "Epoch: [11][165/391]\tLoss 0.5251 (0.4300)\tPrec@1 79.688 (85.137)\n",
      "Epoch: [11][220/391]\tLoss 0.3868 (0.4309)\tPrec@1 86.719 (85.177)\n",
      "Epoch: [11][275/391]\tLoss 0.4737 (0.4323)\tPrec@1 85.156 (85.148)\n",
      "Epoch: [11][330/391]\tLoss 0.5950 (0.4342)\tPrec@1 75.000 (84.975)\n",
      "Epoch: [11][385/391]\tLoss 0.5086 (0.4347)\tPrec@1 83.594 (85.013)\n",
      "Test\t  Prec@1: 80.610 (Err: 19.390 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.5126 (0.5126)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [12][55/391]\tLoss 0.3385 (0.4178)\tPrec@1 90.625 (85.212)\n",
      "Epoch: [12][110/391]\tLoss 0.6274 (0.4199)\tPrec@1 78.906 (85.262)\n",
      "Epoch: [12][165/391]\tLoss 0.2857 (0.4141)\tPrec@1 91.406 (85.608)\n",
      "Epoch: [12][220/391]\tLoss 0.5249 (0.4140)\tPrec@1 80.469 (85.517)\n",
      "Epoch: [12][275/391]\tLoss 0.4175 (0.4197)\tPrec@1 85.156 (85.360)\n",
      "Epoch: [12][330/391]\tLoss 0.4270 (0.4184)\tPrec@1 84.375 (85.310)\n",
      "Epoch: [12][385/391]\tLoss 0.4360 (0.4194)\tPrec@1 82.031 (85.286)\n",
      "Test\t  Prec@1: 80.060 (Err: 19.940 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.3116 (0.3116)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [13][55/391]\tLoss 0.5773 (0.4010)\tPrec@1 81.250 (85.770)\n",
      "Epoch: [13][110/391]\tLoss 0.2593 (0.4049)\tPrec@1 91.406 (86.022)\n",
      "Epoch: [13][165/391]\tLoss 0.4111 (0.4052)\tPrec@1 83.594 (85.895)\n",
      "Epoch: [13][220/391]\tLoss 0.4351 (0.4061)\tPrec@1 81.250 (85.874)\n",
      "Epoch: [13][275/391]\tLoss 0.3964 (0.4056)\tPrec@1 87.500 (85.966)\n",
      "Epoch: [13][330/391]\tLoss 0.3820 (0.4045)\tPrec@1 86.719 (85.947)\n",
      "Epoch: [13][385/391]\tLoss 0.4951 (0.4043)\tPrec@1 82.031 (85.929)\n",
      "Test\t  Prec@1: 82.630 (Err: 17.370 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3894 (0.3894)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [14][55/391]\tLoss 0.3432 (0.3835)\tPrec@1 88.281 (86.761)\n",
      "Epoch: [14][110/391]\tLoss 0.3823 (0.3771)\tPrec@1 88.281 (87.050)\n",
      "Epoch: [14][165/391]\tLoss 0.3703 (0.3893)\tPrec@1 86.719 (86.568)\n",
      "Epoch: [14][220/391]\tLoss 0.2732 (0.3915)\tPrec@1 90.625 (86.425)\n",
      "Epoch: [14][275/391]\tLoss 0.3153 (0.3899)\tPrec@1 91.406 (86.439)\n",
      "Epoch: [14][330/391]\tLoss 0.4573 (0.3884)\tPrec@1 85.156 (86.502)\n",
      "Epoch: [14][385/391]\tLoss 0.5046 (0.3891)\tPrec@1 83.594 (86.460)\n",
      "Test\t  Prec@1: 79.540 (Err: 20.460 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3027 (0.3027)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [15][55/391]\tLoss 0.3996 (0.3671)\tPrec@1 85.938 (87.165)\n",
      "Epoch: [15][110/391]\tLoss 0.4883 (0.3672)\tPrec@1 85.156 (87.247)\n",
      "Epoch: [15][165/391]\tLoss 0.4018 (0.3715)\tPrec@1 85.938 (87.062)\n",
      "Epoch: [15][220/391]\tLoss 0.3642 (0.3729)\tPrec@1 87.500 (87.090)\n",
      "Epoch: [15][275/391]\tLoss 0.4025 (0.3734)\tPrec@1 84.375 (87.141)\n",
      "Epoch: [15][330/391]\tLoss 0.1942 (0.3741)\tPrec@1 95.312 (87.148)\n",
      "Epoch: [15][385/391]\tLoss 0.3381 (0.3736)\tPrec@1 89.062 (87.146)\n",
      "Test\t  Prec@1: 81.030 (Err: 18.970 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.2199 (0.2199)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [16][55/391]\tLoss 0.2792 (0.3240)\tPrec@1 89.062 (88.979)\n",
      "Epoch: [16][110/391]\tLoss 0.3957 (0.3441)\tPrec@1 88.281 (88.260)\n",
      "Epoch: [16][165/391]\tLoss 0.2582 (0.3560)\tPrec@1 90.625 (87.825)\n",
      "Epoch: [16][220/391]\tLoss 0.3526 (0.3570)\tPrec@1 88.281 (87.747)\n",
      "Epoch: [16][275/391]\tLoss 0.3897 (0.3591)\tPrec@1 86.719 (87.698)\n",
      "Epoch: [16][330/391]\tLoss 0.4239 (0.3565)\tPrec@1 86.719 (87.819)\n",
      "Epoch: [16][385/391]\tLoss 0.3861 (0.3575)\tPrec@1 85.938 (87.769)\n",
      "Test\t  Prec@1: 81.360 (Err: 18.640 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.2657 (0.2657)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [17][55/391]\tLoss 0.3795 (0.3202)\tPrec@1 85.156 (88.770)\n",
      "Epoch: [17][110/391]\tLoss 0.3120 (0.3371)\tPrec@1 91.406 (88.323)\n",
      "Epoch: [17][165/391]\tLoss 0.4242 (0.3425)\tPrec@1 80.469 (88.008)\n",
      "Epoch: [17][220/391]\tLoss 0.3201 (0.3395)\tPrec@1 86.719 (88.175)\n",
      "Epoch: [17][275/391]\tLoss 0.3389 (0.3436)\tPrec@1 89.844 (88.168)\n",
      "Epoch: [17][330/391]\tLoss 0.3331 (0.3474)\tPrec@1 89.844 (88.057)\n",
      "Epoch: [17][385/391]\tLoss 0.3086 (0.3484)\tPrec@1 88.281 (87.970)\n",
      "Test\t  Prec@1: 84.070 (Err: 15.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.4066 (0.4066)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [18][55/391]\tLoss 0.3136 (0.3357)\tPrec@1 89.844 (88.421)\n",
      "Epoch: [18][110/391]\tLoss 0.2989 (0.3249)\tPrec@1 89.844 (88.901)\n",
      "Epoch: [18][165/391]\tLoss 0.2688 (0.3247)\tPrec@1 89.062 (88.785)\n",
      "Epoch: [18][220/391]\tLoss 0.4799 (0.3288)\tPrec@1 85.156 (88.674)\n",
      "Epoch: [18][275/391]\tLoss 0.3537 (0.3345)\tPrec@1 89.062 (88.505)\n",
      "Epoch: [18][330/391]\tLoss 0.4821 (0.3365)\tPrec@1 88.281 (88.439)\n",
      "Epoch: [18][385/391]\tLoss 0.4160 (0.3374)\tPrec@1 83.594 (88.338)\n",
      "Test\t  Prec@1: 84.800 (Err: 15.200 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.2601 (0.2601)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [19][55/391]\tLoss 0.2843 (0.3034)\tPrec@1 86.719 (89.160)\n",
      "Epoch: [19][110/391]\tLoss 0.2480 (0.3098)\tPrec@1 93.750 (89.048)\n",
      "Epoch: [19][165/391]\tLoss 0.4476 (0.3202)\tPrec@1 85.156 (88.926)\n",
      "Epoch: [19][220/391]\tLoss 0.1577 (0.3227)\tPrec@1 96.094 (88.804)\n",
      "Epoch: [19][275/391]\tLoss 0.3102 (0.3257)\tPrec@1 85.156 (88.624)\n",
      "Epoch: [19][330/391]\tLoss 0.3695 (0.3298)\tPrec@1 86.719 (88.487)\n",
      "Epoch: [19][385/391]\tLoss 0.3079 (0.3300)\tPrec@1 89.844 (88.522)\n",
      "Test\t  Prec@1: 79.450 (Err: 20.550 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.3316 (0.3316)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [20][55/391]\tLoss 0.4719 (0.3041)\tPrec@1 84.375 (89.244)\n",
      "Epoch: [20][110/391]\tLoss 0.2588 (0.3079)\tPrec@1 89.844 (89.133)\n",
      "Epoch: [20][165/391]\tLoss 0.2833 (0.3183)\tPrec@1 89.062 (88.804)\n",
      "Epoch: [20][220/391]\tLoss 0.4932 (0.3226)\tPrec@1 83.594 (88.663)\n",
      "Epoch: [20][275/391]\tLoss 0.3337 (0.3231)\tPrec@1 86.719 (88.692)\n",
      "Epoch: [20][330/391]\tLoss 0.4314 (0.3227)\tPrec@1 85.156 (88.723)\n",
      "Epoch: [20][385/391]\tLoss 0.3884 (0.3246)\tPrec@1 88.281 (88.621)\n",
      "Test\t  Prec@1: 85.000 (Err: 15.000 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.1725 (0.1725)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [21][55/391]\tLoss 0.3708 (0.2979)\tPrec@1 87.500 (89.481)\n",
      "Epoch: [21][110/391]\tLoss 0.4284 (0.3139)\tPrec@1 88.281 (88.830)\n",
      "Epoch: [21][165/391]\tLoss 0.2997 (0.3112)\tPrec@1 89.844 (89.044)\n",
      "Epoch: [21][220/391]\tLoss 0.2982 (0.3097)\tPrec@1 89.844 (89.119)\n",
      "Epoch: [21][275/391]\tLoss 0.3204 (0.3118)\tPrec@1 90.625 (89.179)\n",
      "Epoch: [21][330/391]\tLoss 0.3373 (0.3099)\tPrec@1 90.625 (89.230)\n",
      "Epoch: [21][385/391]\tLoss 0.2268 (0.3123)\tPrec@1 91.406 (89.139)\n",
      "Test\t  Prec@1: 83.120 (Err: 16.880 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.3568 (0.3568)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [22][55/391]\tLoss 0.3883 (0.2950)\tPrec@1 84.375 (89.676)\n",
      "Epoch: [22][110/391]\tLoss 0.2757 (0.2923)\tPrec@1 89.844 (89.661)\n",
      "Epoch: [22][165/391]\tLoss 0.2516 (0.2970)\tPrec@1 89.844 (89.571)\n",
      "Epoch: [22][220/391]\tLoss 0.3849 (0.3041)\tPrec@1 88.281 (89.292)\n",
      "Epoch: [22][275/391]\tLoss 0.3474 (0.3052)\tPrec@1 86.719 (89.210)\n",
      "Epoch: [22][330/391]\tLoss 0.3294 (0.3104)\tPrec@1 91.406 (89.088)\n",
      "Epoch: [22][385/391]\tLoss 0.3598 (0.3115)\tPrec@1 91.406 (89.081)\n",
      "Test\t  Prec@1: 84.990 (Err: 15.010 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.3950 (0.3950)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [23][55/391]\tLoss 0.2907 (0.2898)\tPrec@1 88.281 (89.872)\n",
      "Epoch: [23][110/391]\tLoss 0.2205 (0.2875)\tPrec@1 90.625 (89.963)\n",
      "Epoch: [23][165/391]\tLoss 0.2010 (0.2897)\tPrec@1 90.625 (89.811)\n",
      "Epoch: [23][220/391]\tLoss 0.2769 (0.2929)\tPrec@1 89.844 (89.688)\n",
      "Epoch: [23][275/391]\tLoss 0.2351 (0.2972)\tPrec@1 91.406 (89.583)\n",
      "Epoch: [23][330/391]\tLoss 0.3049 (0.2990)\tPrec@1 88.281 (89.572)\n",
      "Epoch: [23][385/391]\tLoss 0.3412 (0.3014)\tPrec@1 89.062 (89.455)\n",
      "Test\t  Prec@1: 82.050 (Err: 17.950 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.2123 (0.2123)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [24][55/391]\tLoss 0.2666 (0.2924)\tPrec@1 90.625 (89.565)\n",
      "Epoch: [24][110/391]\tLoss 0.2222 (0.2868)\tPrec@1 92.969 (89.921)\n",
      "Epoch: [24][165/391]\tLoss 0.2939 (0.2858)\tPrec@1 89.062 (89.980)\n",
      "Epoch: [24][220/391]\tLoss 0.3260 (0.2861)\tPrec@1 89.062 (90.017)\n",
      "Epoch: [24][275/391]\tLoss 0.3211 (0.2922)\tPrec@1 87.500 (89.878)\n",
      "Epoch: [24][330/391]\tLoss 0.2278 (0.2946)\tPrec@1 92.188 (89.815)\n",
      "Epoch: [24][385/391]\tLoss 0.3080 (0.2948)\tPrec@1 87.500 (89.755)\n",
      "Test\t  Prec@1: 84.470 (Err: 15.530 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2056 (0.2056)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [25][55/391]\tLoss 0.2470 (0.2588)\tPrec@1 90.625 (90.988)\n",
      "Epoch: [25][110/391]\tLoss 0.2613 (0.2676)\tPrec@1 89.844 (90.646)\n",
      "Epoch: [25][165/391]\tLoss 0.1907 (0.2776)\tPrec@1 96.875 (90.343)\n",
      "Epoch: [25][220/391]\tLoss 0.4183 (0.2823)\tPrec@1 85.156 (90.271)\n",
      "Epoch: [25][275/391]\tLoss 0.3540 (0.2878)\tPrec@1 92.188 (90.022)\n",
      "Epoch: [25][330/391]\tLoss 0.1268 (0.2877)\tPrec@1 96.094 (89.978)\n",
      "Epoch: [25][385/391]\tLoss 0.2304 (0.2888)\tPrec@1 93.750 (89.991)\n",
      "Test\t  Prec@1: 85.230 (Err: 14.770 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.2976 (0.2976)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [26][55/391]\tLoss 0.3066 (0.2701)\tPrec@1 88.281 (90.318)\n",
      "Epoch: [26][110/391]\tLoss 0.3079 (0.2808)\tPrec@1 87.500 (90.006)\n",
      "Epoch: [26][165/391]\tLoss 0.2415 (0.2845)\tPrec@1 90.625 (89.881)\n",
      "Epoch: [26][220/391]\tLoss 0.2244 (0.2840)\tPrec@1 92.969 (89.936)\n",
      "Epoch: [26][275/391]\tLoss 0.2755 (0.2836)\tPrec@1 89.844 (89.909)\n",
      "Epoch: [26][330/391]\tLoss 0.3433 (0.2840)\tPrec@1 89.062 (89.896)\n",
      "Epoch: [26][385/391]\tLoss 0.2669 (0.2826)\tPrec@1 89.844 (89.981)\n",
      "Test\t  Prec@1: 80.460 (Err: 19.540 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.3224 (0.3224)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [27][55/391]\tLoss 0.4115 (0.2591)\tPrec@1 86.719 (91.211)\n",
      "Epoch: [27][110/391]\tLoss 0.2445 (0.2596)\tPrec@1 91.406 (91.054)\n",
      "Epoch: [27][165/391]\tLoss 0.4451 (0.2652)\tPrec@1 86.719 (90.790)\n",
      "Epoch: [27][220/391]\tLoss 0.3531 (0.2698)\tPrec@1 87.500 (90.572)\n",
      "Epoch: [27][275/391]\tLoss 0.1972 (0.2762)\tPrec@1 93.750 (90.311)\n",
      "Epoch: [27][330/391]\tLoss 0.1779 (0.2769)\tPrec@1 96.094 (90.351)\n",
      "Epoch: [27][385/391]\tLoss 0.1913 (0.2779)\tPrec@1 94.531 (90.311)\n",
      "Test\t  Prec@1: 85.790 (Err: 14.210 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.1697 (0.1697)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [28][55/391]\tLoss 0.3470 (0.2477)\tPrec@1 89.062 (91.490)\n",
      "Epoch: [28][110/391]\tLoss 0.2453 (0.2603)\tPrec@1 90.625 (90.871)\n",
      "Epoch: [28][165/391]\tLoss 0.2751 (0.2655)\tPrec@1 89.844 (90.761)\n",
      "Epoch: [28][220/391]\tLoss 0.3246 (0.2683)\tPrec@1 85.938 (90.703)\n",
      "Epoch: [28][275/391]\tLoss 0.2554 (0.2735)\tPrec@1 92.188 (90.455)\n",
      "Epoch: [28][330/391]\tLoss 0.3975 (0.2743)\tPrec@1 89.844 (90.457)\n",
      "Epoch: [28][385/391]\tLoss 0.3156 (0.2768)\tPrec@1 86.719 (90.390)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2907 (0.2907)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [29][55/391]\tLoss 0.3011 (0.2662)\tPrec@1 92.188 (90.485)\n",
      "Epoch: [29][110/391]\tLoss 0.1683 (0.2713)\tPrec@1 92.969 (90.569)\n",
      "Epoch: [29][165/391]\tLoss 0.2370 (0.2646)\tPrec@1 93.750 (90.823)\n",
      "Epoch: [29][220/391]\tLoss 0.2395 (0.2689)\tPrec@1 92.969 (90.682)\n",
      "Epoch: [29][275/391]\tLoss 0.4194 (0.2728)\tPrec@1 84.375 (90.534)\n",
      "Epoch: [29][330/391]\tLoss 0.4264 (0.2718)\tPrec@1 83.594 (90.599)\n",
      "Epoch: [29][385/391]\tLoss 0.2281 (0.2703)\tPrec@1 94.531 (90.659)\n",
      "Test\t  Prec@1: 86.000 (Err: 14.000 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2062 (0.2062)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [30][55/391]\tLoss 0.3552 (0.2437)\tPrec@1 87.500 (91.727)\n",
      "Epoch: [30][110/391]\tLoss 0.2147 (0.2475)\tPrec@1 92.969 (91.441)\n",
      "Epoch: [30][165/391]\tLoss 0.2338 (0.2622)\tPrec@1 92.969 (90.898)\n",
      "Epoch: [30][220/391]\tLoss 0.2197 (0.2621)\tPrec@1 92.969 (90.908)\n",
      "Epoch: [30][275/391]\tLoss 0.1739 (0.2616)\tPrec@1 92.969 (90.953)\n",
      "Epoch: [30][330/391]\tLoss 0.2287 (0.2632)\tPrec@1 92.188 (90.896)\n",
      "Epoch: [30][385/391]\tLoss 0.2275 (0.2655)\tPrec@1 92.188 (90.819)\n",
      "Test\t  Prec@1: 86.530 (Err: 13.470 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.1713 (0.1713)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [31][55/391]\tLoss 0.2623 (0.2426)\tPrec@1 92.188 (91.769)\n",
      "Epoch: [31][110/391]\tLoss 0.3087 (0.2499)\tPrec@1 89.844 (91.596)\n",
      "Epoch: [31][165/391]\tLoss 0.2201 (0.2459)\tPrec@1 91.406 (91.524)\n",
      "Epoch: [31][220/391]\tLoss 0.2664 (0.2497)\tPrec@1 92.188 (91.321)\n",
      "Epoch: [31][275/391]\tLoss 0.2016 (0.2520)\tPrec@1 93.750 (91.205)\n",
      "Epoch: [31][330/391]\tLoss 0.2495 (0.2553)\tPrec@1 89.062 (91.099)\n",
      "Epoch: [31][385/391]\tLoss 0.2895 (0.2573)\tPrec@1 89.062 (91.050)\n",
      "Test\t  Prec@1: 84.710 (Err: 15.290 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.1623 (0.1623)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [32][55/391]\tLoss 0.4153 (0.2453)\tPrec@1 86.719 (91.085)\n",
      "Epoch: [32][110/391]\tLoss 0.2244 (0.2545)\tPrec@1 92.969 (90.963)\n",
      "Epoch: [32][165/391]\tLoss 0.3342 (0.2597)\tPrec@1 86.719 (90.841)\n",
      "Epoch: [32][220/391]\tLoss 0.2424 (0.2580)\tPrec@1 89.062 (90.844)\n",
      "Epoch: [32][275/391]\tLoss 0.2632 (0.2539)\tPrec@1 89.062 (91.004)\n",
      "Epoch: [32][330/391]\tLoss 0.2328 (0.2552)\tPrec@1 93.750 (91.005)\n",
      "Epoch: [32][385/391]\tLoss 0.3124 (0.2572)\tPrec@1 89.844 (90.983)\n",
      "Test\t  Prec@1: 86.070 (Err: 13.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.3594 (0.3594)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [33][55/391]\tLoss 0.2133 (0.2389)\tPrec@1 92.969 (91.671)\n",
      "Epoch: [33][110/391]\tLoss 0.1693 (0.2237)\tPrec@1 95.312 (92.131)\n",
      "Epoch: [33][165/391]\tLoss 0.2641 (0.2352)\tPrec@1 91.406 (91.802)\n",
      "Epoch: [33][220/391]\tLoss 0.2256 (0.2418)\tPrec@1 91.406 (91.572)\n",
      "Epoch: [33][275/391]\tLoss 0.3876 (0.2458)\tPrec@1 88.281 (91.389)\n",
      "Epoch: [33][330/391]\tLoss 0.1995 (0.2514)\tPrec@1 92.969 (91.201)\n",
      "Epoch: [33][385/391]\tLoss 0.1904 (0.2554)\tPrec@1 94.531 (91.036)\n",
      "Test\t  Prec@1: 86.020 (Err: 13.980 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.2659 (0.2659)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [34][55/391]\tLoss 0.2136 (0.2513)\tPrec@1 90.625 (91.295)\n",
      "Epoch: [34][110/391]\tLoss 0.3586 (0.2478)\tPrec@1 88.281 (91.434)\n",
      "Epoch: [34][165/391]\tLoss 0.2775 (0.2409)\tPrec@1 92.188 (91.698)\n",
      "Epoch: [34][220/391]\tLoss 0.2725 (0.2374)\tPrec@1 92.188 (91.753)\n",
      "Epoch: [34][275/391]\tLoss 0.2363 (0.2395)\tPrec@1 92.969 (91.667)\n",
      "Epoch: [34][330/391]\tLoss 0.3035 (0.2416)\tPrec@1 90.625 (91.579)\n",
      "Epoch: [34][385/391]\tLoss 0.3244 (0.2449)\tPrec@1 89.062 (91.487)\n",
      "Test\t  Prec@1: 86.260 (Err: 13.740 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.2251 (0.2251)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [35][55/391]\tLoss 0.3635 (0.2326)\tPrec@1 86.719 (91.713)\n",
      "Epoch: [35][110/391]\tLoss 0.2393 (0.2391)\tPrec@1 92.188 (91.695)\n",
      "Epoch: [35][165/391]\tLoss 0.1555 (0.2372)\tPrec@1 96.094 (91.787)\n",
      "Epoch: [35][220/391]\tLoss 0.2513 (0.2341)\tPrec@1 89.062 (91.894)\n",
      "Epoch: [35][275/391]\tLoss 0.2738 (0.2357)\tPrec@1 91.406 (91.800)\n",
      "Epoch: [35][330/391]\tLoss 0.2603 (0.2365)\tPrec@1 89.844 (91.796)\n",
      "Epoch: [35][385/391]\tLoss 0.2364 (0.2381)\tPrec@1 91.406 (91.708)\n",
      "Test\t  Prec@1: 84.290 (Err: 15.710 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2051 (0.2051)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [36][55/391]\tLoss 0.1439 (0.2386)\tPrec@1 94.531 (92.006)\n",
      "Epoch: [36][110/391]\tLoss 0.3080 (0.2378)\tPrec@1 92.188 (91.864)\n",
      "Epoch: [36][165/391]\tLoss 0.3144 (0.2431)\tPrec@1 91.406 (91.510)\n",
      "Epoch: [36][220/391]\tLoss 0.2279 (0.2467)\tPrec@1 91.406 (91.389)\n",
      "Epoch: [36][275/391]\tLoss 0.2385 (0.2450)\tPrec@1 92.969 (91.429)\n",
      "Epoch: [36][330/391]\tLoss 0.1760 (0.2445)\tPrec@1 94.531 (91.376)\n",
      "Epoch: [36][385/391]\tLoss 0.3383 (0.2459)\tPrec@1 87.500 (91.299)\n",
      "Test\t  Prec@1: 85.370 (Err: 14.630 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.1299 (0.1299)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [37][55/391]\tLoss 0.2616 (0.2399)\tPrec@1 90.625 (91.546)\n",
      "Epoch: [37][110/391]\tLoss 0.3041 (0.2381)\tPrec@1 89.062 (91.681)\n",
      "Epoch: [37][165/391]\tLoss 0.1687 (0.2369)\tPrec@1 96.094 (91.722)\n",
      "Epoch: [37][220/391]\tLoss 0.2209 (0.2371)\tPrec@1 93.750 (91.629)\n",
      "Epoch: [37][275/391]\tLoss 0.1329 (0.2368)\tPrec@1 93.750 (91.590)\n",
      "Epoch: [37][330/391]\tLoss 0.2459 (0.2396)\tPrec@1 91.406 (91.517)\n",
      "Epoch: [37][385/391]\tLoss 0.2636 (0.2413)\tPrec@1 92.188 (91.491)\n",
      "Test\t  Prec@1: 85.580 (Err: 14.420 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.2059 (0.2059)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [38][55/391]\tLoss 0.1621 (0.2153)\tPrec@1 95.312 (92.327)\n",
      "Epoch: [38][110/391]\tLoss 0.2643 (0.2211)\tPrec@1 91.406 (92.244)\n",
      "Epoch: [38][165/391]\tLoss 0.2850 (0.2287)\tPrec@1 90.625 (91.919)\n",
      "Epoch: [38][220/391]\tLoss 0.1281 (0.2306)\tPrec@1 96.094 (91.947)\n",
      "Epoch: [38][275/391]\tLoss 0.2961 (0.2340)\tPrec@1 89.062 (91.803)\n",
      "Epoch: [38][330/391]\tLoss 0.2649 (0.2379)\tPrec@1 89.062 (91.699)\n",
      "Epoch: [38][385/391]\tLoss 0.2358 (0.2396)\tPrec@1 90.625 (91.645)\n",
      "Test\t  Prec@1: 87.340 (Err: 12.660 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2283 (0.2283)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [39][55/391]\tLoss 0.2587 (0.2251)\tPrec@1 92.969 (92.104)\n",
      "Epoch: [39][110/391]\tLoss 0.1646 (0.2244)\tPrec@1 92.969 (92.188)\n",
      "Epoch: [39][165/391]\tLoss 0.2598 (0.2245)\tPrec@1 92.188 (92.084)\n",
      "Epoch: [39][220/391]\tLoss 0.3051 (0.2290)\tPrec@1 89.844 (91.933)\n",
      "Epoch: [39][275/391]\tLoss 0.1799 (0.2344)\tPrec@1 93.750 (91.794)\n",
      "Epoch: [39][330/391]\tLoss 0.2767 (0.2363)\tPrec@1 89.844 (91.687)\n",
      "Epoch: [39][385/391]\tLoss 0.2726 (0.2394)\tPrec@1 91.406 (91.558)\n",
      "Test\t  Prec@1: 85.720 (Err: 14.280 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.1427 (0.1427)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [40][55/391]\tLoss 0.3350 (0.2256)\tPrec@1 87.500 (91.895)\n",
      "Epoch: [40][110/391]\tLoss 0.1772 (0.2256)\tPrec@1 92.188 (92.026)\n",
      "Epoch: [40][165/391]\tLoss 0.2331 (0.2289)\tPrec@1 93.750 (91.891)\n",
      "Epoch: [40][220/391]\tLoss 0.1391 (0.2284)\tPrec@1 95.312 (91.958)\n",
      "Epoch: [40][275/391]\tLoss 0.2026 (0.2291)\tPrec@1 94.531 (91.921)\n",
      "Epoch: [40][330/391]\tLoss 0.2572 (0.2305)\tPrec@1 90.625 (91.831)\n",
      "Epoch: [40][385/391]\tLoss 0.2496 (0.2337)\tPrec@1 91.406 (91.781)\n",
      "Test\t  Prec@1: 87.040 (Err: 12.960 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2934 (0.2934)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [41][55/391]\tLoss 0.2125 (0.2171)\tPrec@1 92.969 (92.439)\n",
      "Epoch: [41][110/391]\tLoss 0.2572 (0.2137)\tPrec@1 88.281 (92.420)\n",
      "Epoch: [41][165/391]\tLoss 0.1859 (0.2130)\tPrec@1 94.531 (92.564)\n",
      "Epoch: [41][220/391]\tLoss 0.2931 (0.2195)\tPrec@1 89.062 (92.301)\n",
      "Epoch: [41][275/391]\tLoss 0.1471 (0.2216)\tPrec@1 95.312 (92.250)\n",
      "Epoch: [41][330/391]\tLoss 0.1781 (0.2253)\tPrec@1 93.750 (92.133)\n",
      "Epoch: [41][385/391]\tLoss 0.2158 (0.2299)\tPrec@1 94.531 (91.999)\n",
      "Test\t  Prec@1: 84.400 (Err: 15.600 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2071 (0.2071)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [42][55/391]\tLoss 0.2993 (0.2290)\tPrec@1 91.406 (92.243)\n",
      "Epoch: [42][110/391]\tLoss 0.1396 (0.2230)\tPrec@1 96.094 (92.237)\n",
      "Epoch: [42][165/391]\tLoss 0.3654 (0.2218)\tPrec@1 89.062 (92.173)\n",
      "Epoch: [42][220/391]\tLoss 0.3122 (0.2208)\tPrec@1 90.625 (92.216)\n",
      "Epoch: [42][275/391]\tLoss 0.3138 (0.2247)\tPrec@1 93.750 (92.097)\n",
      "Epoch: [42][330/391]\tLoss 0.2978 (0.2265)\tPrec@1 90.625 (92.069)\n",
      "Epoch: [42][385/391]\tLoss 0.1421 (0.2286)\tPrec@1 96.094 (92.026)\n",
      "Test\t  Prec@1: 85.820 (Err: 14.180 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.3258 (0.3258)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [43][55/391]\tLoss 0.0756 (0.1976)\tPrec@1 97.656 (93.471)\n",
      "Epoch: [43][110/391]\tLoss 0.1837 (0.2096)\tPrec@1 92.188 (92.842)\n",
      "Epoch: [43][165/391]\tLoss 0.2639 (0.2109)\tPrec@1 90.625 (92.795)\n",
      "Epoch: [43][220/391]\tLoss 0.2359 (0.2159)\tPrec@1 91.406 (92.559)\n",
      "Epoch: [43][275/391]\tLoss 0.2587 (0.2157)\tPrec@1 90.625 (92.499)\n",
      "Epoch: [43][330/391]\tLoss 0.2802 (0.2200)\tPrec@1 89.062 (92.280)\n",
      "Epoch: [43][385/391]\tLoss 0.2868 (0.2217)\tPrec@1 89.844 (92.214)\n",
      "Test\t  Prec@1: 84.820 (Err: 15.180 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2629 (0.2629)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [44][55/391]\tLoss 0.2643 (0.2002)\tPrec@1 90.625 (92.899)\n",
      "Epoch: [44][110/391]\tLoss 0.2007 (0.2096)\tPrec@1 93.750 (92.666)\n",
      "Epoch: [44][165/391]\tLoss 0.3009 (0.2155)\tPrec@1 89.062 (92.479)\n",
      "Epoch: [44][220/391]\tLoss 0.2442 (0.2167)\tPrec@1 91.406 (92.396)\n",
      "Epoch: [44][275/391]\tLoss 0.2196 (0.2219)\tPrec@1 92.969 (92.244)\n",
      "Epoch: [44][330/391]\tLoss 0.2458 (0.2238)\tPrec@1 90.625 (92.157)\n",
      "Epoch: [44][385/391]\tLoss 0.2634 (0.2254)\tPrec@1 92.188 (92.105)\n",
      "Test\t  Prec@1: 87.090 (Err: 12.910 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.2276 (0.2276)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [45][55/391]\tLoss 0.2505 (0.2090)\tPrec@1 87.500 (92.704)\n",
      "Epoch: [45][110/391]\tLoss 0.1427 (0.2081)\tPrec@1 93.750 (92.582)\n",
      "Epoch: [45][165/391]\tLoss 0.1798 (0.2065)\tPrec@1 94.531 (92.597)\n",
      "Epoch: [45][220/391]\tLoss 0.1994 (0.2111)\tPrec@1 93.750 (92.495)\n",
      "Epoch: [45][275/391]\tLoss 0.2020 (0.2147)\tPrec@1 90.625 (92.422)\n",
      "Epoch: [45][330/391]\tLoss 0.2593 (0.2190)\tPrec@1 89.844 (92.265)\n",
      "Epoch: [45][385/391]\tLoss 0.2941 (0.2204)\tPrec@1 90.625 (92.230)\n",
      "Test\t  Prec@1: 85.070 (Err: 14.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2082 (0.2082)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [46][55/391]\tLoss 0.1661 (0.2118)\tPrec@1 94.531 (92.662)\n",
      "Epoch: [46][110/391]\tLoss 0.2239 (0.2105)\tPrec@1 91.406 (92.758)\n",
      "Epoch: [46][165/391]\tLoss 0.1287 (0.2124)\tPrec@1 95.312 (92.611)\n",
      "Epoch: [46][220/391]\tLoss 0.1502 (0.2141)\tPrec@1 94.531 (92.527)\n",
      "Epoch: [46][275/391]\tLoss 0.1484 (0.2172)\tPrec@1 96.875 (92.405)\n",
      "Epoch: [46][330/391]\tLoss 0.2732 (0.2165)\tPrec@1 90.625 (92.400)\n",
      "Epoch: [46][385/391]\tLoss 0.2827 (0.2188)\tPrec@1 91.406 (92.345)\n",
      "Test\t  Prec@1: 85.340 (Err: 14.660 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.1899 (0.1899)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [47][55/391]\tLoss 0.2224 (0.2077)\tPrec@1 92.969 (92.564)\n",
      "Epoch: [47][110/391]\tLoss 0.1932 (0.2094)\tPrec@1 92.188 (92.469)\n",
      "Epoch: [47][165/391]\tLoss 0.1616 (0.2093)\tPrec@1 91.406 (92.503)\n",
      "Epoch: [47][220/391]\tLoss 0.1645 (0.2104)\tPrec@1 94.531 (92.537)\n",
      "Epoch: [47][275/391]\tLoss 0.2292 (0.2139)\tPrec@1 93.750 (92.411)\n",
      "Epoch: [47][330/391]\tLoss 0.2111 (0.2167)\tPrec@1 92.188 (92.357)\n",
      "Epoch: [47][385/391]\tLoss 0.3319 (0.2187)\tPrec@1 89.062 (92.283)\n",
      "Test\t  Prec@1: 86.390 (Err: 13.610 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1079 (0.1079)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [48][55/391]\tLoss 0.2553 (0.1845)\tPrec@1 90.625 (93.401)\n",
      "Epoch: [48][110/391]\tLoss 0.1870 (0.1928)\tPrec@1 92.188 (93.053)\n",
      "Epoch: [48][165/391]\tLoss 0.3447 (0.1994)\tPrec@1 90.625 (92.893)\n",
      "Epoch: [48][220/391]\tLoss 0.3116 (0.2049)\tPrec@1 86.719 (92.735)\n",
      "Epoch: [48][275/391]\tLoss 0.1288 (0.2079)\tPrec@1 95.312 (92.649)\n",
      "Epoch: [48][330/391]\tLoss 0.1765 (0.2122)\tPrec@1 94.531 (92.556)\n",
      "Epoch: [48][385/391]\tLoss 0.1911 (0.2143)\tPrec@1 94.531 (92.536)\n",
      "Test\t  Prec@1: 86.960 (Err: 13.040 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2145 (0.2145)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [49][55/391]\tLoss 0.1953 (0.1940)\tPrec@1 91.406 (93.150)\n",
      "Epoch: [49][110/391]\tLoss 0.1936 (0.1972)\tPrec@1 92.969 (93.074)\n",
      "Epoch: [49][165/391]\tLoss 0.1645 (0.2002)\tPrec@1 96.094 (93.030)\n",
      "Epoch: [49][220/391]\tLoss 0.3274 (0.2048)\tPrec@1 89.062 (92.877)\n",
      "Epoch: [49][275/391]\tLoss 0.2865 (0.2108)\tPrec@1 88.281 (92.632)\n",
      "Epoch: [49][330/391]\tLoss 0.1742 (0.2116)\tPrec@1 95.312 (92.565)\n",
      "Epoch: [49][385/391]\tLoss 0.1899 (0.2125)\tPrec@1 94.531 (92.556)\n",
      "Test\t  Prec@1: 84.790 (Err: 15.210 )\n",
      "\n",
      "The lowest error from resnet44 model after 50 epochs is 12.660\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet44';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-28                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-30             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-33                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-35             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-38                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-40             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-43                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-45             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-50            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-11                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-53                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-55             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-12                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-58                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-60             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-13                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-63                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-65             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-14                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-68                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-70             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-15                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-73                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-75             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-16                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-76                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-77            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-78                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-79            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-80             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-17                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-81                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-82            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-83                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-84            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-85             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-18                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-86                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-87            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-88                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-89            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-90             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-19                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-91                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-92            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-93                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-94            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-95            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-20                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-96                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-97            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-98                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-99            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-100            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-21                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-101                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-102           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-103                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-104           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-105            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-22                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-106                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-107           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-108                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-109           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-110            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-23                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-111                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-112           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-113                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-114           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-115            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-24                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-116                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-117           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-118                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-119           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-120            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-25                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-121                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-122           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-123                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-124           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-125            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-26                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-126                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-127           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-128                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-129           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-130            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-27                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-131                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-132           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-133                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-134           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-135            [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 853,018\n",
      "Trainable params: 853,018\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 127.19\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.13\n",
      "Params size (MB): 3.25\n",
      "Estimated Total Size (MB): 11.39\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet56',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 6.0584 (6.0584)\tPrec@1 12.500 (12.500)\n",
      "Epoch: [0][55/391]\tLoss 2.2840 (3.3470)\tPrec@1 11.719 (11.691)\n",
      "Epoch: [0][110/391]\tLoss 2.2261 (2.8167)\tPrec@1 10.156 (12.324)\n",
      "Epoch: [0][165/391]\tLoss 2.1303 (2.6186)\tPrec@1 19.531 (13.578)\n",
      "Epoch: [0][220/391]\tLoss 2.0265 (2.4902)\tPrec@1 26.562 (15.367)\n",
      "Epoch: [0][275/391]\tLoss 1.9590 (2.3996)\tPrec@1 20.312 (16.667)\n",
      "Epoch: [0][330/391]\tLoss 1.9950 (2.3280)\tPrec@1 26.562 (18.035)\n",
      "Epoch: [0][385/391]\tLoss 1.9009 (2.2683)\tPrec@1 28.906 (19.379)\n",
      "Test\t  Prec@1: 23.250 (Err: 76.750 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.9612 (1.9612)\tPrec@1 21.094 (21.094)\n",
      "Epoch: [1][55/391]\tLoss 1.8317 (1.8575)\tPrec@1 28.906 (29.785)\n",
      "Epoch: [1][110/391]\tLoss 1.9071 (1.8169)\tPrec@1 22.656 (31.292)\n",
      "Epoch: [1][165/391]\tLoss 1.8355 (1.7868)\tPrec@1 28.125 (32.380)\n",
      "Epoch: [1][220/391]\tLoss 1.6024 (1.7593)\tPrec@1 45.312 (33.686)\n",
      "Epoch: [1][275/391]\tLoss 1.6463 (1.7296)\tPrec@1 35.938 (34.817)\n",
      "Epoch: [1][330/391]\tLoss 1.4902 (1.6991)\tPrec@1 44.531 (36.013)\n",
      "Epoch: [1][385/391]\tLoss 1.4417 (1.6715)\tPrec@1 45.312 (37.192)\n",
      "Test\t  Prec@1: 40.520 (Err: 59.480 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.6868 (1.6868)\tPrec@1 41.406 (41.406)\n",
      "Epoch: [2][55/391]\tLoss 1.5263 (1.4867)\tPrec@1 39.062 (45.592)\n",
      "Epoch: [2][110/391]\tLoss 1.3661 (1.4569)\tPrec@1 46.094 (46.699)\n",
      "Epoch: [2][165/391]\tLoss 1.3989 (1.4365)\tPrec@1 50.781 (47.553)\n",
      "Epoch: [2][220/391]\tLoss 1.3215 (1.4153)\tPrec@1 54.688 (48.554)\n",
      "Epoch: [2][275/391]\tLoss 1.1204 (1.3968)\tPrec@1 57.031 (49.114)\n",
      "Epoch: [2][330/391]\tLoss 1.1337 (1.3804)\tPrec@1 64.062 (49.641)\n",
      "Epoch: [2][385/391]\tLoss 1.0638 (1.3625)\tPrec@1 61.719 (50.366)\n",
      "Test\t  Prec@1: 53.860 (Err: 46.140 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.3143 (1.3143)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [3][55/391]\tLoss 1.0588 (1.2303)\tPrec@1 59.375 (55.608)\n",
      "Epoch: [3][110/391]\tLoss 1.0876 (1.2060)\tPrec@1 60.156 (56.553)\n",
      "Epoch: [3][165/391]\tLoss 1.0867 (1.1900)\tPrec@1 61.719 (57.055)\n",
      "Epoch: [3][220/391]\tLoss 1.1053 (1.1791)\tPrec@1 62.500 (57.600)\n",
      "Epoch: [3][275/391]\tLoss 1.1003 (1.1684)\tPrec@1 62.500 (58.070)\n",
      "Epoch: [3][330/391]\tLoss 1.2116 (1.1513)\tPrec@1 57.812 (58.606)\n",
      "Epoch: [3][385/391]\tLoss 1.1349 (1.1377)\tPrec@1 60.156 (59.177)\n",
      "Test\t  Prec@1: 60.550 (Err: 39.450 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.9854 (0.9854)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [4][55/391]\tLoss 1.0267 (1.0309)\tPrec@1 64.062 (64.035)\n",
      "Epoch: [4][110/391]\tLoss 0.9296 (1.0093)\tPrec@1 66.406 (64.196)\n",
      "Epoch: [4][165/391]\tLoss 0.8793 (0.9917)\tPrec@1 74.219 (64.966)\n",
      "Epoch: [4][220/391]\tLoss 0.9051 (0.9847)\tPrec@1 71.875 (65.162)\n",
      "Epoch: [4][275/391]\tLoss 0.9972 (0.9742)\tPrec@1 64.844 (65.486)\n",
      "Epoch: [4][330/391]\tLoss 0.8913 (0.9668)\tPrec@1 68.750 (65.760)\n",
      "Epoch: [4][385/391]\tLoss 0.9080 (0.9563)\tPrec@1 64.844 (66.135)\n",
      "Test\t  Prec@1: 65.010 (Err: 34.990 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.8435 (0.8435)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [5][55/391]\tLoss 0.8574 (0.8770)\tPrec@1 67.969 (69.713)\n",
      "Epoch: [5][110/391]\tLoss 0.9046 (0.8635)\tPrec@1 67.969 (69.658)\n",
      "Epoch: [5][165/391]\tLoss 0.7131 (0.8594)\tPrec@1 72.656 (69.757)\n",
      "Epoch: [5][220/391]\tLoss 0.7081 (0.8538)\tPrec@1 71.875 (69.874)\n",
      "Epoch: [5][275/391]\tLoss 0.9120 (0.8469)\tPrec@1 65.625 (70.194)\n",
      "Epoch: [5][330/391]\tLoss 0.9334 (0.8385)\tPrec@1 63.281 (70.454)\n",
      "Epoch: [5][385/391]\tLoss 0.8366 (0.8299)\tPrec@1 73.438 (70.760)\n",
      "Test\t  Prec@1: 67.870 (Err: 32.130 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.7236 (0.7236)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [6][55/391]\tLoss 0.6750 (0.7529)\tPrec@1 75.000 (73.507)\n",
      "Epoch: [6][110/391]\tLoss 0.5884 (0.7598)\tPrec@1 81.250 (73.297)\n",
      "Epoch: [6][165/391]\tLoss 0.8053 (0.7620)\tPrec@1 74.219 (73.188)\n",
      "Epoch: [6][220/391]\tLoss 0.7685 (0.7500)\tPrec@1 75.781 (73.533)\n",
      "Epoch: [6][275/391]\tLoss 0.5806 (0.7476)\tPrec@1 77.344 (73.610)\n",
      "Epoch: [6][330/391]\tLoss 0.5991 (0.7448)\tPrec@1 83.594 (73.865)\n",
      "Epoch: [6][385/391]\tLoss 0.6846 (0.7412)\tPrec@1 73.438 (74.026)\n",
      "Test\t  Prec@1: 64.290 (Err: 35.710 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.6300 (0.6300)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [7][55/391]\tLoss 0.8036 (0.7022)\tPrec@1 71.875 (76.004)\n",
      "Epoch: [7][110/391]\tLoss 0.7042 (0.6889)\tPrec@1 77.344 (76.105)\n",
      "Epoch: [7][165/391]\tLoss 0.7730 (0.6916)\tPrec@1 73.438 (75.941)\n",
      "Epoch: [7][220/391]\tLoss 0.8316 (0.6859)\tPrec@1 69.531 (76.064)\n",
      "Epoch: [7][275/391]\tLoss 0.6445 (0.6804)\tPrec@1 78.125 (76.296)\n",
      "Epoch: [7][330/391]\tLoss 0.5630 (0.6759)\tPrec@1 78.125 (76.527)\n",
      "Epoch: [7][385/391]\tLoss 0.7481 (0.6746)\tPrec@1 75.781 (76.589)\n",
      "Test\t  Prec@1: 77.490 (Err: 22.510 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5543 (0.5543)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [8][55/391]\tLoss 0.7613 (0.6075)\tPrec@1 72.656 (79.143)\n",
      "Epoch: [8][110/391]\tLoss 0.6415 (0.6197)\tPrec@1 75.781 (78.780)\n",
      "Epoch: [8][165/391]\tLoss 0.7319 (0.6255)\tPrec@1 72.656 (78.304)\n",
      "Epoch: [8][220/391]\tLoss 0.6064 (0.6275)\tPrec@1 78.906 (78.210)\n",
      "Epoch: [8][275/391]\tLoss 0.7380 (0.6263)\tPrec@1 72.656 (78.303)\n",
      "Epoch: [8][330/391]\tLoss 0.7162 (0.6230)\tPrec@1 78.125 (78.434)\n",
      "Epoch: [8][385/391]\tLoss 0.5745 (0.6245)\tPrec@1 76.562 (78.340)\n",
      "Test\t  Prec@1: 71.620 (Err: 28.380 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.6026 (0.6026)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [9][55/391]\tLoss 0.5602 (0.5641)\tPrec@1 82.031 (80.539)\n",
      "Epoch: [9][110/391]\tLoss 0.6662 (0.5775)\tPrec@1 78.125 (79.863)\n",
      "Epoch: [9][165/391]\tLoss 0.5159 (0.5808)\tPrec@1 79.688 (79.735)\n",
      "Epoch: [9][220/391]\tLoss 0.6420 (0.5781)\tPrec@1 73.438 (79.829)\n",
      "Epoch: [9][275/391]\tLoss 0.5030 (0.5751)\tPrec@1 80.469 (79.852)\n",
      "Epoch: [9][330/391]\tLoss 0.5091 (0.5747)\tPrec@1 82.812 (79.997)\n",
      "Epoch: [9][385/391]\tLoss 0.7030 (0.5754)\tPrec@1 75.781 (80.092)\n",
      "Test\t  Prec@1: 76.470 (Err: 23.530 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4731 (0.4731)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.6635 (0.5501)\tPrec@1 79.688 (80.985)\n",
      "Epoch: [10][110/391]\tLoss 0.4972 (0.5476)\tPrec@1 83.594 (80.976)\n",
      "Epoch: [10][165/391]\tLoss 0.5908 (0.5482)\tPrec@1 84.375 (80.991)\n",
      "Epoch: [10][220/391]\tLoss 0.6904 (0.5498)\tPrec@1 71.094 (80.925)\n",
      "Epoch: [10][275/391]\tLoss 0.3710 (0.5462)\tPrec@1 88.281 (81.060)\n",
      "Epoch: [10][330/391]\tLoss 0.6175 (0.5450)\tPrec@1 78.125 (81.137)\n",
      "Epoch: [10][385/391]\tLoss 0.4567 (0.5404)\tPrec@1 85.156 (81.321)\n",
      "Test\t  Prec@1: 79.950 (Err: 20.050 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.5511 (0.5511)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [11][55/391]\tLoss 0.5131 (0.5041)\tPrec@1 83.594 (82.157)\n",
      "Epoch: [11][110/391]\tLoss 0.5804 (0.5089)\tPrec@1 80.469 (82.066)\n",
      "Epoch: [11][165/391]\tLoss 0.4302 (0.5247)\tPrec@1 87.500 (81.824)\n",
      "Epoch: [11][220/391]\tLoss 0.6112 (0.5213)\tPrec@1 74.219 (81.794)\n",
      "Epoch: [11][275/391]\tLoss 0.5484 (0.5186)\tPrec@1 80.469 (81.819)\n",
      "Epoch: [11][330/391]\tLoss 0.4328 (0.5166)\tPrec@1 86.719 (81.991)\n",
      "Epoch: [11][385/391]\tLoss 0.5221 (0.5140)\tPrec@1 85.938 (82.082)\n",
      "Test\t  Prec@1: 79.440 (Err: 20.560 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.4461 (0.4461)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [12][55/391]\tLoss 0.6361 (0.5079)\tPrec@1 80.469 (82.254)\n",
      "Epoch: [12][110/391]\tLoss 0.5114 (0.4972)\tPrec@1 79.688 (82.475)\n",
      "Epoch: [12][165/391]\tLoss 0.3792 (0.4957)\tPrec@1 85.938 (82.676)\n",
      "Epoch: [12][220/391]\tLoss 0.4172 (0.4941)\tPrec@1 82.031 (82.820)\n",
      "Epoch: [12][275/391]\tLoss 0.6426 (0.4951)\tPrec@1 78.906 (82.716)\n",
      "Epoch: [12][330/391]\tLoss 0.3890 (0.4961)\tPrec@1 83.594 (82.775)\n",
      "Epoch: [12][385/391]\tLoss 0.4601 (0.4923)\tPrec@1 84.375 (82.889)\n",
      "Test\t  Prec@1: 82.210 (Err: 17.790 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.3079 (0.3079)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [13][55/391]\tLoss 0.5727 (0.4546)\tPrec@1 79.688 (84.780)\n",
      "Epoch: [13][110/391]\tLoss 0.4999 (0.4676)\tPrec@1 84.375 (84.185)\n",
      "Epoch: [13][165/391]\tLoss 0.5060 (0.4639)\tPrec@1 85.156 (84.191)\n",
      "Epoch: [13][220/391]\tLoss 0.4865 (0.4662)\tPrec@1 82.031 (84.053)\n",
      "Epoch: [13][275/391]\tLoss 0.4619 (0.4712)\tPrec@1 82.031 (83.849)\n",
      "Epoch: [13][330/391]\tLoss 0.5025 (0.4718)\tPrec@1 83.594 (83.771)\n",
      "Epoch: [13][385/391]\tLoss 0.5953 (0.4733)\tPrec@1 78.125 (83.725)\n",
      "Test\t  Prec@1: 82.530 (Err: 17.470 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.5493 (0.5493)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [14][55/391]\tLoss 0.4233 (0.4417)\tPrec@1 86.719 (85.045)\n",
      "Epoch: [14][110/391]\tLoss 0.3354 (0.4268)\tPrec@1 87.500 (85.480)\n",
      "Epoch: [14][165/391]\tLoss 0.4070 (0.4356)\tPrec@1 84.375 (85.100)\n",
      "Epoch: [14][220/391]\tLoss 0.4248 (0.4340)\tPrec@1 85.938 (85.146)\n",
      "Epoch: [14][275/391]\tLoss 0.4984 (0.4384)\tPrec@1 81.250 (84.998)\n",
      "Epoch: [14][330/391]\tLoss 0.4875 (0.4445)\tPrec@1 82.031 (84.800)\n",
      "Epoch: [14][385/391]\tLoss 0.4465 (0.4408)\tPrec@1 84.375 (84.893)\n",
      "Test\t  Prec@1: 81.230 (Err: 18.770 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3812 (0.3812)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [15][55/391]\tLoss 0.3964 (0.4252)\tPrec@1 85.938 (85.714)\n",
      "Epoch: [15][110/391]\tLoss 0.5824 (0.4328)\tPrec@1 80.469 (85.191)\n",
      "Epoch: [15][165/391]\tLoss 0.4125 (0.4323)\tPrec@1 85.938 (85.119)\n",
      "Epoch: [15][220/391]\tLoss 0.4029 (0.4318)\tPrec@1 89.844 (85.061)\n",
      "Epoch: [15][275/391]\tLoss 0.3164 (0.4302)\tPrec@1 90.625 (85.182)\n",
      "Epoch: [15][330/391]\tLoss 0.4015 (0.4315)\tPrec@1 83.594 (85.083)\n",
      "Epoch: [15][385/391]\tLoss 0.5330 (0.4304)\tPrec@1 82.812 (85.124)\n",
      "Test\t  Prec@1: 81.610 (Err: 18.390 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.4440 (0.4440)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [16][55/391]\tLoss 0.3845 (0.3914)\tPrec@1 85.938 (86.747)\n",
      "Epoch: [16][110/391]\tLoss 0.4873 (0.4040)\tPrec@1 83.594 (86.149)\n",
      "Epoch: [16][165/391]\tLoss 0.3934 (0.4013)\tPrec@1 85.156 (86.215)\n",
      "Epoch: [16][220/391]\tLoss 0.4428 (0.4011)\tPrec@1 84.375 (86.150)\n",
      "Epoch: [16][275/391]\tLoss 0.4166 (0.4059)\tPrec@1 87.500 (85.963)\n",
      "Epoch: [16][330/391]\tLoss 0.3456 (0.4051)\tPrec@1 89.062 (85.994)\n",
      "Epoch: [16][385/391]\tLoss 0.4502 (0.4059)\tPrec@1 85.156 (85.938)\n",
      "Test\t  Prec@1: 82.410 (Err: 17.590 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.3704 (0.3704)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [17][55/391]\tLoss 0.4392 (0.3792)\tPrec@1 82.812 (86.733)\n",
      "Epoch: [17][110/391]\tLoss 0.5388 (0.3904)\tPrec@1 82.031 (86.494)\n",
      "Epoch: [17][165/391]\tLoss 0.3279 (0.3950)\tPrec@1 88.281 (86.314)\n",
      "Epoch: [17][220/391]\tLoss 0.5338 (0.4032)\tPrec@1 82.031 (86.079)\n",
      "Epoch: [17][275/391]\tLoss 0.4251 (0.4022)\tPrec@1 84.375 (86.136)\n",
      "Epoch: [17][330/391]\tLoss 0.4225 (0.4032)\tPrec@1 85.156 (86.070)\n",
      "Epoch: [17][385/391]\tLoss 0.5422 (0.4083)\tPrec@1 82.031 (85.865)\n",
      "Test\t  Prec@1: 81.330 (Err: 18.670 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3136 (0.3136)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [18][55/391]\tLoss 0.4079 (0.3652)\tPrec@1 86.719 (87.040)\n",
      "Epoch: [18][110/391]\tLoss 0.3798 (0.3696)\tPrec@1 88.281 (87.099)\n",
      "Epoch: [18][165/391]\tLoss 0.3331 (0.3761)\tPrec@1 90.625 (86.893)\n",
      "Epoch: [18][220/391]\tLoss 0.4999 (0.3831)\tPrec@1 85.156 (86.669)\n",
      "Epoch: [18][275/391]\tLoss 0.3932 (0.3896)\tPrec@1 88.281 (86.416)\n",
      "Epoch: [18][330/391]\tLoss 0.4164 (0.3894)\tPrec@1 85.156 (86.433)\n",
      "Epoch: [18][385/391]\tLoss 0.4194 (0.3904)\tPrec@1 84.375 (86.413)\n",
      "Test\t  Prec@1: 83.230 (Err: 16.770 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3569 (0.3569)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [19][55/391]\tLoss 0.3628 (0.3578)\tPrec@1 87.500 (87.667)\n",
      "Epoch: [19][110/391]\tLoss 0.4142 (0.3813)\tPrec@1 85.938 (86.930)\n",
      "Epoch: [19][165/391]\tLoss 0.3147 (0.3777)\tPrec@1 89.062 (87.067)\n",
      "Epoch: [19][220/391]\tLoss 0.4201 (0.3800)\tPrec@1 85.156 (86.888)\n",
      "Epoch: [19][275/391]\tLoss 0.3146 (0.3794)\tPrec@1 89.062 (86.770)\n",
      "Epoch: [19][330/391]\tLoss 0.3692 (0.3786)\tPrec@1 83.594 (86.792)\n",
      "Epoch: [19][385/391]\tLoss 0.3148 (0.3776)\tPrec@1 89.844 (86.869)\n",
      "Test\t  Prec@1: 81.530 (Err: 18.470 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.4828 (0.4828)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [20][55/391]\tLoss 0.2384 (0.3666)\tPrec@1 93.750 (87.570)\n",
      "Epoch: [20][110/391]\tLoss 0.2966 (0.3563)\tPrec@1 90.625 (87.669)\n",
      "Epoch: [20][165/391]\tLoss 0.4119 (0.3604)\tPrec@1 85.156 (87.467)\n",
      "Epoch: [20][220/391]\tLoss 0.3879 (0.3632)\tPrec@1 84.375 (87.422)\n",
      "Epoch: [20][275/391]\tLoss 0.3602 (0.3658)\tPrec@1 90.625 (87.291)\n",
      "Epoch: [20][330/391]\tLoss 0.3968 (0.3614)\tPrec@1 86.719 (87.408)\n",
      "Epoch: [20][385/391]\tLoss 0.3521 (0.3629)\tPrec@1 89.062 (87.314)\n",
      "Test\t  Prec@1: 83.480 (Err: 16.520 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.3119 (0.3119)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [21][55/391]\tLoss 0.3030 (0.3417)\tPrec@1 86.719 (87.974)\n",
      "Epoch: [21][110/391]\tLoss 0.4586 (0.3410)\tPrec@1 84.375 (88.373)\n",
      "Epoch: [21][165/391]\tLoss 0.3401 (0.3397)\tPrec@1 85.938 (88.484)\n",
      "Epoch: [21][220/391]\tLoss 0.3217 (0.3490)\tPrec@1 86.719 (88.080)\n",
      "Epoch: [21][275/391]\tLoss 0.3499 (0.3493)\tPrec@1 87.500 (88.021)\n",
      "Epoch: [21][330/391]\tLoss 0.3841 (0.3531)\tPrec@1 85.156 (87.750)\n",
      "Epoch: [21][385/391]\tLoss 0.3285 (0.3554)\tPrec@1 84.375 (87.696)\n",
      "Test\t  Prec@1: 84.370 (Err: 15.630 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.4911 (0.4911)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [22][55/391]\tLoss 0.3350 (0.3315)\tPrec@1 87.500 (88.574)\n",
      "Epoch: [22][110/391]\tLoss 0.3429 (0.3466)\tPrec@1 88.281 (88.007)\n",
      "Epoch: [22][165/391]\tLoss 0.3285 (0.3455)\tPrec@1 84.375 (88.027)\n",
      "Epoch: [22][220/391]\tLoss 0.3210 (0.3430)\tPrec@1 84.375 (88.175)\n",
      "Epoch: [22][275/391]\tLoss 0.2877 (0.3456)\tPrec@1 90.625 (88.089)\n",
      "Epoch: [22][330/391]\tLoss 0.2940 (0.3445)\tPrec@1 89.062 (88.130)\n",
      "Epoch: [22][385/391]\tLoss 0.3270 (0.3453)\tPrec@1 91.406 (88.063)\n",
      "Test\t  Prec@1: 84.110 (Err: 15.890 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2910 (0.2910)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [23][55/391]\tLoss 0.2453 (0.3148)\tPrec@1 91.406 (88.909)\n",
      "Epoch: [23][110/391]\tLoss 0.3553 (0.3279)\tPrec@1 88.281 (88.535)\n",
      "Epoch: [23][165/391]\tLoss 0.3606 (0.3299)\tPrec@1 89.062 (88.493)\n",
      "Epoch: [23][220/391]\tLoss 0.3355 (0.3335)\tPrec@1 86.719 (88.317)\n",
      "Epoch: [23][275/391]\tLoss 0.3249 (0.3369)\tPrec@1 85.938 (88.210)\n",
      "Epoch: [23][330/391]\tLoss 0.3920 (0.3354)\tPrec@1 85.938 (88.213)\n",
      "Epoch: [23][385/391]\tLoss 0.3609 (0.3353)\tPrec@1 89.062 (88.235)\n",
      "Test\t  Prec@1: 83.520 (Err: 16.480 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.3274 (0.3274)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [24][55/391]\tLoss 0.3416 (0.3162)\tPrec@1 91.406 (88.714)\n",
      "Epoch: [24][110/391]\tLoss 0.5240 (0.3259)\tPrec@1 80.469 (88.563)\n",
      "Epoch: [24][165/391]\tLoss 0.3695 (0.3328)\tPrec@1 85.156 (88.375)\n",
      "Epoch: [24][220/391]\tLoss 0.2634 (0.3352)\tPrec@1 90.625 (88.292)\n",
      "Epoch: [24][275/391]\tLoss 0.4297 (0.3320)\tPrec@1 85.156 (88.414)\n",
      "Epoch: [24][330/391]\tLoss 0.3863 (0.3323)\tPrec@1 86.719 (88.397)\n",
      "Epoch: [24][385/391]\tLoss 0.4023 (0.3333)\tPrec@1 84.375 (88.356)\n",
      "Test\t  Prec@1: 83.150 (Err: 16.850 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2022 (0.2022)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [25][55/391]\tLoss 0.2767 (0.2991)\tPrec@1 91.406 (89.314)\n",
      "Epoch: [25][110/391]\tLoss 0.3873 (0.3045)\tPrec@1 85.938 (89.281)\n",
      "Epoch: [25][165/391]\tLoss 0.4021 (0.3100)\tPrec@1 85.156 (89.157)\n",
      "Epoch: [25][220/391]\tLoss 0.3469 (0.3118)\tPrec@1 87.500 (89.165)\n",
      "Epoch: [25][275/391]\tLoss 0.2640 (0.3146)\tPrec@1 92.188 (89.091)\n",
      "Epoch: [25][330/391]\tLoss 0.3315 (0.3161)\tPrec@1 86.719 (89.041)\n",
      "Epoch: [25][385/391]\tLoss 0.2192 (0.3181)\tPrec@1 91.406 (88.935)\n",
      "Test\t  Prec@1: 83.850 (Err: 16.150 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.2978 (0.2978)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [26][55/391]\tLoss 0.2602 (0.3088)\tPrec@1 90.625 (89.342)\n",
      "Epoch: [26][110/391]\tLoss 0.2274 (0.3103)\tPrec@1 92.969 (89.231)\n",
      "Epoch: [26][165/391]\tLoss 0.1980 (0.3153)\tPrec@1 93.750 (89.128)\n",
      "Epoch: [26][220/391]\tLoss 0.2593 (0.3172)\tPrec@1 88.281 (89.098)\n",
      "Epoch: [26][275/391]\tLoss 0.1910 (0.3139)\tPrec@1 90.625 (89.196)\n",
      "Epoch: [26][330/391]\tLoss 0.2412 (0.3134)\tPrec@1 92.188 (89.230)\n",
      "Epoch: [26][385/391]\tLoss 0.2703 (0.3148)\tPrec@1 90.625 (89.127)\n",
      "Test\t  Prec@1: 79.790 (Err: 20.210 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.3580 (0.3580)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [27][55/391]\tLoss 0.2285 (0.2994)\tPrec@1 91.406 (89.523)\n",
      "Epoch: [27][110/391]\tLoss 0.3484 (0.3057)\tPrec@1 88.281 (89.288)\n",
      "Epoch: [27][165/391]\tLoss 0.2730 (0.3058)\tPrec@1 92.188 (89.326)\n",
      "Epoch: [27][220/391]\tLoss 0.2112 (0.3011)\tPrec@1 90.625 (89.483)\n",
      "Epoch: [27][275/391]\tLoss 0.2024 (0.2985)\tPrec@1 89.844 (89.558)\n",
      "Epoch: [27][330/391]\tLoss 0.2742 (0.3041)\tPrec@1 89.844 (89.332)\n",
      "Epoch: [27][385/391]\tLoss 0.4223 (0.3090)\tPrec@1 85.156 (89.208)\n",
      "Test\t  Prec@1: 84.420 (Err: 15.580 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.3696 (0.3696)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [28][55/391]\tLoss 0.3902 (0.2957)\tPrec@1 86.719 (89.872)\n",
      "Epoch: [28][110/391]\tLoss 0.2573 (0.3001)\tPrec@1 89.844 (89.513)\n",
      "Epoch: [28][165/391]\tLoss 0.2385 (0.2980)\tPrec@1 89.062 (89.674)\n",
      "Epoch: [28][220/391]\tLoss 0.2826 (0.3018)\tPrec@1 88.281 (89.480)\n",
      "Epoch: [28][275/391]\tLoss 0.2968 (0.3019)\tPrec@1 88.281 (89.456)\n",
      "Epoch: [28][330/391]\tLoss 0.3234 (0.3008)\tPrec@1 91.406 (89.516)\n",
      "Epoch: [28][385/391]\tLoss 0.3207 (0.3030)\tPrec@1 87.500 (89.388)\n",
      "Test\t  Prec@1: 85.880 (Err: 14.120 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2903 (0.2903)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [29][55/391]\tLoss 0.3043 (0.2586)\tPrec@1 92.188 (91.239)\n",
      "Epoch: [29][110/391]\tLoss 0.3402 (0.2717)\tPrec@1 87.500 (90.632)\n",
      "Epoch: [29][165/391]\tLoss 0.2162 (0.2858)\tPrec@1 92.188 (90.027)\n",
      "Epoch: [29][220/391]\tLoss 0.3920 (0.2893)\tPrec@1 86.719 (89.748)\n",
      "Epoch: [29][275/391]\tLoss 0.2152 (0.2909)\tPrec@1 91.406 (89.821)\n",
      "Epoch: [29][330/391]\tLoss 0.3467 (0.2926)\tPrec@1 86.719 (89.782)\n",
      "Epoch: [29][385/391]\tLoss 0.2695 (0.2944)\tPrec@1 92.188 (89.803)\n",
      "Test\t  Prec@1: 84.650 (Err: 15.350 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2862 (0.2862)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [30][55/391]\tLoss 0.2371 (0.2830)\tPrec@1 93.750 (90.011)\n",
      "Epoch: [30][110/391]\tLoss 0.4465 (0.2961)\tPrec@1 83.594 (89.661)\n",
      "Epoch: [30][165/391]\tLoss 0.3302 (0.2918)\tPrec@1 89.844 (89.891)\n",
      "Epoch: [30][220/391]\tLoss 0.2434 (0.2903)\tPrec@1 92.969 (89.964)\n",
      "Epoch: [30][275/391]\tLoss 0.2468 (0.2897)\tPrec@1 89.844 (89.946)\n",
      "Epoch: [30][330/391]\tLoss 0.4015 (0.2941)\tPrec@1 85.156 (89.768)\n",
      "Epoch: [30][385/391]\tLoss 0.2204 (0.2952)\tPrec@1 93.750 (89.741)\n",
      "Test\t  Prec@1: 87.020 (Err: 12.980 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.2135 (0.2135)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [31][55/391]\tLoss 0.2891 (0.2560)\tPrec@1 92.188 (91.225)\n",
      "Epoch: [31][110/391]\tLoss 0.2366 (0.2631)\tPrec@1 92.969 (90.878)\n",
      "Epoch: [31][165/391]\tLoss 0.2897 (0.2660)\tPrec@1 86.719 (90.705)\n",
      "Epoch: [31][220/391]\tLoss 0.2224 (0.2792)\tPrec@1 92.188 (90.321)\n",
      "Epoch: [31][275/391]\tLoss 0.2420 (0.2784)\tPrec@1 90.625 (90.299)\n",
      "Epoch: [31][330/391]\tLoss 0.2534 (0.2803)\tPrec@1 92.188 (90.252)\n",
      "Epoch: [31][385/391]\tLoss 0.3658 (0.2828)\tPrec@1 89.062 (90.174)\n",
      "Test\t  Prec@1: 83.980 (Err: 16.020 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.4635 (0.4635)\tPrec@1 82.031 (82.031)\n",
      "Epoch: [32][55/391]\tLoss 0.2637 (0.2718)\tPrec@1 90.625 (90.778)\n",
      "Epoch: [32][110/391]\tLoss 0.1655 (0.2683)\tPrec@1 96.094 (90.878)\n",
      "Epoch: [32][165/391]\tLoss 0.2225 (0.2698)\tPrec@1 94.531 (90.639)\n",
      "Epoch: [32][220/391]\tLoss 0.1996 (0.2743)\tPrec@1 92.969 (90.537)\n",
      "Epoch: [32][275/391]\tLoss 0.2112 (0.2779)\tPrec@1 93.750 (90.331)\n",
      "Epoch: [32][330/391]\tLoss 0.2509 (0.2777)\tPrec@1 91.406 (90.290)\n",
      "Epoch: [32][385/391]\tLoss 0.2203 (0.2796)\tPrec@1 92.188 (90.234)\n",
      "Test\t  Prec@1: 86.680 (Err: 13.320 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.1484 (0.1484)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [33][55/391]\tLoss 0.2844 (0.2655)\tPrec@1 90.625 (90.932)\n",
      "Epoch: [33][110/391]\tLoss 0.3613 (0.2741)\tPrec@1 90.625 (90.660)\n",
      "Epoch: [33][165/391]\tLoss 0.3556 (0.2766)\tPrec@1 88.281 (90.489)\n",
      "Epoch: [33][220/391]\tLoss 0.2823 (0.2759)\tPrec@1 92.188 (90.466)\n",
      "Epoch: [33][275/391]\tLoss 0.3150 (0.2756)\tPrec@1 89.062 (90.427)\n",
      "Epoch: [33][330/391]\tLoss 0.4234 (0.2786)\tPrec@1 85.938 (90.306)\n",
      "Epoch: [33][385/391]\tLoss 0.2424 (0.2812)\tPrec@1 92.969 (90.289)\n",
      "Test\t  Prec@1: 85.780 (Err: 14.220 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.2180 (0.2180)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [34][55/391]\tLoss 0.3191 (0.2592)\tPrec@1 86.719 (90.946)\n",
      "Epoch: [34][110/391]\tLoss 0.2200 (0.2568)\tPrec@1 91.406 (91.054)\n",
      "Epoch: [34][165/391]\tLoss 0.1931 (0.2635)\tPrec@1 92.188 (90.705)\n",
      "Epoch: [34][220/391]\tLoss 0.3533 (0.2651)\tPrec@1 88.281 (90.632)\n",
      "Epoch: [34][275/391]\tLoss 0.2180 (0.2674)\tPrec@1 91.406 (90.517)\n",
      "Epoch: [34][330/391]\tLoss 0.2502 (0.2694)\tPrec@1 92.188 (90.453)\n",
      "Epoch: [34][385/391]\tLoss 0.1712 (0.2698)\tPrec@1 94.531 (90.427)\n",
      "Test\t  Prec@1: 86.670 (Err: 13.330 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.2243 (0.2243)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [35][55/391]\tLoss 0.2538 (0.2641)\tPrec@1 91.406 (91.044)\n",
      "Epoch: [35][110/391]\tLoss 0.2114 (0.2645)\tPrec@1 92.969 (90.857)\n",
      "Epoch: [35][165/391]\tLoss 0.2784 (0.2674)\tPrec@1 89.062 (90.804)\n",
      "Epoch: [35][220/391]\tLoss 0.2669 (0.2691)\tPrec@1 88.281 (90.629)\n",
      "Epoch: [35][275/391]\tLoss 0.2114 (0.2705)\tPrec@1 92.188 (90.568)\n",
      "Epoch: [35][330/391]\tLoss 0.2434 (0.2702)\tPrec@1 94.531 (90.585)\n",
      "Epoch: [35][385/391]\tLoss 0.2676 (0.2672)\tPrec@1 92.188 (90.694)\n",
      "Test\t  Prec@1: 86.730 (Err: 13.270 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.1755 (0.1755)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [36][55/391]\tLoss 0.4178 (0.2539)\tPrec@1 86.719 (91.030)\n",
      "Epoch: [36][110/391]\tLoss 0.2096 (0.2606)\tPrec@1 91.406 (90.738)\n",
      "Epoch: [36][165/391]\tLoss 0.3331 (0.2630)\tPrec@1 91.406 (90.738)\n",
      "Epoch: [36][220/391]\tLoss 0.3188 (0.2654)\tPrec@1 88.281 (90.696)\n",
      "Epoch: [36][275/391]\tLoss 0.2397 (0.2652)\tPrec@1 92.969 (90.724)\n",
      "Epoch: [36][330/391]\tLoss 0.3260 (0.2676)\tPrec@1 89.062 (90.670)\n",
      "Epoch: [36][385/391]\tLoss 0.3994 (0.2685)\tPrec@1 86.719 (90.694)\n",
      "Test\t  Prec@1: 84.160 (Err: 15.840 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.3893 (0.3893)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [37][55/391]\tLoss 0.2894 (0.2533)\tPrec@1 88.281 (91.183)\n",
      "Epoch: [37][110/391]\tLoss 0.2683 (0.2601)\tPrec@1 89.844 (90.885)\n",
      "Epoch: [37][165/391]\tLoss 0.4002 (0.2602)\tPrec@1 89.062 (90.846)\n",
      "Epoch: [37][220/391]\tLoss 0.2438 (0.2588)\tPrec@1 90.625 (90.971)\n",
      "Epoch: [37][275/391]\tLoss 0.1663 (0.2608)\tPrec@1 92.969 (90.942)\n",
      "Epoch: [37][330/391]\tLoss 0.2260 (0.2601)\tPrec@1 91.406 (90.984)\n",
      "Epoch: [37][385/391]\tLoss 0.2875 (0.2590)\tPrec@1 90.625 (90.969)\n",
      "Test\t  Prec@1: 85.610 (Err: 14.390 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.2462 (0.2462)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.2495 (0.2347)\tPrec@1 92.188 (91.518)\n",
      "Epoch: [38][110/391]\tLoss 0.3847 (0.2419)\tPrec@1 87.500 (91.265)\n",
      "Epoch: [38][165/391]\tLoss 0.1542 (0.2486)\tPrec@1 92.969 (91.091)\n",
      "Epoch: [38][220/391]\tLoss 0.2115 (0.2488)\tPrec@1 90.625 (91.162)\n",
      "Epoch: [38][275/391]\tLoss 0.3012 (0.2495)\tPrec@1 88.281 (91.118)\n",
      "Epoch: [38][330/391]\tLoss 0.3167 (0.2542)\tPrec@1 87.500 (91.005)\n",
      "Epoch: [38][385/391]\tLoss 0.2836 (0.2566)\tPrec@1 89.844 (90.977)\n",
      "Test\t  Prec@1: 86.080 (Err: 13.920 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.3494 (0.3494)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [39][55/391]\tLoss 0.2898 (0.2451)\tPrec@1 89.844 (91.588)\n",
      "Epoch: [39][110/391]\tLoss 0.2327 (0.2538)\tPrec@1 92.969 (91.118)\n",
      "Epoch: [39][165/391]\tLoss 0.1876 (0.2516)\tPrec@1 92.188 (91.086)\n",
      "Epoch: [39][220/391]\tLoss 0.1738 (0.2514)\tPrec@1 93.750 (91.127)\n",
      "Epoch: [39][275/391]\tLoss 0.2140 (0.2525)\tPrec@1 91.406 (91.078)\n",
      "Epoch: [39][330/391]\tLoss 0.2352 (0.2544)\tPrec@1 93.750 (91.024)\n",
      "Epoch: [39][385/391]\tLoss 0.2137 (0.2553)\tPrec@1 91.406 (91.044)\n",
      "Test\t  Prec@1: 83.590 (Err: 16.410 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.2273 (0.2273)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [40][55/391]\tLoss 0.2011 (0.2237)\tPrec@1 92.188 (92.160)\n",
      "Epoch: [40][110/391]\tLoss 0.2609 (0.2372)\tPrec@1 91.406 (91.653)\n",
      "Epoch: [40][165/391]\tLoss 0.1776 (0.2358)\tPrec@1 92.969 (91.693)\n",
      "Epoch: [40][220/391]\tLoss 0.1551 (0.2364)\tPrec@1 94.531 (91.717)\n",
      "Epoch: [40][275/391]\tLoss 0.2891 (0.2420)\tPrec@1 88.281 (91.483)\n",
      "Epoch: [40][330/391]\tLoss 0.3334 (0.2439)\tPrec@1 90.625 (91.465)\n",
      "Epoch: [40][385/391]\tLoss 0.2652 (0.2463)\tPrec@1 90.625 (91.386)\n",
      "Test\t  Prec@1: 86.170 (Err: 13.830 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2309 (0.2309)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [41][55/391]\tLoss 0.2012 (0.2188)\tPrec@1 92.969 (92.243)\n",
      "Epoch: [41][110/391]\tLoss 0.2272 (0.2180)\tPrec@1 94.531 (92.413)\n",
      "Epoch: [41][165/391]\tLoss 0.2339 (0.2225)\tPrec@1 92.188 (92.258)\n",
      "Epoch: [41][220/391]\tLoss 0.2057 (0.2314)\tPrec@1 92.969 (91.982)\n",
      "Epoch: [41][275/391]\tLoss 0.3226 (0.2371)\tPrec@1 85.938 (91.766)\n",
      "Epoch: [41][330/391]\tLoss 0.1728 (0.2410)\tPrec@1 92.969 (91.605)\n",
      "Epoch: [41][385/391]\tLoss 0.2766 (0.2420)\tPrec@1 89.844 (91.546)\n",
      "Test\t  Prec@1: 85.270 (Err: 14.730 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2486 (0.2486)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [42][55/391]\tLoss 0.2482 (0.2322)\tPrec@1 92.188 (91.602)\n",
      "Epoch: [42][110/391]\tLoss 0.2158 (0.2284)\tPrec@1 92.188 (91.899)\n",
      "Epoch: [42][165/391]\tLoss 0.4241 (0.2376)\tPrec@1 85.938 (91.703)\n",
      "Epoch: [42][220/391]\tLoss 0.1558 (0.2383)\tPrec@1 94.531 (91.565)\n",
      "Epoch: [42][275/391]\tLoss 0.2594 (0.2426)\tPrec@1 90.625 (91.435)\n",
      "Epoch: [42][330/391]\tLoss 0.3396 (0.2465)\tPrec@1 87.500 (91.276)\n",
      "Epoch: [42][385/391]\tLoss 0.2036 (0.2452)\tPrec@1 89.844 (91.301)\n",
      "Test\t  Prec@1: 86.890 (Err: 13.110 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.1121 (0.1121)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [43][55/391]\tLoss 0.1885 (0.2265)\tPrec@1 92.969 (92.174)\n",
      "Epoch: [43][110/391]\tLoss 0.1696 (0.2214)\tPrec@1 95.312 (92.603)\n",
      "Epoch: [43][165/391]\tLoss 0.3336 (0.2284)\tPrec@1 90.625 (92.211)\n",
      "Epoch: [43][220/391]\tLoss 0.2446 (0.2280)\tPrec@1 89.844 (92.159)\n",
      "Epoch: [43][275/391]\tLoss 0.1316 (0.2323)\tPrec@1 96.094 (91.984)\n",
      "Epoch: [43][330/391]\tLoss 0.1694 (0.2330)\tPrec@1 94.531 (91.902)\n",
      "Epoch: [43][385/391]\tLoss 0.4356 (0.2378)\tPrec@1 82.031 (91.696)\n",
      "Test\t  Prec@1: 84.750 (Err: 15.250 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.3332 (0.3332)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [44][55/391]\tLoss 0.1871 (0.2280)\tPrec@1 93.750 (91.922)\n",
      "Epoch: [44][110/391]\tLoss 0.2235 (0.2315)\tPrec@1 92.188 (92.012)\n",
      "Epoch: [44][165/391]\tLoss 0.3569 (0.2333)\tPrec@1 88.281 (91.924)\n",
      "Epoch: [44][220/391]\tLoss 0.2221 (0.2332)\tPrec@1 90.625 (91.876)\n",
      "Epoch: [44][275/391]\tLoss 0.1703 (0.2370)\tPrec@1 92.969 (91.715)\n",
      "Epoch: [44][330/391]\tLoss 0.2096 (0.2397)\tPrec@1 92.188 (91.614)\n",
      "Epoch: [44][385/391]\tLoss 0.2217 (0.2413)\tPrec@1 95.312 (91.611)\n",
      "Test\t  Prec@1: 84.330 (Err: 15.670 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.1467 (0.1467)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [45][55/391]\tLoss 0.1628 (0.2116)\tPrec@1 92.969 (92.425)\n",
      "Epoch: [45][110/391]\tLoss 0.3321 (0.2246)\tPrec@1 87.500 (92.152)\n",
      "Epoch: [45][165/391]\tLoss 0.2332 (0.2274)\tPrec@1 93.750 (92.060)\n",
      "Epoch: [45][220/391]\tLoss 0.2949 (0.2310)\tPrec@1 91.406 (91.905)\n",
      "Epoch: [45][275/391]\tLoss 0.2046 (0.2336)\tPrec@1 93.750 (91.890)\n",
      "Epoch: [45][330/391]\tLoss 0.1743 (0.2358)\tPrec@1 94.531 (91.784)\n",
      "Epoch: [45][385/391]\tLoss 0.2780 (0.2368)\tPrec@1 90.625 (91.728)\n",
      "Test\t  Prec@1: 83.100 (Err: 16.900 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2347 (0.2347)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [46][55/391]\tLoss 0.1536 (0.2107)\tPrec@1 94.531 (92.662)\n",
      "Epoch: [46][110/391]\tLoss 0.2311 (0.2134)\tPrec@1 92.969 (92.645)\n",
      "Epoch: [46][165/391]\tLoss 0.2474 (0.2121)\tPrec@1 90.625 (92.540)\n",
      "Epoch: [46][220/391]\tLoss 0.2484 (0.2218)\tPrec@1 92.188 (92.226)\n",
      "Epoch: [46][275/391]\tLoss 0.2719 (0.2254)\tPrec@1 87.500 (92.094)\n",
      "Epoch: [46][330/391]\tLoss 0.2917 (0.2258)\tPrec@1 90.625 (92.065)\n",
      "Epoch: [46][385/391]\tLoss 0.2317 (0.2296)\tPrec@1 92.969 (91.987)\n",
      "Test\t  Prec@1: 85.970 (Err: 14.030 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.2296 (0.2296)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [47][55/391]\tLoss 0.2158 (0.2101)\tPrec@1 91.406 (92.550)\n",
      "Epoch: [47][110/391]\tLoss 0.1970 (0.2200)\tPrec@1 92.969 (92.300)\n",
      "Epoch: [47][165/391]\tLoss 0.2626 (0.2305)\tPrec@1 88.281 (91.919)\n",
      "Epoch: [47][220/391]\tLoss 0.2046 (0.2293)\tPrec@1 92.969 (91.997)\n",
      "Epoch: [47][275/391]\tLoss 0.2391 (0.2321)\tPrec@1 91.406 (91.859)\n",
      "Epoch: [47][330/391]\tLoss 0.2430 (0.2307)\tPrec@1 94.531 (91.907)\n",
      "Epoch: [47][385/391]\tLoss 0.2272 (0.2318)\tPrec@1 89.844 (91.845)\n",
      "Test\t  Prec@1: 83.710 (Err: 16.290 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1050 (0.1050)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [48][55/391]\tLoss 0.1379 (0.2183)\tPrec@1 95.312 (92.201)\n",
      "Epoch: [48][110/391]\tLoss 0.3210 (0.2224)\tPrec@1 89.844 (92.202)\n",
      "Epoch: [48][165/391]\tLoss 0.1869 (0.2189)\tPrec@1 92.188 (92.249)\n",
      "Epoch: [48][220/391]\tLoss 0.2612 (0.2220)\tPrec@1 90.625 (92.149)\n",
      "Epoch: [48][275/391]\tLoss 0.1911 (0.2255)\tPrec@1 93.750 (92.052)\n",
      "Epoch: [48][330/391]\tLoss 0.1849 (0.2260)\tPrec@1 93.750 (92.003)\n",
      "Epoch: [48][385/391]\tLoss 0.2088 (0.2303)\tPrec@1 89.844 (91.866)\n",
      "Test\t  Prec@1: 85.110 (Err: 14.890 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2315 (0.2315)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [49][55/391]\tLoss 0.1877 (0.2150)\tPrec@1 93.750 (92.759)\n",
      "Epoch: [49][110/391]\tLoss 0.2490 (0.2166)\tPrec@1 90.625 (92.511)\n",
      "Epoch: [49][165/391]\tLoss 0.1914 (0.2152)\tPrec@1 92.969 (92.536)\n",
      "Epoch: [49][220/391]\tLoss 0.2514 (0.2231)\tPrec@1 90.625 (92.276)\n",
      "Epoch: [49][275/391]\tLoss 0.2017 (0.2233)\tPrec@1 95.312 (92.278)\n",
      "Epoch: [49][330/391]\tLoss 0.2225 (0.2258)\tPrec@1 90.625 (92.128)\n",
      "Epoch: [49][385/391]\tLoss 0.1472 (0.2270)\tPrec@1 93.750 (92.058)\n",
      "Test\t  Prec@1: 86.720 (Err: 13.280 )\n",
      "\n",
      "The lowest error from resnet44 model after 50 epochs is 12.980\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet56';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ba9ca613c00912bf2bb7336c6f7b766b0be232b7fbb6881178983a86316f18c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
