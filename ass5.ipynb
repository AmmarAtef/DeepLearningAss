{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet20\n",
      "Total number of params 269722\n",
      "Total layers 20\n",
      "\n",
      "resnet32\n",
      "Total number of params 464154\n",
      "Total layers 32\n",
      "\n",
      "resnet44\n",
      "Total number of params 658586\n",
      "Total layers 44\n",
      "\n",
      "resnet56\n",
      "Total number of params 853018\n",
      "Total layers 56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56']\n",
    " \n",
    "def _weights_init(m):\n",
    "    \"\"\"\n",
    "        Initialization of CNN weights\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
    "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
    "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
    "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 experiment, ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
    "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
    "# The subsampling is performed by convolutions with a stride of 2.\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for net_name in __all__:\n",
    "        if net_name.startswith('resnet'):\n",
    "            print(net_name)\n",
    "            test(globals()[net_name]())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNetArgs:\n",
    "       \"\"\"Passing the hyperparameters to the model\"\"\"\n",
    "       def __init__(self, arch='resnet20' ,epochs=50, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
    "                 evaluate=0, pretrained=0, half=0, save_dir='save_temp', save_every=10):\n",
    "          \n",
    "          self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "          self.save_dir = save_dir #The directory used to save the trained models\n",
    "          self.half = half #use half-precision(16-bit)\n",
    "          self.evaluate = evaluate #evaluate model on the validation set\n",
    "          self.pretrained = pretrained #evaluate the pretrained model on the validation set\n",
    "          self.print_freq = print_freq #print frequency \n",
    "          self.weight_decay = weight_decay\n",
    "          self.momentum = momentum \n",
    "          self.lr = lr #Learning rate\n",
    "          self.batch_size = batch_size \n",
    "          self.start_epoch = start_epoch\n",
    "          self.epochs = epochs\n",
    "          self.arch = arch #ResNet model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
      "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
      "        BasicBlock-7           [-1, 16, 32, 32]               0\n",
      "            Conv2d-8           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-9           [-1, 16, 32, 32]              32\n",
      "           Conv2d-10           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
      "       BasicBlock-12           [-1, 16, 32, 32]               0\n",
      "           Conv2d-13           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
      "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
      "       BasicBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
      "           Conv2d-20           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-21           [-1, 32, 16, 16]              64\n",
      "      LambdaLayer-22           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-23           [-1, 32, 16, 16]               0\n",
      "           Conv2d-24           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-25           [-1, 32, 16, 16]              64\n",
      "           Conv2d-26           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
      "       BasicBlock-28           [-1, 32, 16, 16]               0\n",
      "           Conv2d-29           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
      "           Conv2d-31           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
      "       BasicBlock-33           [-1, 32, 16, 16]               0\n",
      "           Conv2d-34             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
      "           Conv2d-36             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-37             [-1, 64, 8, 8]             128\n",
      "      LambdaLayer-38             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-39             [-1, 64, 8, 8]               0\n",
      "           Conv2d-40             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-41             [-1, 64, 8, 8]             128\n",
      "           Conv2d-42             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
      "       BasicBlock-44             [-1, 64, 8, 8]               0\n",
      "           Conv2d-45             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
      "           Conv2d-47             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-48             [-1, 64, 8, 8]             128\n",
      "       BasicBlock-49             [-1, 64, 8, 8]               0\n",
      "           Linear-50                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 269,722\n",
      "Trainable params: 269,722\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.63\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 4.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet20',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        if args.half:\n",
    "            input_var = input_var.half()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            if args.half:\n",
    "                input_var = input_var.half()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.th'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           frog            ship            frog             cat             car             car             cat           horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 179,
       "width": 1150
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=128, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=128, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "plt.figure(figsize=(20,10)) \n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
    "# print labels\n",
    "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    \n",
    "    # Check the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model =  globals()[args.arch]().to(device)\n",
    "    model.cuda()\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    if args.half:\n",
    "        print('half persicion is used.')\n",
    "        model.half()\n",
    "        criterion.half()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    if args.arch in ['resnet1202']:\n",
    "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
    "        # then switch back. In this setup it will correspond for first epoch.\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr*0.1\n",
    "\n",
    "\n",
    "    if args.evaluate:\n",
    "        print('evalution mode')\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    if args.pretrained:\n",
    "        print('evalution of pretrained model')\n",
    "        args.save_dir='pretrained_models'\n",
    "        pretrained_model= args.arch +'.th'\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        # train for one epoch\n",
    "        print('Training {} model'.format(args.arch))\n",
    "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
    "        if is_best:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
    "\n",
    "    return best_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 3.4544 (3.4544)\tPrec@1 12.500 (12.500)\n",
      "Epoch: [0][55/391]\tLoss 1.8573 (2.2423)\tPrec@1 34.375 (23.340)\n",
      "Epoch: [0][110/391]\tLoss 1.6454 (2.0287)\tPrec@1 32.031 (27.034)\n",
      "Epoch: [0][165/391]\tLoss 1.7298 (1.9146)\tPrec@1 31.250 (29.998)\n",
      "Epoch: [0][220/391]\tLoss 1.6435 (1.8355)\tPrec@1 42.969 (32.427)\n",
      "Epoch: [0][275/391]\tLoss 1.4351 (1.7723)\tPrec@1 46.094 (34.715)\n",
      "Epoch: [0][330/391]\tLoss 1.3060 (1.7193)\tPrec@1 50.000 (36.742)\n",
      "Epoch: [0][385/391]\tLoss 1.3827 (1.6690)\tPrec@1 50.781 (38.686)\n",
      "Test\t  Prec@1: 46.690 (Err: 53.310 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.3519 (1.3519)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [1][55/391]\tLoss 1.3445 (1.2831)\tPrec@1 55.469 (53.404)\n",
      "Epoch: [1][110/391]\tLoss 1.1492 (1.2664)\tPrec@1 60.938 (53.857)\n",
      "Epoch: [1][165/391]\tLoss 1.3132 (1.2421)\tPrec@1 52.344 (54.880)\n",
      "Epoch: [1][220/391]\tLoss 1.0740 (1.2104)\tPrec@1 60.156 (56.162)\n",
      "Epoch: [1][275/391]\tLoss 0.9410 (1.1906)\tPrec@1 62.500 (57.034)\n",
      "Epoch: [1][330/391]\tLoss 1.0818 (1.1715)\tPrec@1 61.719 (57.796)\n",
      "Epoch: [1][385/391]\tLoss 1.1638 (1.1544)\tPrec@1 58.594 (58.397)\n",
      "Test\t  Prec@1: 54.360 (Err: 45.640 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 0.8964 (0.8964)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [2][55/391]\tLoss 0.9574 (0.9775)\tPrec@1 66.406 (65.248)\n",
      "Epoch: [2][110/391]\tLoss 0.9966 (0.9836)\tPrec@1 67.188 (64.865)\n",
      "Epoch: [2][165/391]\tLoss 0.8253 (0.9637)\tPrec@1 77.344 (65.766)\n",
      "Epoch: [2][220/391]\tLoss 0.8270 (0.9500)\tPrec@1 69.531 (66.095)\n",
      "Epoch: [2][275/391]\tLoss 0.9213 (0.9421)\tPrec@1 64.062 (66.302)\n",
      "Epoch: [2][330/391]\tLoss 0.8662 (0.9283)\tPrec@1 72.656 (66.862)\n",
      "Epoch: [2][385/391]\tLoss 1.0143 (0.9190)\tPrec@1 65.625 (67.250)\n",
      "Test\t  Prec@1: 61.990 (Err: 38.010 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 0.6893 (0.6893)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [3][55/391]\tLoss 0.8883 (0.8366)\tPrec@1 67.969 (70.675)\n",
      "Epoch: [3][110/391]\tLoss 0.7991 (0.8238)\tPrec@1 72.656 (70.932)\n",
      "Epoch: [3][165/391]\tLoss 0.8910 (0.8090)\tPrec@1 70.312 (71.564)\n",
      "Epoch: [3][220/391]\tLoss 0.7534 (0.7969)\tPrec@1 75.000 (71.903)\n",
      "Epoch: [3][275/391]\tLoss 0.8457 (0.7944)\tPrec@1 71.094 (71.988)\n",
      "Epoch: [3][330/391]\tLoss 0.7919 (0.7943)\tPrec@1 73.438 (71.962)\n",
      "Epoch: [3][385/391]\tLoss 0.8581 (0.7859)\tPrec@1 66.406 (72.282)\n",
      "Test\t  Prec@1: 72.790 (Err: 27.210 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.7062 (0.7062)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [4][55/391]\tLoss 0.7937 (0.7220)\tPrec@1 72.656 (74.693)\n",
      "Epoch: [4][110/391]\tLoss 0.7592 (0.7114)\tPrec@1 74.219 (75.056)\n",
      "Epoch: [4][165/391]\tLoss 0.7743 (0.7094)\tPrec@1 75.000 (75.193)\n",
      "Epoch: [4][220/391]\tLoss 0.7624 (0.7007)\tPrec@1 75.000 (75.604)\n",
      "Epoch: [4][275/391]\tLoss 0.5974 (0.6984)\tPrec@1 78.125 (75.643)\n",
      "Epoch: [4][330/391]\tLoss 0.6328 (0.6961)\tPrec@1 78.125 (75.659)\n",
      "Epoch: [4][385/391]\tLoss 0.8808 (0.6925)\tPrec@1 67.969 (75.698)\n",
      "Test\t  Prec@1: 73.130 (Err: 26.870 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.6789 (0.6789)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [5][55/391]\tLoss 0.6551 (0.6403)\tPrec@1 78.906 (78.055)\n",
      "Epoch: [5][110/391]\tLoss 0.6383 (0.6406)\tPrec@1 78.125 (77.738)\n",
      "Epoch: [5][165/391]\tLoss 0.5606 (0.6363)\tPrec@1 82.031 (77.970)\n",
      "Epoch: [5][220/391]\tLoss 0.5731 (0.6289)\tPrec@1 78.125 (78.185)\n",
      "Epoch: [5][275/391]\tLoss 0.6216 (0.6252)\tPrec@1 76.562 (78.317)\n",
      "Epoch: [5][330/391]\tLoss 0.4477 (0.6257)\tPrec@1 85.938 (78.326)\n",
      "Epoch: [5][385/391]\tLoss 0.6091 (0.6256)\tPrec@1 77.344 (78.392)\n",
      "Test\t  Prec@1: 65.830 (Err: 34.170 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.8198 (0.8198)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [6][55/391]\tLoss 0.6810 (0.5855)\tPrec@1 75.000 (80.008)\n",
      "Epoch: [6][110/391]\tLoss 0.5702 (0.5824)\tPrec@1 81.250 (80.082)\n",
      "Epoch: [6][165/391]\tLoss 0.5068 (0.5898)\tPrec@1 83.594 (79.631)\n",
      "Epoch: [6][220/391]\tLoss 0.6401 (0.5888)\tPrec@1 80.469 (79.733)\n",
      "Epoch: [6][275/391]\tLoss 0.6112 (0.5883)\tPrec@1 75.781 (79.716)\n",
      "Epoch: [6][330/391]\tLoss 0.5481 (0.5924)\tPrec@1 82.031 (79.562)\n",
      "Epoch: [6][385/391]\tLoss 0.5307 (0.5876)\tPrec@1 81.250 (79.738)\n",
      "Test\t  Prec@1: 70.800 (Err: 29.200 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.6309 (0.6309)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [7][55/391]\tLoss 0.6007 (0.5513)\tPrec@1 82.031 (80.943)\n",
      "Epoch: [7][110/391]\tLoss 0.5359 (0.5602)\tPrec@1 81.250 (80.687)\n",
      "Epoch: [7][165/391]\tLoss 0.3524 (0.5534)\tPrec@1 85.156 (80.798)\n",
      "Epoch: [7][220/391]\tLoss 0.5828 (0.5473)\tPrec@1 78.906 (81.112)\n",
      "Epoch: [7][275/391]\tLoss 0.5170 (0.5469)\tPrec@1 81.250 (81.196)\n",
      "Epoch: [7][330/391]\tLoss 0.5535 (0.5469)\tPrec@1 82.031 (81.101)\n",
      "Epoch: [7][385/391]\tLoss 0.6659 (0.5437)\tPrec@1 77.344 (81.205)\n",
      "Test\t  Prec@1: 77.510 (Err: 22.490 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.3870 (0.3870)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [8][55/391]\tLoss 0.5468 (0.5221)\tPrec@1 84.375 (81.892)\n",
      "Epoch: [8][110/391]\tLoss 0.5425 (0.5242)\tPrec@1 79.688 (81.869)\n",
      "Epoch: [8][165/391]\tLoss 0.5277 (0.5191)\tPrec@1 81.250 (82.107)\n",
      "Epoch: [8][220/391]\tLoss 0.3109 (0.5249)\tPrec@1 91.406 (81.904)\n",
      "Epoch: [8][275/391]\tLoss 0.5776 (0.5239)\tPrec@1 80.469 (81.924)\n",
      "Epoch: [8][330/391]\tLoss 0.2763 (0.5186)\tPrec@1 90.625 (82.133)\n",
      "Epoch: [8][385/391]\tLoss 0.6303 (0.5200)\tPrec@1 80.469 (82.056)\n",
      "Test\t  Prec@1: 79.190 (Err: 20.810 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.6855 (0.6855)\tPrec@1 80.469 (80.469)\n",
      "Epoch: [9][55/391]\tLoss 0.3905 (0.4617)\tPrec@1 85.938 (84.235)\n",
      "Epoch: [9][110/391]\tLoss 0.4698 (0.4752)\tPrec@1 85.156 (83.685)\n",
      "Epoch: [9][165/391]\tLoss 0.3687 (0.4803)\tPrec@1 87.500 (83.504)\n",
      "Epoch: [9][220/391]\tLoss 0.5056 (0.4796)\tPrec@1 80.469 (83.569)\n",
      "Epoch: [9][275/391]\tLoss 0.4381 (0.4860)\tPrec@1 85.938 (83.291)\n",
      "Epoch: [9][330/391]\tLoss 0.3685 (0.4870)\tPrec@1 88.281 (83.306)\n",
      "Epoch: [9][385/391]\tLoss 0.5431 (0.4879)\tPrec@1 81.250 (83.244)\n",
      "Test\t  Prec@1: 77.970 (Err: 22.030 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.3834 (0.3834)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [10][55/391]\tLoss 0.4552 (0.4571)\tPrec@1 82.031 (84.501)\n",
      "Epoch: [10][110/391]\tLoss 0.4367 (0.4623)\tPrec@1 84.375 (83.910)\n",
      "Epoch: [10][165/391]\tLoss 0.3903 (0.4640)\tPrec@1 81.250 (83.806)\n",
      "Epoch: [10][220/391]\tLoss 0.4475 (0.4670)\tPrec@1 84.375 (83.682)\n",
      "Epoch: [10][275/391]\tLoss 0.5277 (0.4697)\tPrec@1 82.812 (83.614)\n",
      "Epoch: [10][330/391]\tLoss 0.4412 (0.4681)\tPrec@1 84.375 (83.712)\n",
      "Epoch: [10][385/391]\tLoss 0.4331 (0.4689)\tPrec@1 86.719 (83.725)\n",
      "Test\t  Prec@1: 81.290 (Err: 18.710 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.3395 (0.3395)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [11][55/391]\tLoss 0.3984 (0.4469)\tPrec@1 89.062 (84.361)\n",
      "Epoch: [11][110/391]\tLoss 0.5095 (0.4545)\tPrec@1 85.156 (84.157)\n",
      "Epoch: [11][165/391]\tLoss 0.4685 (0.4570)\tPrec@1 83.594 (84.111)\n",
      "Epoch: [11][220/391]\tLoss 0.4174 (0.4535)\tPrec@1 86.719 (84.244)\n",
      "Epoch: [11][275/391]\tLoss 0.4647 (0.4523)\tPrec@1 84.375 (84.290)\n",
      "Epoch: [11][330/391]\tLoss 0.4024 (0.4480)\tPrec@1 83.594 (84.399)\n",
      "Epoch: [11][385/391]\tLoss 0.3861 (0.4474)\tPrec@1 87.500 (84.430)\n",
      "Test\t  Prec@1: 78.400 (Err: 21.600 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.4943 (0.4943)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [12][55/391]\tLoss 0.4714 (0.4430)\tPrec@1 83.594 (84.584)\n",
      "Epoch: [12][110/391]\tLoss 0.4601 (0.4406)\tPrec@1 82.031 (84.692)\n",
      "Epoch: [12][165/391]\tLoss 0.4186 (0.4372)\tPrec@1 80.469 (84.794)\n",
      "Epoch: [12][220/391]\tLoss 0.4619 (0.4340)\tPrec@1 83.594 (84.909)\n",
      "Epoch: [12][275/391]\tLoss 0.4034 (0.4325)\tPrec@1 83.594 (85.035)\n",
      "Epoch: [12][330/391]\tLoss 0.4890 (0.4343)\tPrec@1 81.250 (84.996)\n",
      "Epoch: [12][385/391]\tLoss 0.5915 (0.4352)\tPrec@1 78.906 (84.938)\n",
      "Test\t  Prec@1: 81.020 (Err: 18.980 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.2875 (0.2875)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [13][55/391]\tLoss 0.3850 (0.4173)\tPrec@1 85.156 (85.463)\n",
      "Epoch: [13][110/391]\tLoss 0.4624 (0.4178)\tPrec@1 82.031 (85.564)\n",
      "Epoch: [13][165/391]\tLoss 0.4329 (0.4210)\tPrec@1 85.938 (85.448)\n",
      "Epoch: [13][220/391]\tLoss 0.5449 (0.4197)\tPrec@1 82.812 (85.506)\n",
      "Epoch: [13][275/391]\tLoss 0.3045 (0.4239)\tPrec@1 91.406 (85.295)\n",
      "Epoch: [13][330/391]\tLoss 0.3941 (0.4228)\tPrec@1 86.719 (85.404)\n",
      "Epoch: [13][385/391]\tLoss 0.3629 (0.4236)\tPrec@1 86.719 (85.353)\n",
      "Test\t  Prec@1: 80.700 (Err: 19.300 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3497 (0.3497)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [14][55/391]\tLoss 0.3253 (0.4146)\tPrec@1 89.844 (85.993)\n",
      "Epoch: [14][110/391]\tLoss 0.4661 (0.4018)\tPrec@1 85.938 (86.191)\n",
      "Epoch: [14][165/391]\tLoss 0.5008 (0.3988)\tPrec@1 85.156 (86.215)\n",
      "Epoch: [14][220/391]\tLoss 0.4054 (0.3997)\tPrec@1 82.812 (86.185)\n",
      "Epoch: [14][275/391]\tLoss 0.3330 (0.4036)\tPrec@1 90.625 (86.110)\n",
      "Epoch: [14][330/391]\tLoss 0.4030 (0.4040)\tPrec@1 83.594 (86.032)\n",
      "Epoch: [14][385/391]\tLoss 0.2286 (0.4063)\tPrec@1 90.625 (85.970)\n",
      "Test\t  Prec@1: 82.680 (Err: 17.320 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3111 (0.3111)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [15][55/391]\tLoss 0.4227 (0.3925)\tPrec@1 87.500 (86.230)\n",
      "Epoch: [15][110/391]\tLoss 0.4701 (0.3988)\tPrec@1 83.594 (85.902)\n",
      "Epoch: [15][165/391]\tLoss 0.3584 (0.3975)\tPrec@1 85.156 (86.107)\n",
      "Epoch: [15][220/391]\tLoss 0.2919 (0.3976)\tPrec@1 91.406 (86.178)\n",
      "Epoch: [15][275/391]\tLoss 0.5105 (0.3984)\tPrec@1 84.375 (86.184)\n",
      "Epoch: [15][330/391]\tLoss 0.4352 (0.3964)\tPrec@1 85.938 (86.280)\n",
      "Epoch: [15][385/391]\tLoss 0.4977 (0.3967)\tPrec@1 84.375 (86.336)\n",
      "Test\t  Prec@1: 82.820 (Err: 17.180 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.5393 (0.5393)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [16][55/391]\tLoss 0.3959 (0.4010)\tPrec@1 89.844 (86.286)\n",
      "Epoch: [16][110/391]\tLoss 0.3956 (0.3864)\tPrec@1 86.719 (86.740)\n",
      "Epoch: [16][165/391]\tLoss 0.2979 (0.3818)\tPrec@1 88.281 (86.883)\n",
      "Epoch: [16][220/391]\tLoss 0.4406 (0.3866)\tPrec@1 84.375 (86.712)\n",
      "Epoch: [16][275/391]\tLoss 0.2444 (0.3861)\tPrec@1 91.406 (86.671)\n",
      "Epoch: [16][330/391]\tLoss 0.3648 (0.3879)\tPrec@1 89.062 (86.563)\n",
      "Epoch: [16][385/391]\tLoss 0.3430 (0.3872)\tPrec@1 85.938 (86.589)\n",
      "Test\t  Prec@1: 79.170 (Err: 20.830 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.3179 (0.3179)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [17][55/391]\tLoss 0.3841 (0.3471)\tPrec@1 88.281 (88.114)\n",
      "Epoch: [17][110/391]\tLoss 0.3625 (0.3597)\tPrec@1 85.938 (87.542)\n",
      "Epoch: [17][165/391]\tLoss 0.3316 (0.3627)\tPrec@1 87.500 (87.382)\n",
      "Epoch: [17][220/391]\tLoss 0.4752 (0.3703)\tPrec@1 84.375 (87.090)\n",
      "Epoch: [17][275/391]\tLoss 0.3330 (0.3683)\tPrec@1 90.625 (87.090)\n",
      "Epoch: [17][330/391]\tLoss 0.3829 (0.3740)\tPrec@1 85.156 (86.957)\n",
      "Epoch: [17][385/391]\tLoss 0.4717 (0.3742)\tPrec@1 85.156 (86.923)\n",
      "Test\t  Prec@1: 77.800 (Err: 22.200 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.4410 (0.4410)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [18][55/391]\tLoss 0.3387 (0.3711)\tPrec@1 88.281 (87.123)\n",
      "Epoch: [18][110/391]\tLoss 0.4099 (0.3675)\tPrec@1 85.938 (87.134)\n",
      "Epoch: [18][165/391]\tLoss 0.5189 (0.3713)\tPrec@1 83.594 (87.039)\n",
      "Epoch: [18][220/391]\tLoss 0.3944 (0.3714)\tPrec@1 89.844 (87.055)\n",
      "Epoch: [18][275/391]\tLoss 0.3268 (0.3676)\tPrec@1 89.062 (87.200)\n",
      "Epoch: [18][330/391]\tLoss 0.2926 (0.3699)\tPrec@1 89.844 (87.089)\n",
      "Epoch: [18][385/391]\tLoss 0.3281 (0.3703)\tPrec@1 87.500 (87.095)\n",
      "Test\t  Prec@1: 82.980 (Err: 17.020 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3319 (0.3319)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [19][55/391]\tLoss 0.4205 (0.3579)\tPrec@1 89.062 (87.598)\n",
      "Epoch: [19][110/391]\tLoss 0.2746 (0.3525)\tPrec@1 89.062 (87.789)\n",
      "Epoch: [19][165/391]\tLoss 0.5358 (0.3520)\tPrec@1 80.469 (87.844)\n",
      "Epoch: [19][220/391]\tLoss 0.3840 (0.3598)\tPrec@1 88.281 (87.585)\n",
      "Epoch: [19][275/391]\tLoss 0.4131 (0.3601)\tPrec@1 84.375 (87.588)\n",
      "Epoch: [19][330/391]\tLoss 0.4038 (0.3616)\tPrec@1 86.719 (87.502)\n",
      "Epoch: [19][385/391]\tLoss 0.2329 (0.3598)\tPrec@1 92.188 (87.526)\n",
      "Test\t  Prec@1: 78.360 (Err: 21.640 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.3734 (0.3734)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [20][55/391]\tLoss 0.4396 (0.3422)\tPrec@1 81.250 (88.225)\n",
      "Epoch: [20][110/391]\tLoss 0.2562 (0.3448)\tPrec@1 92.188 (88.021)\n",
      "Epoch: [20][165/391]\tLoss 0.2250 (0.3486)\tPrec@1 92.188 (87.825)\n",
      "Epoch: [20][220/391]\tLoss 0.2786 (0.3502)\tPrec@1 85.938 (87.836)\n",
      "Epoch: [20][275/391]\tLoss 0.5402 (0.3491)\tPrec@1 82.812 (87.831)\n",
      "Epoch: [20][330/391]\tLoss 0.4272 (0.3525)\tPrec@1 85.938 (87.677)\n",
      "Epoch: [20][385/391]\tLoss 0.2927 (0.3530)\tPrec@1 89.844 (87.678)\n",
      "Test\t  Prec@1: 81.900 (Err: 18.100 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.3183 (0.3183)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [21][55/391]\tLoss 0.3883 (0.3380)\tPrec@1 83.594 (88.518)\n",
      "Epoch: [21][110/391]\tLoss 0.3367 (0.3477)\tPrec@1 88.281 (87.859)\n",
      "Epoch: [21][165/391]\tLoss 0.3720 (0.3487)\tPrec@1 84.375 (87.844)\n",
      "Epoch: [21][220/391]\tLoss 0.3341 (0.3482)\tPrec@1 89.062 (87.793)\n",
      "Epoch: [21][275/391]\tLoss 0.2725 (0.3490)\tPrec@1 90.625 (87.876)\n",
      "Epoch: [21][330/391]\tLoss 0.4783 (0.3501)\tPrec@1 82.812 (87.821)\n",
      "Epoch: [21][385/391]\tLoss 0.3598 (0.3511)\tPrec@1 85.938 (87.775)\n",
      "Test\t  Prec@1: 83.490 (Err: 16.510 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.3660 (0.3660)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [22][55/391]\tLoss 0.3616 (0.3330)\tPrec@1 85.938 (87.932)\n",
      "Epoch: [22][110/391]\tLoss 0.3204 (0.3377)\tPrec@1 87.500 (87.880)\n",
      "Epoch: [22][165/391]\tLoss 0.3504 (0.3455)\tPrec@1 86.719 (87.688)\n",
      "Epoch: [22][220/391]\tLoss 0.2806 (0.3482)\tPrec@1 89.844 (87.634)\n",
      "Epoch: [22][275/391]\tLoss 0.3684 (0.3445)\tPrec@1 91.406 (87.769)\n",
      "Epoch: [22][330/391]\tLoss 0.3538 (0.3421)\tPrec@1 88.281 (87.944)\n",
      "Epoch: [22][385/391]\tLoss 0.3670 (0.3445)\tPrec@1 90.625 (87.852)\n",
      "Test\t  Prec@1: 83.590 (Err: 16.410 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2849 (0.2849)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [23][55/391]\tLoss 0.3403 (0.3328)\tPrec@1 89.062 (88.393)\n",
      "Epoch: [23][110/391]\tLoss 0.2927 (0.3400)\tPrec@1 89.844 (88.155)\n",
      "Epoch: [23][165/391]\tLoss 0.4778 (0.3395)\tPrec@1 83.594 (88.211)\n",
      "Epoch: [23][220/391]\tLoss 0.2997 (0.3358)\tPrec@1 90.625 (88.345)\n",
      "Epoch: [23][275/391]\tLoss 0.3860 (0.3380)\tPrec@1 85.156 (88.261)\n",
      "Epoch: [23][330/391]\tLoss 0.2801 (0.3371)\tPrec@1 89.062 (88.295)\n",
      "Epoch: [23][385/391]\tLoss 0.3522 (0.3382)\tPrec@1 86.719 (88.259)\n",
      "Test\t  Prec@1: 84.550 (Err: 15.450 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.4031 (0.4031)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [24][55/391]\tLoss 0.3340 (0.3248)\tPrec@1 87.500 (88.867)\n",
      "Epoch: [24][110/391]\tLoss 0.3840 (0.3220)\tPrec@1 86.719 (88.851)\n",
      "Epoch: [24][165/391]\tLoss 0.3362 (0.3281)\tPrec@1 88.281 (88.695)\n",
      "Epoch: [24][220/391]\tLoss 0.3058 (0.3313)\tPrec@1 89.062 (88.550)\n",
      "Epoch: [24][275/391]\tLoss 0.3915 (0.3309)\tPrec@1 86.719 (88.420)\n",
      "Epoch: [24][330/391]\tLoss 0.2815 (0.3320)\tPrec@1 86.719 (88.366)\n",
      "Epoch: [24][385/391]\tLoss 0.1733 (0.3353)\tPrec@1 94.531 (88.295)\n",
      "Test\t  Prec@1: 83.540 (Err: 16.460 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.3200 (0.3200)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [25][55/391]\tLoss 0.3608 (0.3204)\tPrec@1 90.625 (89.049)\n",
      "Epoch: [25][110/391]\tLoss 0.3274 (0.3064)\tPrec@1 87.500 (89.499)\n",
      "Epoch: [25][165/391]\tLoss 0.2363 (0.3170)\tPrec@1 91.406 (88.997)\n",
      "Epoch: [25][220/391]\tLoss 0.3190 (0.3234)\tPrec@1 86.719 (88.723)\n",
      "Epoch: [25][275/391]\tLoss 0.3338 (0.3239)\tPrec@1 86.719 (88.737)\n",
      "Epoch: [25][330/391]\tLoss 0.3837 (0.3286)\tPrec@1 83.594 (88.484)\n",
      "Epoch: [25][385/391]\tLoss 0.4013 (0.3285)\tPrec@1 84.375 (88.502)\n",
      "Test\t  Prec@1: 83.830 (Err: 16.170 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.3517 (0.3517)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [26][55/391]\tLoss 0.3186 (0.3175)\tPrec@1 88.281 (88.825)\n",
      "Epoch: [26][110/391]\tLoss 0.3878 (0.3256)\tPrec@1 87.500 (88.774)\n",
      "Epoch: [26][165/391]\tLoss 0.3860 (0.3267)\tPrec@1 86.719 (88.648)\n",
      "Epoch: [26][220/391]\tLoss 0.2583 (0.3232)\tPrec@1 89.844 (88.833)\n",
      "Epoch: [26][275/391]\tLoss 0.3309 (0.3230)\tPrec@1 88.281 (88.847)\n",
      "Epoch: [26][330/391]\tLoss 0.5284 (0.3266)\tPrec@1 82.812 (88.713)\n",
      "Epoch: [26][385/391]\tLoss 0.3283 (0.3249)\tPrec@1 87.500 (88.785)\n",
      "Test\t  Prec@1: 81.430 (Err: 18.570 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.2758 (0.2758)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [27][55/391]\tLoss 0.3830 (0.3061)\tPrec@1 87.500 (89.160)\n",
      "Epoch: [27][110/391]\tLoss 0.2947 (0.3114)\tPrec@1 86.719 (88.915)\n",
      "Epoch: [27][165/391]\tLoss 0.2894 (0.3145)\tPrec@1 91.406 (88.888)\n",
      "Epoch: [27][220/391]\tLoss 0.3944 (0.3163)\tPrec@1 85.938 (88.861)\n",
      "Epoch: [27][275/391]\tLoss 0.1432 (0.3173)\tPrec@1 96.094 (88.879)\n",
      "Epoch: [27][330/391]\tLoss 0.2555 (0.3185)\tPrec@1 92.188 (88.862)\n",
      "Epoch: [27][385/391]\tLoss 0.4759 (0.3209)\tPrec@1 85.156 (88.747)\n",
      "Test\t  Prec@1: 84.190 (Err: 15.810 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.3314 (0.3314)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [28][55/391]\tLoss 0.1578 (0.2906)\tPrec@1 94.531 (89.844)\n",
      "Epoch: [28][110/391]\tLoss 0.2768 (0.2965)\tPrec@1 89.844 (89.724)\n",
      "Epoch: [28][165/391]\tLoss 0.2392 (0.2990)\tPrec@1 92.969 (89.557)\n",
      "Epoch: [28][220/391]\tLoss 0.2451 (0.3055)\tPrec@1 94.531 (89.437)\n",
      "Epoch: [28][275/391]\tLoss 0.3018 (0.3088)\tPrec@1 84.375 (89.331)\n",
      "Epoch: [28][330/391]\tLoss 0.2976 (0.3105)\tPrec@1 90.625 (89.251)\n",
      "Epoch: [28][385/391]\tLoss 0.3186 (0.3112)\tPrec@1 88.281 (89.198)\n",
      "Test\t  Prec@1: 84.760 (Err: 15.240 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2092 (0.2092)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [29][55/391]\tLoss 0.3607 (0.3174)\tPrec@1 86.719 (89.118)\n",
      "Epoch: [29][110/391]\tLoss 0.3760 (0.3166)\tPrec@1 88.281 (88.992)\n",
      "Epoch: [29][165/391]\tLoss 0.3054 (0.3161)\tPrec@1 92.969 (89.030)\n",
      "Epoch: [29][220/391]\tLoss 0.3734 (0.3122)\tPrec@1 89.062 (89.130)\n",
      "Epoch: [29][275/391]\tLoss 0.3679 (0.3091)\tPrec@1 87.500 (89.269)\n",
      "Epoch: [29][330/391]\tLoss 0.2408 (0.3085)\tPrec@1 92.188 (89.306)\n",
      "Epoch: [29][385/391]\tLoss 0.3201 (0.3101)\tPrec@1 86.719 (89.241)\n",
      "Test\t  Prec@1: 83.720 (Err: 16.280 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.3285 (0.3285)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [30][55/391]\tLoss 0.2547 (0.2903)\tPrec@1 89.844 (90.109)\n",
      "Epoch: [30][110/391]\tLoss 0.2244 (0.2959)\tPrec@1 92.969 (89.604)\n",
      "Epoch: [30][165/391]\tLoss 0.2963 (0.2972)\tPrec@1 87.500 (89.500)\n",
      "Epoch: [30][220/391]\tLoss 0.3363 (0.2965)\tPrec@1 87.500 (89.575)\n",
      "Epoch: [30][275/391]\tLoss 0.3241 (0.3061)\tPrec@1 87.500 (89.289)\n",
      "Epoch: [30][330/391]\tLoss 0.3933 (0.3073)\tPrec@1 88.281 (89.273)\n",
      "Epoch: [30][385/391]\tLoss 0.3584 (0.3104)\tPrec@1 88.281 (89.186)\n",
      "Test\t  Prec@1: 84.470 (Err: 15.530 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.2699 (0.2699)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [31][55/391]\tLoss 0.3878 (0.2777)\tPrec@1 88.281 (90.625)\n",
      "Epoch: [31][110/391]\tLoss 0.3847 (0.2781)\tPrec@1 87.500 (90.519)\n",
      "Epoch: [31][165/391]\tLoss 0.3106 (0.2826)\tPrec@1 89.844 (90.329)\n",
      "Epoch: [31][220/391]\tLoss 0.4335 (0.2883)\tPrec@1 87.500 (90.134)\n",
      "Epoch: [31][275/391]\tLoss 0.2463 (0.2909)\tPrec@1 89.844 (90.070)\n",
      "Epoch: [31][330/391]\tLoss 0.2800 (0.2958)\tPrec@1 87.500 (89.801)\n",
      "Epoch: [31][385/391]\tLoss 0.1808 (0.2986)\tPrec@1 96.094 (89.730)\n",
      "Test\t  Prec@1: 85.810 (Err: 14.190 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.4599 (0.4599)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [32][55/391]\tLoss 0.3175 (0.3002)\tPrec@1 86.719 (89.369)\n",
      "Epoch: [32][110/391]\tLoss 0.3671 (0.2907)\tPrec@1 87.500 (89.682)\n",
      "Epoch: [32][165/391]\tLoss 0.3359 (0.2947)\tPrec@1 86.719 (89.599)\n",
      "Epoch: [32][220/391]\tLoss 0.3033 (0.2933)\tPrec@1 88.281 (89.593)\n",
      "Epoch: [32][275/391]\tLoss 0.2058 (0.2940)\tPrec@1 94.531 (89.654)\n",
      "Epoch: [32][330/391]\tLoss 0.1943 (0.2988)\tPrec@1 94.531 (89.499)\n",
      "Epoch: [32][385/391]\tLoss 0.3024 (0.3021)\tPrec@1 86.719 (89.396)\n",
      "Test\t  Prec@1: 81.990 (Err: 18.010 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.3045 (0.3045)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [33][55/391]\tLoss 0.2494 (0.2903)\tPrec@1 92.188 (90.039)\n",
      "Epoch: [33][110/391]\tLoss 0.1650 (0.3008)\tPrec@1 95.312 (89.499)\n",
      "Epoch: [33][165/391]\tLoss 0.3428 (0.2951)\tPrec@1 86.719 (89.599)\n",
      "Epoch: [33][220/391]\tLoss 0.3525 (0.2943)\tPrec@1 88.281 (89.649)\n",
      "Epoch: [33][275/391]\tLoss 0.3469 (0.2937)\tPrec@1 89.844 (89.685)\n",
      "Epoch: [33][330/391]\tLoss 0.2431 (0.2943)\tPrec@1 91.406 (89.667)\n",
      "Epoch: [33][385/391]\tLoss 0.2250 (0.2945)\tPrec@1 93.750 (89.647)\n",
      "Test\t  Prec@1: 82.890 (Err: 17.110 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.3063 (0.3063)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [34][55/391]\tLoss 0.3379 (0.2694)\tPrec@1 87.500 (90.737)\n",
      "Epoch: [34][110/391]\tLoss 0.2883 (0.2704)\tPrec@1 91.406 (90.541)\n",
      "Epoch: [34][165/391]\tLoss 0.3321 (0.2738)\tPrec@1 88.281 (90.503)\n",
      "Epoch: [34][220/391]\tLoss 0.3599 (0.2804)\tPrec@1 86.719 (90.257)\n",
      "Epoch: [34][275/391]\tLoss 0.3359 (0.2852)\tPrec@1 92.188 (90.036)\n",
      "Epoch: [34][330/391]\tLoss 0.2450 (0.2882)\tPrec@1 92.969 (89.955)\n",
      "Epoch: [34][385/391]\tLoss 0.4259 (0.2889)\tPrec@1 88.281 (89.896)\n",
      "Test\t  Prec@1: 83.140 (Err: 16.860 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.2788 (0.2788)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [35][55/391]\tLoss 0.2420 (0.2871)\tPrec@1 91.406 (90.011)\n",
      "Epoch: [35][110/391]\tLoss 0.2709 (0.2909)\tPrec@1 89.062 (89.907)\n",
      "Epoch: [35][165/391]\tLoss 0.4113 (0.2883)\tPrec@1 86.719 (90.070)\n",
      "Epoch: [35][220/391]\tLoss 0.3195 (0.2908)\tPrec@1 87.500 (89.904)\n",
      "Epoch: [35][275/391]\tLoss 0.1942 (0.2896)\tPrec@1 92.969 (89.988)\n",
      "Epoch: [35][330/391]\tLoss 0.2471 (0.2921)\tPrec@1 91.406 (89.915)\n",
      "Epoch: [35][385/391]\tLoss 0.4430 (0.2935)\tPrec@1 85.938 (89.801)\n",
      "Test\t  Prec@1: 81.780 (Err: 18.220 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2149 (0.2149)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [36][55/391]\tLoss 0.2665 (0.2787)\tPrec@1 92.188 (90.067)\n",
      "Epoch: [36][110/391]\tLoss 0.3371 (0.2836)\tPrec@1 87.500 (90.076)\n",
      "Epoch: [36][165/391]\tLoss 0.3925 (0.2783)\tPrec@1 82.812 (90.286)\n",
      "Epoch: [36][220/391]\tLoss 0.3634 (0.2885)\tPrec@1 87.500 (89.975)\n",
      "Epoch: [36][275/391]\tLoss 0.2899 (0.2893)\tPrec@1 86.719 (89.895)\n",
      "Epoch: [36][330/391]\tLoss 0.2475 (0.2918)\tPrec@1 92.969 (89.853)\n",
      "Epoch: [36][385/391]\tLoss 0.3061 (0.2928)\tPrec@1 88.281 (89.767)\n",
      "Test\t  Prec@1: 81.070 (Err: 18.930 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.2039 (0.2039)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [37][55/391]\tLoss 0.4786 (0.2685)\tPrec@1 86.719 (90.388)\n",
      "Epoch: [37][110/391]\tLoss 0.1963 (0.2745)\tPrec@1 93.750 (90.189)\n",
      "Epoch: [37][165/391]\tLoss 0.3053 (0.2753)\tPrec@1 89.844 (90.281)\n",
      "Epoch: [37][220/391]\tLoss 0.2135 (0.2714)\tPrec@1 90.625 (90.455)\n",
      "Epoch: [37][275/391]\tLoss 0.4078 (0.2736)\tPrec@1 84.375 (90.379)\n",
      "Epoch: [37][330/391]\tLoss 0.3647 (0.2769)\tPrec@1 88.281 (90.285)\n",
      "Epoch: [37][385/391]\tLoss 0.3675 (0.2814)\tPrec@1 89.062 (90.107)\n",
      "Test\t  Prec@1: 84.540 (Err: 15.460 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.2958 (0.2958)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [38][55/391]\tLoss 0.1674 (0.2767)\tPrec@1 93.750 (90.402)\n",
      "Epoch: [38][110/391]\tLoss 0.2446 (0.2793)\tPrec@1 92.188 (90.336)\n",
      "Epoch: [38][165/391]\tLoss 0.2940 (0.2752)\tPrec@1 89.844 (90.418)\n",
      "Epoch: [38][220/391]\tLoss 0.2986 (0.2785)\tPrec@1 92.969 (90.173)\n",
      "Epoch: [38][275/391]\tLoss 0.3281 (0.2816)\tPrec@1 89.844 (90.110)\n",
      "Epoch: [38][330/391]\tLoss 0.2405 (0.2820)\tPrec@1 92.969 (90.179)\n",
      "Epoch: [38][385/391]\tLoss 0.3958 (0.2843)\tPrec@1 85.938 (90.099)\n",
      "Test\t  Prec@1: 83.430 (Err: 16.570 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.3653 (0.3653)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [39][55/391]\tLoss 0.4064 (0.2881)\tPrec@1 86.719 (90.332)\n",
      "Epoch: [39][110/391]\tLoss 0.2692 (0.2836)\tPrec@1 89.062 (90.301)\n",
      "Epoch: [39][165/391]\tLoss 0.3153 (0.2836)\tPrec@1 89.062 (90.225)\n",
      "Epoch: [39][220/391]\tLoss 0.3428 (0.2790)\tPrec@1 87.500 (90.363)\n",
      "Epoch: [39][275/391]\tLoss 0.2187 (0.2802)\tPrec@1 94.531 (90.305)\n",
      "Epoch: [39][330/391]\tLoss 0.3839 (0.2820)\tPrec@1 88.281 (90.177)\n",
      "Epoch: [39][385/391]\tLoss 0.2673 (0.2822)\tPrec@1 89.844 (90.180)\n",
      "Test\t  Prec@1: 83.940 (Err: 16.060 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.3104 (0.3104)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [40][55/391]\tLoss 0.2987 (0.2855)\tPrec@1 90.625 (90.513)\n",
      "Epoch: [40][110/391]\tLoss 0.3338 (0.2846)\tPrec@1 88.281 (90.259)\n",
      "Epoch: [40][165/391]\tLoss 0.2572 (0.2837)\tPrec@1 90.625 (90.164)\n",
      "Epoch: [40][220/391]\tLoss 0.2081 (0.2862)\tPrec@1 90.625 (90.031)\n",
      "Epoch: [40][275/391]\tLoss 0.2171 (0.2873)\tPrec@1 91.406 (89.923)\n",
      "Epoch: [40][330/391]\tLoss 0.1551 (0.2860)\tPrec@1 95.312 (89.955)\n",
      "Epoch: [40][385/391]\tLoss 0.2181 (0.2851)\tPrec@1 93.750 (89.991)\n",
      "Test\t  Prec@1: 86.110 (Err: 13.890 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.3489 (0.3489)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [41][55/391]\tLoss 0.2524 (0.2649)\tPrec@1 89.844 (90.792)\n",
      "Epoch: [41][110/391]\tLoss 0.2140 (0.2601)\tPrec@1 92.188 (90.942)\n",
      "Epoch: [41][165/391]\tLoss 0.2823 (0.2662)\tPrec@1 89.062 (90.724)\n",
      "Epoch: [41][220/391]\tLoss 0.2365 (0.2685)\tPrec@1 92.188 (90.579)\n",
      "Epoch: [41][275/391]\tLoss 0.2674 (0.2726)\tPrec@1 90.625 (90.447)\n",
      "Epoch: [41][330/391]\tLoss 0.3427 (0.2714)\tPrec@1 89.062 (90.486)\n",
      "Epoch: [41][385/391]\tLoss 0.3598 (0.2750)\tPrec@1 87.500 (90.342)\n",
      "Test\t  Prec@1: 85.850 (Err: 14.150 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2441 (0.2441)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [42][55/391]\tLoss 0.2121 (0.2669)\tPrec@1 92.969 (90.820)\n",
      "Epoch: [42][110/391]\tLoss 0.2405 (0.2771)\tPrec@1 92.188 (90.463)\n",
      "Epoch: [42][165/391]\tLoss 0.3686 (0.2741)\tPrec@1 85.156 (90.498)\n",
      "Epoch: [42][220/391]\tLoss 0.2842 (0.2734)\tPrec@1 90.625 (90.565)\n",
      "Epoch: [42][275/391]\tLoss 0.3596 (0.2757)\tPrec@1 85.156 (90.495)\n",
      "Epoch: [42][330/391]\tLoss 0.2701 (0.2739)\tPrec@1 91.406 (90.490)\n",
      "Epoch: [42][385/391]\tLoss 0.2553 (0.2766)\tPrec@1 91.406 (90.394)\n",
      "Test\t  Prec@1: 83.490 (Err: 16.510 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.1899 (0.1899)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [43][55/391]\tLoss 0.2899 (0.2584)\tPrec@1 92.188 (90.848)\n",
      "Epoch: [43][110/391]\tLoss 0.2995 (0.2599)\tPrec@1 89.062 (90.878)\n",
      "Epoch: [43][165/391]\tLoss 0.2810 (0.2610)\tPrec@1 89.062 (90.771)\n",
      "Epoch: [43][220/391]\tLoss 0.1789 (0.2645)\tPrec@1 94.531 (90.614)\n",
      "Epoch: [43][275/391]\tLoss 0.2562 (0.2660)\tPrec@1 92.188 (90.500)\n",
      "Epoch: [43][330/391]\tLoss 0.2392 (0.2707)\tPrec@1 94.531 (90.434)\n",
      "Epoch: [43][385/391]\tLoss 0.2853 (0.2740)\tPrec@1 88.281 (90.323)\n",
      "Test\t  Prec@1: 83.360 (Err: 16.640 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2492 (0.2492)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [44][55/391]\tLoss 0.2078 (0.2705)\tPrec@1 92.188 (90.639)\n",
      "Epoch: [44][110/391]\tLoss 0.1624 (0.2632)\tPrec@1 94.531 (90.724)\n",
      "Epoch: [44][165/391]\tLoss 0.3193 (0.2658)\tPrec@1 90.625 (90.719)\n",
      "Epoch: [44][220/391]\tLoss 0.2370 (0.2669)\tPrec@1 91.406 (90.706)\n",
      "Epoch: [44][275/391]\tLoss 0.4274 (0.2701)\tPrec@1 85.156 (90.662)\n",
      "Epoch: [44][330/391]\tLoss 0.3234 (0.2727)\tPrec@1 90.625 (90.533)\n",
      "Epoch: [44][385/391]\tLoss 0.4139 (0.2752)\tPrec@1 89.062 (90.475)\n",
      "Test\t  Prec@1: 86.230 (Err: 13.770 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.2358 (0.2358)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [45][55/391]\tLoss 0.3482 (0.2522)\tPrec@1 89.844 (90.946)\n",
      "Epoch: [45][110/391]\tLoss 0.2748 (0.2507)\tPrec@1 88.281 (90.977)\n",
      "Epoch: [45][165/391]\tLoss 0.2717 (0.2510)\tPrec@1 89.062 (90.917)\n",
      "Epoch: [45][220/391]\tLoss 0.1621 (0.2615)\tPrec@1 92.969 (90.522)\n",
      "Epoch: [45][275/391]\tLoss 0.2733 (0.2659)\tPrec@1 92.188 (90.410)\n",
      "Epoch: [45][330/391]\tLoss 0.2792 (0.2675)\tPrec@1 86.719 (90.405)\n",
      "Epoch: [45][385/391]\tLoss 0.1880 (0.2678)\tPrec@1 92.188 (90.421)\n",
      "Test\t  Prec@1: 84.540 (Err: 15.460 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2492 (0.2492)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [46][55/391]\tLoss 0.3284 (0.2530)\tPrec@1 90.625 (91.462)\n",
      "Epoch: [46][110/391]\tLoss 0.2238 (0.2534)\tPrec@1 92.969 (91.251)\n",
      "Epoch: [46][165/391]\tLoss 0.3943 (0.2606)\tPrec@1 89.062 (90.973)\n",
      "Epoch: [46][220/391]\tLoss 0.2324 (0.2626)\tPrec@1 91.406 (90.894)\n",
      "Epoch: [46][275/391]\tLoss 0.2337 (0.2649)\tPrec@1 90.625 (90.820)\n",
      "Epoch: [46][330/391]\tLoss 0.2112 (0.2657)\tPrec@1 92.188 (90.771)\n",
      "Epoch: [46][385/391]\tLoss 0.2082 (0.2663)\tPrec@1 93.750 (90.724)\n",
      "Test\t  Prec@1: 80.970 (Err: 19.030 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.2806 (0.2806)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [47][55/391]\tLoss 0.3315 (0.2464)\tPrec@1 89.062 (91.420)\n",
      "Epoch: [47][110/391]\tLoss 0.2519 (0.2519)\tPrec@1 89.062 (91.237)\n",
      "Epoch: [47][165/391]\tLoss 0.2745 (0.2557)\tPrec@1 88.281 (91.034)\n",
      "Epoch: [47][220/391]\tLoss 0.1953 (0.2591)\tPrec@1 90.625 (90.950)\n",
      "Epoch: [47][275/391]\tLoss 0.2318 (0.2633)\tPrec@1 92.188 (90.718)\n",
      "Epoch: [47][330/391]\tLoss 0.2444 (0.2673)\tPrec@1 92.188 (90.594)\n",
      "Epoch: [47][385/391]\tLoss 0.2841 (0.2683)\tPrec@1 89.844 (90.560)\n",
      "Test\t  Prec@1: 84.720 (Err: 15.280 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.2632 (0.2632)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [48][55/391]\tLoss 0.4684 (0.2465)\tPrec@1 84.375 (91.309)\n",
      "Epoch: [48][110/391]\tLoss 0.2357 (0.2382)\tPrec@1 89.844 (91.575)\n",
      "Epoch: [48][165/391]\tLoss 0.2642 (0.2449)\tPrec@1 89.062 (91.387)\n",
      "Epoch: [48][220/391]\tLoss 0.2421 (0.2511)\tPrec@1 89.062 (91.208)\n",
      "Epoch: [48][275/391]\tLoss 0.3523 (0.2557)\tPrec@1 85.938 (90.973)\n",
      "Epoch: [48][330/391]\tLoss 0.1936 (0.2580)\tPrec@1 93.750 (90.870)\n",
      "Epoch: [48][385/391]\tLoss 0.2582 (0.2610)\tPrec@1 89.844 (90.793)\n",
      "Test\t  Prec@1: 84.400 (Err: 15.600 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.1527 (0.1527)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [49][55/391]\tLoss 0.3252 (0.2632)\tPrec@1 92.188 (90.988)\n",
      "Epoch: [49][110/391]\tLoss 0.2735 (0.2559)\tPrec@1 92.969 (91.216)\n",
      "Epoch: [49][165/391]\tLoss 0.1452 (0.2610)\tPrec@1 94.531 (90.936)\n",
      "Epoch: [49][220/391]\tLoss 0.3313 (0.2635)\tPrec@1 90.625 (90.887)\n",
      "Epoch: [49][275/391]\tLoss 0.2184 (0.2615)\tPrec@1 92.188 (90.880)\n",
      "Epoch: [49][330/391]\tLoss 0.3577 (0.2628)\tPrec@1 86.719 (90.804)\n",
      "Epoch: [49][385/391]\tLoss 0.5225 (0.2627)\tPrec@1 83.594 (90.803)\n",
      "Test\t  Prec@1: 85.290 (Err: 14.710 )\n",
      "\n",
      "The lowest error from resnet20 model after 50 epochs is 13.770\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (c:\\Users\\Ammar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:301742)",
      "at S.execute (c:\\Users\\Ammar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:300732)",
      "at S.start (c:\\Users\\Ammar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Ammar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:312326)",
      "at t.CellExecutionQueue.start (c:\\Users\\Ammar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.rand(3,5)\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ba9ca613c00912bf2bb7336c6f7b766b0be232b7fbb6881178983a86316f18c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
