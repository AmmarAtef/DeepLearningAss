{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.159\n",
      "[1,  4000] loss: 1.782\n",
      "[1,  6000] loss: 1.608\n",
      "[1,  8000] loss: 1.516\n",
      "[1, 10000] loss: 1.447\n",
      "[1, 12000] loss: 1.390\n",
      "[2,  2000] loss: 1.328\n",
      "[2,  4000] loss: 1.300\n",
      "[2,  6000] loss: 1.251\n",
      "[2,  8000] loss: 1.251\n",
      "[2, 10000] loss: 1.222\n",
      "[2, 12000] loss: 1.183\n",
      "[3,  2000] loss: 1.124\n",
      "[3,  4000] loss: 1.134\n",
      "[3,  6000] loss: 1.113\n",
      "[3,  8000] loss: 1.072\n",
      "[3, 10000] loss: 1.101\n",
      "[3, 12000] loss: 1.066\n",
      "[4,  2000] loss: 0.995\n",
      "[4,  4000] loss: 1.020\n",
      "[4,  6000] loss: 0.961\n",
      "[4,  8000] loss: 1.010\n",
      "[4, 10000] loss: 1.010\n",
      "[4, 12000] loss: 0.985\n",
      "[5,  2000] loss: 0.910\n",
      "[5,  4000] loss: 0.941\n",
      "[5,  6000] loss: 0.922\n",
      "[5,  8000] loss: 0.933\n",
      "[5, 10000] loss: 0.937\n",
      "[5, 12000] loss: 0.914\n",
      "[6,  2000] loss: 0.851\n",
      "[6,  4000] loss: 0.868\n",
      "[6,  6000] loss: 0.887\n",
      "[6,  8000] loss: 0.852\n",
      "[6, 10000] loss: 0.906\n",
      "[6, 12000] loss: 0.876\n",
      "[7,  2000] loss: 0.799\n",
      "[7,  4000] loss: 0.813\n",
      "[7,  6000] loss: 0.836\n",
      "[7,  8000] loss: 0.837\n",
      "[7, 10000] loss: 0.835\n",
      "[7, 12000] loss: 0.826\n",
      "[8,  2000] loss: 0.740\n",
      "[8,  4000] loss: 0.790\n",
      "[8,  6000] loss: 0.796\n",
      "[8,  8000] loss: 0.808\n",
      "[8, 10000] loss: 0.806\n",
      "[8, 12000] loss: 0.790\n",
      "[9,  2000] loss: 0.714\n",
      "[9,  4000] loss: 0.753\n",
      "[9,  6000] loss: 0.763\n",
      "[9,  8000] loss: 0.779\n",
      "[9, 10000] loss: 0.757\n",
      "[9, 12000] loss: 0.772\n",
      "[10,  2000] loss: 0.677\n",
      "[10,  4000] loss: 0.702\n",
      "[10,  6000] loss: 0.724\n",
      "[10,  8000] loss: 0.726\n",
      "[10, 10000] loss: 0.748\n",
      "[10, 12000] loss: 0.750\n",
      "[11,  2000] loss: 0.659\n",
      "[11,  4000] loss: 0.678\n",
      "[11,  6000] loss: 0.694\n",
      "[11,  8000] loss: 0.714\n",
      "[11, 10000] loss: 0.729\n",
      "[11, 12000] loss: 0.727\n",
      "[12,  2000] loss: 0.617\n",
      "[12,  4000] loss: 0.637\n",
      "[12,  6000] loss: 0.678\n",
      "[12,  8000] loss: 0.709\n",
      "[12, 10000] loss: 0.694\n",
      "[12, 12000] loss: 0.716\n",
      "[13,  2000] loss: 0.587\n",
      "[13,  4000] loss: 0.636\n",
      "[13,  6000] loss: 0.648\n",
      "[13,  8000] loss: 0.669\n",
      "[13, 10000] loss: 0.687\n",
      "[13, 12000] loss: 0.712\n",
      "[14,  2000] loss: 0.583\n",
      "[14,  4000] loss: 0.618\n",
      "[14,  6000] loss: 0.639\n",
      "[14,  8000] loss: 0.647\n",
      "[14, 10000] loss: 0.667\n",
      "[14, 12000] loss: 0.660\n",
      "[15,  2000] loss: 0.579\n",
      "[15,  4000] loss: 0.600\n",
      "[15,  6000] loss: 0.626\n",
      "[15,  8000] loss: 0.646\n",
      "[15, 10000] loss: 0.649\n",
      "[15, 12000] loss: 0.650\n",
      "[16,  2000] loss: 0.562\n",
      "[16,  4000] loss: 0.569\n",
      "[16,  6000] loss: 0.585\n",
      "[16,  8000] loss: 0.633\n",
      "[16, 10000] loss: 0.636\n",
      "[16, 12000] loss: 0.633\n",
      "[17,  2000] loss: 0.527\n",
      "[17,  4000] loss: 0.590\n",
      "[17,  6000] loss: 0.598\n",
      "[17,  8000] loss: 0.580\n",
      "[17, 10000] loss: 0.623\n",
      "[17, 12000] loss: 0.633\n",
      "[18,  2000] loss: 0.528\n",
      "[18,  4000] loss: 0.544\n",
      "[18,  6000] loss: 0.595\n",
      "[18,  8000] loss: 0.603\n",
      "[18, 10000] loss: 0.606\n",
      "[18, 12000] loss: 0.621\n",
      "[19,  2000] loss: 0.524\n",
      "[19,  4000] loss: 0.537\n",
      "[19,  6000] loss: 0.573\n",
      "[19,  8000] loss: 0.573\n",
      "[19, 10000] loss: 0.588\n",
      "[19, 12000] loss: 0.596\n",
      "[20,  2000] loss: 0.497\n",
      "[20,  4000] loss: 0.544\n",
      "[20,  6000] loss: 0.586\n",
      "[20,  8000] loss: 0.580\n",
      "[20, 10000] loss: 0.584\n",
      "[20, 12000] loss: 0.579\n",
      "[21,  2000] loss: 0.501\n",
      "[21,  4000] loss: 0.514\n",
      "[21,  6000] loss: 0.560\n",
      "[21,  8000] loss: 0.586\n",
      "[21, 10000] loss: 0.580\n",
      "[21, 12000] loss: 0.591\n",
      "[22,  2000] loss: 0.480\n",
      "[22,  4000] loss: 0.522\n",
      "[22,  6000] loss: 0.559\n",
      "[22,  8000] loss: 0.542\n",
      "[22, 10000] loss: 0.604\n",
      "[22, 12000] loss: 0.577\n",
      "[23,  2000] loss: 0.482\n",
      "[23,  4000] loss: 0.511\n",
      "[23,  6000] loss: 0.549\n",
      "[23,  8000] loss: 0.563\n",
      "[23, 10000] loss: 0.559\n",
      "[23, 12000] loss: 0.574\n",
      "[24,  2000] loss: 0.471\n",
      "[24,  4000] loss: 0.511\n",
      "[24,  6000] loss: 0.544\n",
      "[24,  8000] loss: 0.522\n",
      "[24, 10000] loss: 0.582\n",
      "[24, 12000] loss: 0.568\n",
      "[25,  2000] loss: 0.472\n",
      "[25,  4000] loss: 0.510\n",
      "[25,  6000] loss: 0.503\n",
      "[25,  8000] loss: 0.550\n",
      "[25, 10000] loss: 0.552\n",
      "[25, 12000] loss: 0.556\n",
      "[26,  2000] loss: 0.461\n",
      "[26,  4000] loss: 0.518\n",
      "[26,  6000] loss: 0.505\n",
      "[26,  8000] loss: 0.538\n",
      "[26, 10000] loss: 0.579\n",
      "[26, 12000] loss: 0.565\n",
      "[27,  2000] loss: 0.440\n",
      "[27,  4000] loss: 0.491\n",
      "[27,  6000] loss: 0.489\n",
      "[27,  8000] loss: 0.524\n",
      "[27, 10000] loss: 0.542\n",
      "[27, 12000] loss: 0.564\n",
      "[28,  2000] loss: 0.469\n",
      "[28,  4000] loss: 0.490\n",
      "[28,  6000] loss: 0.502\n",
      "[28,  8000] loss: 0.531\n",
      "[28, 10000] loss: 0.538\n",
      "[28, 12000] loss: 0.563\n",
      "[29,  2000] loss: 0.425\n",
      "[29,  4000] loss: 0.489\n",
      "[29,  6000] loss: 0.490\n",
      "[29,  8000] loss: 0.534\n",
      "[29, 10000] loss: 0.536\n",
      "[29, 12000] loss: 0.540\n",
      "[30,  2000] loss: 0.453\n",
      "[30,  4000] loss: 0.481\n",
      "[30,  6000] loss: 0.503\n",
      "[30,  8000] loss: 0.527\n",
      "[30, 10000] loss: 0.518\n",
      "[30, 12000] loss: 0.566\n",
      "[31,  2000] loss: 0.445\n",
      "[31,  4000] loss: 0.484\n",
      "[31,  6000] loss: 0.498\n",
      "[31,  8000] loss: 0.514\n",
      "[31, 10000] loss: 0.556\n",
      "[31, 12000] loss: 0.536\n",
      "[32,  2000] loss: 0.451\n",
      "[32,  4000] loss: 0.483\n",
      "[32,  6000] loss: 0.487\n",
      "[32,  8000] loss: 0.507\n",
      "[32, 10000] loss: 0.536\n",
      "[32, 12000] loss: 0.531\n",
      "[33,  2000] loss: 0.434\n",
      "[33,  4000] loss: 0.488\n",
      "[33,  6000] loss: 0.513\n",
      "[33,  8000] loss: 0.510\n",
      "[33, 10000] loss: 0.514\n",
      "[33, 12000] loss: 0.520\n",
      "[34,  2000] loss: 0.419\n",
      "[34,  4000] loss: 0.463\n",
      "[34,  6000] loss: 0.518\n",
      "[34,  8000] loss: 0.494\n",
      "[34, 10000] loss: 0.528\n",
      "[34, 12000] loss: 0.546\n",
      "[35,  2000] loss: 0.447\n",
      "[35,  4000] loss: 0.460\n",
      "[35,  6000] loss: 0.503\n",
      "[35,  8000] loss: 0.489\n",
      "[35, 10000] loss: 0.525\n",
      "[35, 12000] loss: 0.531\n",
      "[36,  2000] loss: 0.427\n",
      "[36,  4000] loss: 0.458\n",
      "[36,  6000] loss: 0.489\n",
      "[36,  8000] loss: 0.503\n",
      "[36, 10000] loss: 0.513\n",
      "[36, 12000] loss: 0.540\n",
      "[37,  2000] loss: 0.411\n",
      "[37,  4000] loss: 0.481\n",
      "[37,  6000] loss: 0.501\n",
      "[37,  8000] loss: 0.503\n",
      "[37, 10000] loss: 0.537\n",
      "[37, 12000] loss: 0.517\n",
      "[38,  2000] loss: 0.430\n",
      "[38,  4000] loss: 0.461\n",
      "[38,  6000] loss: 0.492\n",
      "[38,  8000] loss: 0.527\n",
      "[38, 10000] loss: 0.520\n",
      "[38, 12000] loss: 0.500\n",
      "[39,  2000] loss: 0.454\n",
      "[39,  4000] loss: 0.466\n",
      "[39,  6000] loss: 0.483\n",
      "[39,  8000] loss: 0.501\n",
      "[39, 10000] loss: 0.549\n",
      "[39, 12000] loss: 0.545\n",
      "[40,  2000] loss: 0.415\n",
      "[40,  4000] loss: 0.441\n",
      "[40,  6000] loss: 0.490\n",
      "[40,  8000] loss: 0.476\n",
      "[40, 10000] loss: 0.560\n",
      "[40, 12000] loss: 0.558\n",
      "[41,  2000] loss: 0.417\n",
      "[41,  4000] loss: 0.458\n",
      "[41,  6000] loss: 0.490\n",
      "[41,  8000] loss: 0.540\n",
      "[41, 10000] loss: 0.525\n",
      "[41, 12000] loss: 0.557\n",
      "[42,  2000] loss: 0.422\n",
      "[42,  4000] loss: 0.452\n",
      "[42,  6000] loss: 0.505\n",
      "[42,  8000] loss: 0.516\n",
      "[42, 10000] loss: 0.504\n",
      "[42, 12000] loss: 0.526\n",
      "[43,  2000] loss: 0.419\n",
      "[43,  4000] loss: 0.453\n",
      "[43,  6000] loss: 0.506\n",
      "[43,  8000] loss: 0.501\n",
      "[43, 10000] loss: 0.510\n",
      "[43, 12000] loss: 0.526\n",
      "[44,  2000] loss: 0.414\n",
      "[44,  4000] loss: 0.466\n",
      "[44,  6000] loss: 0.465\n",
      "[44,  8000] loss: 0.474\n",
      "[44, 10000] loss: 0.524\n",
      "[44, 12000] loss: 0.484\n",
      "[45,  2000] loss: 0.412\n",
      "[45,  4000] loss: 0.456\n",
      "[45,  6000] loss: 0.461\n",
      "[45,  8000] loss: 0.481\n",
      "[45, 10000] loss: 0.517\n",
      "[45, 12000] loss: 0.531\n",
      "[46,  2000] loss: 0.380\n",
      "[46,  4000] loss: 0.497\n",
      "[46,  6000] loss: 0.478\n",
      "[46,  8000] loss: 0.532\n",
      "[46, 10000] loss: 0.540\n",
      "[46, 12000] loss: 0.518\n",
      "[47,  2000] loss: 0.437\n",
      "[47,  4000] loss: 0.450\n",
      "[47,  6000] loss: 0.518\n",
      "[47,  8000] loss: 0.499\n",
      "[47, 10000] loss: 0.535\n",
      "[47, 12000] loss: 0.498\n",
      "[48,  2000] loss: 0.406\n",
      "[48,  4000] loss: 0.451\n",
      "[48,  6000] loss: 0.463\n",
      "[48,  8000] loss: 0.470\n",
      "[48, 10000] loss: 0.514\n",
      "[48, 12000] loss: 0.542\n",
      "[49,  2000] loss: 0.410\n",
      "[49,  4000] loss: 0.457\n",
      "[49,  6000] loss: 0.460\n",
      "[49,  8000] loss: 0.485\n",
      "[49, 10000] loss: 0.535\n",
      "[49, 12000] loss: 0.520\n",
      "[50,  2000] loss: 0.452\n",
      "[50,  4000] loss: 0.452\n",
      "[50,  6000] loss: 0.475\n",
      "[50,  8000] loss: 0.480\n",
      "[50, 10000] loss: 0.500\n",
      "[50, 12000] loss: 0.524\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 65 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.306\n",
      "[1,  4000] loss: 2.307\n",
      "[1,  6000] loss: 2.305\n",
      "[1,  8000] loss: 2.306\n",
      "[1, 10000] loss: 2.306\n",
      "[1, 12000] loss: 2.305\n",
      "[2,  2000] loss: 2.306\n",
      "[2,  4000] loss: 2.305\n",
      "[2,  6000] loss: 2.305\n",
      "[2,  8000] loss: 2.305\n",
      "[2, 10000] loss: 2.305\n",
      "[2, 12000] loss: 2.306\n",
      "[3,  2000] loss: 2.306\n",
      "[3,  4000] loss: 2.305\n",
      "[3,  6000] loss: 2.305\n",
      "[3,  8000] loss: 2.305\n",
      "[3, 10000] loss: 2.306\n",
      "[3, 12000] loss: 2.304\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  4000] loss: 2.306\n",
      "[4,  6000] loss: 2.306\n",
      "[4,  8000] loss: 2.305\n",
      "[4, 10000] loss: 2.305\n",
      "[4, 12000] loss: 2.306\n",
      "[5,  2000] loss: 2.306\n",
      "[5,  4000] loss: 2.306\n",
      "[5,  6000] loss: 2.305\n",
      "[5,  8000] loss: 2.305\n",
      "[5, 10000] loss: 2.306\n",
      "[5, 12000] loss: 2.306\n",
      "[6,  2000] loss: 2.306\n",
      "[6,  4000] loss: 2.305\n",
      "[6,  6000] loss: 2.307\n",
      "[6,  8000] loss: 2.306\n",
      "[6, 10000] loss: 2.305\n",
      "[6, 12000] loss: 2.305\n",
      "[7,  2000] loss: 2.305\n",
      "[7,  4000] loss: 2.306\n",
      "[7,  6000] loss: 2.306\n",
      "[7,  8000] loss: 2.306\n",
      "[7, 10000] loss: 2.305\n",
      "[7, 12000] loss: 2.306\n",
      "[8,  2000] loss: 2.306\n",
      "[8,  4000] loss: 2.306\n",
      "[8,  6000] loss: 2.307\n",
      "[8,  8000] loss: 2.305\n",
      "[8, 10000] loss: 2.306\n",
      "[8, 12000] loss: 2.305\n",
      "[9,  2000] loss: 2.306\n",
      "[9,  4000] loss: 2.306\n",
      "[9,  6000] loss: 2.306\n",
      "[9,  8000] loss: 2.305\n",
      "[9, 10000] loss: 2.305\n",
      "[9, 12000] loss: 2.306\n",
      "[10,  2000] loss: 2.305\n",
      "[10,  4000] loss: 2.306\n",
      "[10,  6000] loss: 2.307\n",
      "[10,  8000] loss: 2.304\n",
      "[10, 10000] loss: 2.305\n",
      "[10, 12000] loss: 2.305\n",
      "[11,  2000] loss: 2.304\n",
      "[11,  4000] loss: 2.306\n",
      "[11,  6000] loss: 2.306\n",
      "[11,  8000] loss: 2.306\n",
      "[11, 10000] loss: 2.306\n",
      "[11, 12000] loss: 2.305\n",
      "[12,  2000] loss: 2.306\n",
      "[12,  4000] loss: 2.306\n",
      "[12,  6000] loss: 2.306\n",
      "[12,  8000] loss: 2.306\n",
      "[12, 10000] loss: 2.305\n",
      "[12, 12000] loss: 2.305\n",
      "[13,  2000] loss: 2.305\n",
      "[13,  4000] loss: 2.305\n",
      "[13,  6000] loss: 2.305\n",
      "[13,  8000] loss: 2.306\n",
      "[13, 10000] loss: 2.305\n",
      "[13, 12000] loss: 2.306\n",
      "[14,  2000] loss: 2.306\n",
      "[14,  4000] loss: 2.305\n",
      "[14,  6000] loss: 2.305\n",
      "[14,  8000] loss: 2.305\n",
      "[14, 10000] loss: 2.306\n",
      "[14, 12000] loss: 2.306\n",
      "[15,  2000] loss: 2.306\n",
      "[15,  4000] loss: 2.306\n",
      "[15,  6000] loss: 2.305\n",
      "[15,  8000] loss: 2.307\n",
      "[15, 10000] loss: 2.306\n",
      "[15, 12000] loss: 2.305\n",
      "[16,  2000] loss: 2.305\n",
      "[16,  4000] loss: 2.306\n",
      "[16,  6000] loss: 2.305\n",
      "[16,  8000] loss: 2.306\n",
      "[16, 10000] loss: 2.306\n",
      "[16, 12000] loss: 2.306\n",
      "[17,  2000] loss: 2.306\n",
      "[17,  4000] loss: 2.306\n",
      "[17,  6000] loss: 2.306\n",
      "[17,  8000] loss: 2.306\n",
      "[17, 10000] loss: 2.304\n",
      "[17, 12000] loss: 2.306\n",
      "[18,  2000] loss: 2.307\n",
      "[18,  4000] loss: 2.305\n",
      "[18,  6000] loss: 2.306\n",
      "[18,  8000] loss: 2.304\n",
      "[18, 10000] loss: 2.305\n",
      "[18, 12000] loss: 2.306\n",
      "[19,  2000] loss: 2.305\n",
      "[19,  4000] loss: 2.306\n",
      "[19,  6000] loss: 2.307\n",
      "[19,  8000] loss: 2.307\n",
      "[19, 10000] loss: 2.306\n",
      "[19, 12000] loss: 2.303\n",
      "[20,  2000] loss: 2.306\n",
      "[20,  4000] loss: 2.305\n",
      "[20,  6000] loss: 2.305\n",
      "[20,  8000] loss: 2.304\n",
      "[20, 10000] loss: 2.306\n",
      "[20, 12000] loss: 2.307\n",
      "[21,  2000] loss: 2.305\n",
      "[21,  4000] loss: 2.306\n",
      "[21,  6000] loss: 2.305\n",
      "[21,  8000] loss: 2.306\n",
      "[21, 10000] loss: 2.306\n",
      "[21, 12000] loss: 2.305\n",
      "[22,  2000] loss: 2.306\n",
      "[22,  4000] loss: 2.306\n",
      "[22,  6000] loss: 2.306\n",
      "[22,  8000] loss: 2.305\n",
      "[22, 10000] loss: 2.306\n",
      "[22, 12000] loss: 2.305\n",
      "[23,  2000] loss: 2.305\n",
      "[23,  4000] loss: 2.305\n",
      "[23,  6000] loss: 2.307\n",
      "[23,  8000] loss: 2.306\n",
      "[23, 10000] loss: 2.304\n",
      "[23, 12000] loss: 2.307\n",
      "[24,  2000] loss: 2.305\n",
      "[24,  4000] loss: 2.305\n",
      "[24,  6000] loss: 2.306\n",
      "[24,  8000] loss: 2.306\n",
      "[24, 10000] loss: 2.305\n",
      "[24, 12000] loss: 2.306\n",
      "[25,  2000] loss: 2.306\n",
      "[25,  4000] loss: 2.306\n",
      "[25,  6000] loss: 2.306\n",
      "[25,  8000] loss: 2.305\n",
      "[25, 10000] loss: 2.305\n",
      "[25, 12000] loss: 2.305\n",
      "[26,  2000] loss: 2.306\n",
      "[26,  4000] loss: 2.306\n",
      "[26,  6000] loss: 2.305\n",
      "[26,  8000] loss: 2.304\n",
      "[26, 10000] loss: 2.306\n",
      "[26, 12000] loss: 2.306\n",
      "[27,  2000] loss: 2.306\n",
      "[27,  4000] loss: 2.305\n",
      "[27,  6000] loss: 2.305\n",
      "[27,  8000] loss: 2.305\n",
      "[27, 10000] loss: 2.305\n",
      "[27, 12000] loss: 2.305\n",
      "[28,  2000] loss: 2.305\n",
      "[28,  4000] loss: 2.306\n",
      "[28,  6000] loss: 2.306\n",
      "[28,  8000] loss: 2.305\n",
      "[28, 10000] loss: 2.306\n",
      "[28, 12000] loss: 2.305\n",
      "[29,  2000] loss: 2.305\n",
      "[29,  4000] loss: 2.306\n",
      "[29,  6000] loss: 2.306\n",
      "[29,  8000] loss: 2.304\n",
      "[29, 10000] loss: 2.305\n",
      "[29, 12000] loss: 2.307\n",
      "[30,  2000] loss: 2.306\n",
      "[30,  4000] loss: 2.306\n",
      "[30,  6000] loss: 2.306\n",
      "[30,  8000] loss: 2.305\n",
      "[30, 10000] loss: 2.306\n",
      "[30, 12000] loss: 2.305\n",
      "[31,  2000] loss: 2.305\n",
      "[31,  4000] loss: 2.305\n",
      "[31,  6000] loss: 2.306\n",
      "[31,  8000] loss: 2.304\n",
      "[31, 10000] loss: 2.305\n",
      "[31, 12000] loss: 2.306\n",
      "[32,  2000] loss: 2.305\n",
      "[32,  4000] loss: 2.306\n",
      "[32,  6000] loss: 2.306\n",
      "[32,  8000] loss: 2.306\n",
      "[32, 10000] loss: 2.304\n",
      "[32, 12000] loss: 2.305\n",
      "[33,  2000] loss: 2.307\n",
      "[33,  4000] loss: 2.305\n",
      "[33,  6000] loss: 2.305\n",
      "[33,  8000] loss: 2.306\n",
      "[33, 10000] loss: 2.305\n",
      "[33, 12000] loss: 2.306\n",
      "[34,  2000] loss: 2.305\n",
      "[34,  4000] loss: 2.305\n",
      "[34,  6000] loss: 2.305\n",
      "[34,  8000] loss: 2.307\n",
      "[34, 10000] loss: 2.306\n",
      "[34, 12000] loss: 2.306\n",
      "[35,  2000] loss: 2.305\n",
      "[35,  4000] loss: 2.305\n",
      "[35,  6000] loss: 2.306\n",
      "[35,  8000] loss: 2.306\n",
      "[35, 10000] loss: 2.306\n",
      "[35, 12000] loss: 2.305\n",
      "[36,  2000] loss: 2.305\n",
      "[36,  4000] loss: 2.306\n",
      "[36,  6000] loss: 2.305\n",
      "[36,  8000] loss: 2.306\n",
      "[36, 10000] loss: 2.305\n",
      "[36, 12000] loss: 2.305\n",
      "[37,  2000] loss: 2.305\n",
      "[37,  4000] loss: 2.305\n",
      "[37,  6000] loss: 2.307\n",
      "[37,  8000] loss: 2.306\n",
      "[37, 10000] loss: 2.305\n",
      "[37, 12000] loss: 2.305\n",
      "[38,  2000] loss: 2.306\n",
      "[38,  4000] loss: 2.306\n",
      "[38,  6000] loss: 2.305\n",
      "[38,  8000] loss: 2.306\n",
      "[38, 10000] loss: 2.305\n",
      "[38, 12000] loss: 2.305\n",
      "[39,  2000] loss: 2.306\n",
      "[39,  4000] loss: 2.305\n",
      "[39,  6000] loss: 2.306\n",
      "[39,  8000] loss: 2.304\n",
      "[39, 10000] loss: 2.306\n",
      "[39, 12000] loss: 2.306\n",
      "[40,  2000] loss: 2.305\n",
      "[40,  4000] loss: 2.305\n",
      "[40,  6000] loss: 2.306\n",
      "[40,  8000] loss: 2.305\n",
      "[40, 10000] loss: 2.307\n",
      "[40, 12000] loss: 2.306\n",
      "[41,  2000] loss: 2.305\n",
      "[41,  4000] loss: 2.306\n",
      "[41,  6000] loss: 2.306\n",
      "[41,  8000] loss: 2.305\n",
      "[41, 10000] loss: 2.305\n",
      "[41, 12000] loss: 2.306\n",
      "[42,  2000] loss: 2.307\n",
      "[42,  4000] loss: 2.305\n",
      "[42,  6000] loss: 2.305\n",
      "[42,  8000] loss: 2.306\n",
      "[42, 10000] loss: 2.305\n",
      "[42, 12000] loss: 2.305\n",
      "[43,  2000] loss: 2.305\n",
      "[43,  4000] loss: 2.306\n",
      "[43,  6000] loss: 2.306\n",
      "[43,  8000] loss: 2.304\n",
      "[43, 10000] loss: 2.306\n",
      "[43, 12000] loss: 2.305\n",
      "[44,  2000] loss: 2.305\n",
      "[44,  4000] loss: 2.306\n",
      "[44,  6000] loss: 2.304\n",
      "[44,  8000] loss: 2.305\n",
      "[44, 10000] loss: 2.307\n",
      "[44, 12000] loss: 2.306\n",
      "[45,  2000] loss: 2.306\n",
      "[45,  4000] loss: 2.306\n",
      "[45,  6000] loss: 2.306\n",
      "[45,  8000] loss: 2.306\n",
      "[45, 10000] loss: 2.305\n",
      "[45, 12000] loss: 2.305\n",
      "[46,  2000] loss: 2.306\n",
      "[46,  4000] loss: 2.304\n",
      "[46,  6000] loss: 2.307\n",
      "[46,  8000] loss: 2.306\n",
      "[46, 10000] loss: 2.305\n",
      "[46, 12000] loss: 2.306\n",
      "[47,  2000] loss: 2.306\n",
      "[47,  4000] loss: 2.305\n",
      "[47,  6000] loss: 2.305\n",
      "[47,  8000] loss: 2.305\n",
      "[47, 10000] loss: 2.305\n",
      "[47, 12000] loss: 2.307\n",
      "[48,  2000] loss: 2.305\n",
      "[48,  4000] loss: 2.305\n",
      "[48,  6000] loss: 2.306\n",
      "[48,  8000] loss: 2.305\n",
      "[48, 10000] loss: 2.307\n",
      "[48, 12000] loss: 2.306\n",
      "[49,  2000] loss: 2.304\n",
      "[49,  4000] loss: 2.305\n",
      "[49,  6000] loss: 2.306\n",
      "[49,  8000] loss: 2.306\n",
      "[49, 10000] loss: 2.306\n",
      "[49, 12000] loss: 2.305\n",
      "[50,  2000] loss: 2.306\n",
      "[50,  4000] loss: 2.305\n",
      "[50,  6000] loss: 2.306\n",
      "[50,  8000] loss: 2.305\n",
      "[50, 10000] loss: 2.306\n",
      "[50, 12000] loss: 2.305\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.305\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.305\n",
      "[2,  4000] loss: 2.305\n",
      "[2,  6000] loss: 2.305\n",
      "[2,  8000] loss: 2.305\n",
      "[2, 10000] loss: 2.304\n",
      "[2, 12000] loss: 2.304\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  4000] loss: 2.305\n",
      "[3,  6000] loss: 2.304\n",
      "[3,  8000] loss: 2.304\n",
      "[3, 10000] loss: 2.305\n",
      "[3, 12000] loss: 2.305\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  4000] loss: 2.305\n",
      "[4,  6000] loss: 2.305\n",
      "[4,  8000] loss: 2.305\n",
      "[4, 10000] loss: 2.304\n",
      "[4, 12000] loss: 2.305\n",
      "[5,  2000] loss: 2.304\n",
      "[5,  4000] loss: 2.305\n",
      "[5,  6000] loss: 2.305\n",
      "[5,  8000] loss: 2.304\n",
      "[5, 10000] loss: 2.305\n",
      "[5, 12000] loss: 2.305\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 44, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(44, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.306\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.305\n",
      "[2,  2000] loss: 2.305\n",
      "[2,  4000] loss: 2.304\n",
      "[2,  6000] loss: 2.304\n",
      "[2,  8000] loss: 2.305\n",
      "[2, 10000] loss: 2.305\n",
      "[2, 12000] loss: 2.304\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  4000] loss: 2.304\n",
      "[3,  6000] loss: 2.303\n",
      "[3,  8000] loss: 2.305\n",
      "[3, 10000] loss: 2.306\n",
      "[3, 12000] loss: 2.304\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  4000] loss: 2.304\n",
      "[4,  6000] loss: 2.304\n",
      "[4,  8000] loss: 2.304\n",
      "[4, 10000] loss: 2.305\n",
      "[4, 12000] loss: 2.305\n",
      "[5,  2000] loss: 2.303\n",
      "[5,  4000] loss: 2.305\n",
      "[5,  6000] loss: 2.305\n",
      "[5,  8000] loss: 2.306\n",
      "[5, 10000] loss: 2.304\n",
      "[5, 12000] loss: 2.305\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 56, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(56, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.868\n",
      "[1,  4000] loss: 1.489\n",
      "[1,  6000] loss: 1.309\n",
      "[1,  8000] loss: 1.132\n",
      "[1, 10000] loss: 1.019\n",
      "[1, 12000] loss: 0.958\n",
      "[2,  2000] loss: 0.834\n",
      "[2,  4000] loss: 0.824\n",
      "[2,  6000] loss: 0.794\n",
      "[2,  8000] loss: 0.753\n",
      "[2, 10000] loss: 0.729\n",
      "[2, 12000] loss: 0.730\n",
      "[3,  2000] loss: 0.584\n",
      "[3,  4000] loss: 0.590\n",
      "[3,  6000] loss: 0.601\n",
      "[3,  8000] loss: 0.592\n",
      "[3, 10000] loss: 0.569\n",
      "[3, 12000] loss: 0.559\n",
      "[4,  2000] loss: 0.432\n",
      "[4,  4000] loss: 0.459\n",
      "[4,  6000] loss: 0.469\n",
      "[4,  8000] loss: 0.455\n",
      "[4, 10000] loss: 0.440\n",
      "[4, 12000] loss: 0.459\n",
      "[5,  2000] loss: 0.318\n",
      "[5,  4000] loss: 0.333\n",
      "[5,  6000] loss: 0.344\n",
      "[5,  8000] loss: 0.365\n",
      "[5, 10000] loss: 0.353\n",
      "[5, 12000] loss: 0.366\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=56, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(56),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=56, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                       # this is what you had\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ba9ca613c00912bf2bb7336c6f7b766b0be232b7fbb6881178983a86316f18c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
