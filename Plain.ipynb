{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.177\n",
      "[1,  4000] loss: 1.801\n",
      "[1,  6000] loss: 1.608\n",
      "[1,  8000] loss: 1.516\n",
      "[1, 10000] loss: 1.443\n",
      "[1, 12000] loss: 1.420\n",
      "[2,  2000] loss: 1.335\n",
      "[2,  4000] loss: 1.306\n",
      "[2,  6000] loss: 1.266\n",
      "[2,  8000] loss: 1.233\n",
      "[2, 10000] loss: 1.222\n",
      "[2, 12000] loss: 1.190\n",
      "[3,  2000] loss: 1.104\n",
      "[3,  4000] loss: 1.109\n",
      "[3,  6000] loss: 1.084\n",
      "[3,  8000] loss: 1.072\n",
      "[3, 10000] loss: 1.074\n",
      "[3, 12000] loss: 1.066\n",
      "[4,  2000] loss: 0.979\n",
      "[4,  4000] loss: 1.005\n",
      "[4,  6000] loss: 0.986\n",
      "[4,  8000] loss: 0.953\n",
      "[4, 10000] loss: 0.971\n",
      "[4, 12000] loss: 0.995\n",
      "[5,  2000] loss: 0.884\n",
      "[5,  4000] loss: 0.898\n",
      "[5,  6000] loss: 0.904\n",
      "[5,  8000] loss: 0.914\n",
      "[5, 10000] loss: 0.911\n",
      "[5, 12000] loss: 0.907\n",
      "[6,  2000] loss: 0.829\n",
      "[6,  4000] loss: 0.846\n",
      "[6,  6000] loss: 0.860\n",
      "[6,  8000] loss: 0.867\n",
      "[6, 10000] loss: 0.863\n",
      "[6, 12000] loss: 0.863\n",
      "[7,  2000] loss: 0.779\n",
      "[7,  4000] loss: 0.778\n",
      "[7,  6000] loss: 0.809\n",
      "[7,  8000] loss: 0.821\n",
      "[7, 10000] loss: 0.830\n",
      "[7, 12000] loss: 0.807\n",
      "[8,  2000] loss: 0.725\n",
      "[8,  4000] loss: 0.748\n",
      "[8,  6000] loss: 0.753\n",
      "[8,  8000] loss: 0.785\n",
      "[8, 10000] loss: 0.790\n",
      "[8, 12000] loss: 0.786\n",
      "[9,  2000] loss: 0.684\n",
      "[9,  4000] loss: 0.715\n",
      "[9,  6000] loss: 0.734\n",
      "[9,  8000] loss: 0.735\n",
      "[9, 10000] loss: 0.774\n",
      "[9, 12000] loss: 0.752\n",
      "[10,  2000] loss: 0.653\n",
      "[10,  4000] loss: 0.694\n",
      "[10,  6000] loss: 0.696\n",
      "[10,  8000] loss: 0.705\n",
      "[10, 10000] loss: 0.746\n",
      "[10, 12000] loss: 0.730\n",
      "[11,  2000] loss: 0.632\n",
      "[11,  4000] loss: 0.665\n",
      "[11,  6000] loss: 0.671\n",
      "[11,  8000] loss: 0.676\n",
      "[11, 10000] loss: 0.714\n",
      "[11, 12000] loss: 0.708\n",
      "[12,  2000] loss: 0.592\n",
      "[12,  4000] loss: 0.647\n",
      "[12,  6000] loss: 0.655\n",
      "[12,  8000] loss: 0.656\n",
      "[12, 10000] loss: 0.682\n",
      "[12, 12000] loss: 0.699\n",
      "[13,  2000] loss: 0.571\n",
      "[13,  4000] loss: 0.618\n",
      "[13,  6000] loss: 0.639\n",
      "[13,  8000] loss: 0.653\n",
      "[13, 10000] loss: 0.663\n",
      "[13, 12000] loss: 0.662\n",
      "[14,  2000] loss: 0.549\n",
      "[14,  4000] loss: 0.609\n",
      "[14,  6000] loss: 0.597\n",
      "[14,  8000] loss: 0.638\n",
      "[14, 10000] loss: 0.641\n",
      "[14, 12000] loss: 0.663\n",
      "[15,  2000] loss: 0.541\n",
      "[15,  4000] loss: 0.593\n",
      "[15,  6000] loss: 0.579\n",
      "[15,  8000] loss: 0.605\n",
      "[15, 10000] loss: 0.627\n",
      "[15, 12000] loss: 0.642\n",
      "[16,  2000] loss: 0.524\n",
      "[16,  4000] loss: 0.575\n",
      "[16,  6000] loss: 0.566\n",
      "[16,  8000] loss: 0.576\n",
      "[16, 10000] loss: 0.619\n",
      "[16, 12000] loss: 0.633\n",
      "[17,  2000] loss: 0.514\n",
      "[17,  4000] loss: 0.571\n",
      "[17,  6000] loss: 0.577\n",
      "[17,  8000] loss: 0.589\n",
      "[17, 10000] loss: 0.620\n",
      "[17, 12000] loss: 0.615\n",
      "[18,  2000] loss: 0.515\n",
      "[18,  4000] loss: 0.535\n",
      "[18,  6000] loss: 0.569\n",
      "[18,  8000] loss: 0.597\n",
      "[18, 10000] loss: 0.583\n",
      "[18, 12000] loss: 0.606\n",
      "[19,  2000] loss: 0.492\n",
      "[19,  4000] loss: 0.537\n",
      "[19,  6000] loss: 0.540\n",
      "[19,  8000] loss: 0.572\n",
      "[19, 10000] loss: 0.583\n",
      "[19, 12000] loss: 0.599\n",
      "[20,  2000] loss: 0.476\n",
      "[20,  4000] loss: 0.528\n",
      "[20,  6000] loss: 0.548\n",
      "[20,  8000] loss: 0.549\n",
      "[20, 10000] loss: 0.588\n",
      "[20, 12000] loss: 0.591\n",
      "[21,  2000] loss: 0.483\n",
      "[21,  4000] loss: 0.521\n",
      "[21,  6000] loss: 0.523\n",
      "[21,  8000] loss: 0.550\n",
      "[21, 10000] loss: 0.558\n",
      "[21, 12000] loss: 0.587\n",
      "[22,  2000] loss: 0.449\n",
      "[22,  4000] loss: 0.495\n",
      "[22,  6000] loss: 0.541\n",
      "[22,  8000] loss: 0.542\n",
      "[22, 10000] loss: 0.551\n",
      "[22, 12000] loss: 0.565\n",
      "[23,  2000] loss: 0.467\n",
      "[23,  4000] loss: 0.502\n",
      "[23,  6000] loss: 0.537\n",
      "[23,  8000] loss: 0.525\n",
      "[23, 10000] loss: 0.530\n",
      "[23, 12000] loss: 0.592\n",
      "[24,  2000] loss: 0.464\n",
      "[24,  4000] loss: 0.489\n",
      "[24,  6000] loss: 0.504\n",
      "[24,  8000] loss: 0.535\n",
      "[24, 10000] loss: 0.549\n",
      "[24, 12000] loss: 0.561\n",
      "[25,  2000] loss: 0.454\n",
      "[25,  4000] loss: 0.473\n",
      "[25,  6000] loss: 0.502\n",
      "[25,  8000] loss: 0.546\n",
      "[25, 10000] loss: 0.552\n",
      "[25, 12000] loss: 0.546\n",
      "[26,  2000] loss: 0.431\n",
      "[26,  4000] loss: 0.485\n",
      "[26,  6000] loss: 0.484\n",
      "[26,  8000] loss: 0.509\n",
      "[26, 10000] loss: 0.552\n",
      "[26, 12000] loss: 0.563\n",
      "[27,  2000] loss: 0.467\n",
      "[27,  4000] loss: 0.464\n",
      "[27,  6000] loss: 0.487\n",
      "[27,  8000] loss: 0.531\n",
      "[27, 10000] loss: 0.539\n",
      "[27, 12000] loss: 0.527\n",
      "[28,  2000] loss: 0.421\n",
      "[28,  4000] loss: 0.471\n",
      "[28,  6000] loss: 0.506\n",
      "[28,  8000] loss: 0.501\n",
      "[28, 10000] loss: 0.522\n",
      "[28, 12000] loss: 0.510\n",
      "[29,  2000] loss: 0.408\n",
      "[29,  4000] loss: 0.490\n",
      "[29,  6000] loss: 0.506\n",
      "[29,  8000] loss: 0.492\n",
      "[29, 10000] loss: 0.553\n",
      "[29, 12000] loss: 0.530\n",
      "[30,  2000] loss: 0.438\n",
      "[30,  4000] loss: 0.466\n",
      "[30,  6000] loss: 0.501\n",
      "[30,  8000] loss: 0.500\n",
      "[30, 10000] loss: 0.521\n",
      "[30, 12000] loss: 0.548\n",
      "[31,  2000] loss: 0.440\n",
      "[31,  4000] loss: 0.448\n",
      "[31,  6000] loss: 0.499\n",
      "[31,  8000] loss: 0.493\n",
      "[31, 10000] loss: 0.549\n",
      "[31, 12000] loss: 0.555\n",
      "[32,  2000] loss: 0.407\n",
      "[32,  4000] loss: 0.471\n",
      "[32,  6000] loss: 0.473\n",
      "[32,  8000] loss: 0.511\n",
      "[32, 10000] loss: 0.526\n",
      "[32, 12000] loss: 0.530\n",
      "[33,  2000] loss: 0.435\n",
      "[33,  4000] loss: 0.442\n",
      "[33,  6000] loss: 0.494\n",
      "[33,  8000] loss: 0.498\n",
      "[33, 10000] loss: 0.544\n",
      "[33, 12000] loss: 0.527\n",
      "[34,  2000] loss: 0.428\n",
      "[34,  4000] loss: 0.449\n",
      "[34,  6000] loss: 0.473\n",
      "[34,  8000] loss: 0.489\n",
      "[34, 10000] loss: 0.519\n",
      "[34, 12000] loss: 0.544\n",
      "[35,  2000] loss: 0.418\n",
      "[35,  4000] loss: 0.452\n",
      "[35,  6000] loss: 0.462\n",
      "[35,  8000] loss: 0.490\n",
      "[35, 10000] loss: 0.517\n",
      "[35, 12000] loss: 0.510\n",
      "[36,  2000] loss: 0.425\n",
      "[36,  4000] loss: 0.445\n",
      "[36,  6000] loss: 0.487\n",
      "[36,  8000] loss: 0.518\n",
      "[36, 10000] loss: 0.516\n",
      "[36, 12000] loss: 0.535\n",
      "[37,  2000] loss: 0.396\n",
      "[37,  4000] loss: 0.446\n",
      "[37,  6000] loss: 0.481\n",
      "[37,  8000] loss: 0.511\n",
      "[37, 10000] loss: 0.502\n",
      "[37, 12000] loss: 0.539\n",
      "[38,  2000] loss: 0.402\n",
      "[38,  4000] loss: 0.457\n",
      "[38,  6000] loss: 0.486\n",
      "[38,  8000] loss: 0.497\n",
      "[38, 10000] loss: 0.498\n",
      "[38, 12000] loss: 0.514\n",
      "[39,  2000] loss: 0.422\n",
      "[39,  4000] loss: 0.447\n",
      "[39,  6000] loss: 0.494\n",
      "[39,  8000] loss: 0.486\n",
      "[39, 10000] loss: 0.518\n",
      "[39, 12000] loss: 0.534\n",
      "[40,  2000] loss: 0.408\n",
      "[40,  4000] loss: 0.470\n",
      "[40,  6000] loss: 0.489\n",
      "[40,  8000] loss: 0.499\n",
      "[40, 10000] loss: 0.508\n",
      "[40, 12000] loss: 0.564\n",
      "[41,  2000] loss: 0.413\n",
      "[41,  4000] loss: 0.467\n",
      "[41,  6000] loss: 0.491\n",
      "[41,  8000] loss: 0.485\n",
      "[41, 10000] loss: 0.518\n",
      "[41, 12000] loss: 0.537\n",
      "[42,  2000] loss: 0.420\n",
      "[42,  4000] loss: 0.415\n",
      "[42,  6000] loss: 0.491\n",
      "[42,  8000] loss: 0.513\n",
      "[42, 10000] loss: 0.490\n",
      "[42, 12000] loss: 0.499\n",
      "[43,  2000] loss: 0.394\n",
      "[43,  4000] loss: 0.459\n",
      "[43,  6000] loss: 0.486\n",
      "[43,  8000] loss: 0.492\n",
      "[43, 10000] loss: 0.499\n",
      "[43, 12000] loss: 0.514\n",
      "[44,  2000] loss: 0.392\n",
      "[44,  4000] loss: 0.436\n",
      "[44,  6000] loss: 0.468\n",
      "[44,  8000] loss: 0.499\n",
      "[44, 10000] loss: 0.519\n",
      "[44, 12000] loss: 0.525\n",
      "[45,  2000] loss: 0.416\n",
      "[45,  4000] loss: 0.461\n",
      "[45,  6000] loss: 0.508\n",
      "[45,  8000] loss: 0.471\n",
      "[45, 10000] loss: 0.478\n",
      "[45, 12000] loss: 0.524\n",
      "[46,  2000] loss: 0.417\n",
      "[46,  4000] loss: 0.444\n",
      "[46,  6000] loss: 0.487\n",
      "[46,  8000] loss: 0.505\n",
      "[46, 10000] loss: 0.528\n",
      "[46, 12000] loss: 0.518\n",
      "[47,  2000] loss: 0.445\n",
      "[47,  4000] loss: 0.451\n",
      "[47,  6000] loss: 0.494\n",
      "[47,  8000] loss: 0.485\n",
      "[47, 10000] loss: 0.527\n",
      "[47, 12000] loss: 0.531\n",
      "[48,  2000] loss: 0.424\n",
      "[48,  4000] loss: 0.469\n",
      "[48,  6000] loss: 0.476\n",
      "[48,  8000] loss: 0.496\n",
      "[48, 10000] loss: 0.520\n",
      "[48, 12000] loss: 0.521\n",
      "[49,  2000] loss: 0.424\n",
      "[49,  4000] loss: 0.469\n",
      "[49,  6000] loss: 0.469\n",
      "[49,  8000] loss: 0.486\n",
      "[49, 10000] loss: 0.519\n",
      "[49, 12000] loss: 0.524\n",
      "[50,  2000] loss: 0.432\n",
      "[50,  4000] loss: 0.480\n",
      "[50,  6000] loss: 0.476\n",
      "[50,  8000] loss: 0.533\n",
      "[50, 10000] loss: 0.506\n",
      "[50, 12000] loss: 0.522\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 63 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        images = images.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1175f2d68aea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-1175f2d68aea>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# flatten all dimensions except batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        images = images.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.305\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.305\n",
      "[2,  4000] loss: 2.305\n",
      "[2,  6000] loss: 2.305\n",
      "[2,  8000] loss: 2.305\n",
      "[2, 10000] loss: 2.304\n",
      "[2, 12000] loss: 2.304\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  4000] loss: 2.305\n",
      "[3,  6000] loss: 2.304\n",
      "[3,  8000] loss: 2.304\n",
      "[3, 10000] loss: 2.305\n",
      "[3, 12000] loss: 2.305\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  4000] loss: 2.305\n",
      "[4,  6000] loss: 2.305\n",
      "[4,  8000] loss: 2.305\n",
      "[4, 10000] loss: 2.304\n",
      "[4, 12000] loss: 2.305\n",
      "[5,  2000] loss: 2.304\n",
      "[5,  4000] loss: 2.305\n",
      "[5,  6000] loss: 2.305\n",
      "[5,  8000] loss: 2.304\n",
      "[5, 10000] loss: 2.305\n",
      "[5, 12000] loss: 2.305\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 44, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(44, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.306\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.305\n",
      "[2,  2000] loss: 2.305\n",
      "[2,  4000] loss: 2.304\n",
      "[2,  6000] loss: 2.304\n",
      "[2,  8000] loss: 2.305\n",
      "[2, 10000] loss: 2.305\n",
      "[2, 12000] loss: 2.304\n",
      "[3,  2000] loss: 2.305\n",
      "[3,  4000] loss: 2.304\n",
      "[3,  6000] loss: 2.303\n",
      "[3,  8000] loss: 2.305\n",
      "[3, 10000] loss: 2.306\n",
      "[3, 12000] loss: 2.304\n",
      "[4,  2000] loss: 2.305\n",
      "[4,  4000] loss: 2.304\n",
      "[4,  6000] loss: 2.304\n",
      "[4,  8000] loss: 2.304\n",
      "[4, 10000] loss: 2.305\n",
      "[4, 12000] loss: 2.305\n",
      "[5,  2000] loss: 2.303\n",
      "[5,  4000] loss: 2.305\n",
      "[5,  6000] loss: 2.305\n",
      "[5,  8000] loss: 2.306\n",
      "[5, 10000] loss: 2.304\n",
      "[5, 12000] loss: 2.305\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 56, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(56, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.868\n",
      "[1,  4000] loss: 1.489\n",
      "[1,  6000] loss: 1.309\n",
      "[1,  8000] loss: 1.132\n",
      "[1, 10000] loss: 1.019\n",
      "[1, 12000] loss: 0.958\n",
      "[2,  2000] loss: 0.834\n",
      "[2,  4000] loss: 0.824\n",
      "[2,  6000] loss: 0.794\n",
      "[2,  8000] loss: 0.753\n",
      "[2, 10000] loss: 0.729\n",
      "[2, 12000] loss: 0.730\n",
      "[3,  2000] loss: 0.584\n",
      "[3,  4000] loss: 0.590\n",
      "[3,  6000] loss: 0.601\n",
      "[3,  8000] loss: 0.592\n",
      "[3, 10000] loss: 0.569\n",
      "[3, 12000] loss: 0.559\n",
      "[4,  2000] loss: 0.432\n",
      "[4,  4000] loss: 0.459\n",
      "[4,  6000] loss: 0.469\n",
      "[4,  8000] loss: 0.455\n",
      "[4, 10000] loss: 0.440\n",
      "[4, 12000] loss: 0.459\n",
      "[5,  2000] loss: 0.318\n",
      "[5,  4000] loss: 0.333\n",
      "[5,  6000] loss: 0.344\n",
      "[5,  8000] loss: 0.365\n",
      "[5, 10000] loss: 0.353\n",
      "[5, 12000] loss: 0.366\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=56, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(56),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=56, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                       # this is what you had\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.875\n",
      "[1,  4000] loss: 1.486\n",
      "[1,  6000] loss: 1.274\n",
      "[1,  8000] loss: 1.108\n",
      "[1, 10000] loss: 1.042\n",
      "[1, 12000] loss: 0.970\n",
      "[2,  2000] loss: 0.831\n",
      "[2,  4000] loss: 0.833\n",
      "[2,  6000] loss: 0.796\n",
      "[2,  8000] loss: 0.774\n",
      "[2, 10000] loss: 0.745\n",
      "[2, 12000] loss: 0.734\n",
      "[3,  2000] loss: 0.600\n",
      "[3,  4000] loss: 0.590\n",
      "[3,  6000] loss: 0.606\n",
      "[3,  8000] loss: 0.578\n",
      "[3, 10000] loss: 0.577\n",
      "[3, 12000] loss: 0.595\n",
      "[4,  2000] loss: 0.452\n",
      "[4,  4000] loss: 0.456\n",
      "[4,  6000] loss: 0.466\n",
      "[4,  8000] loss: 0.452\n",
      "[4, 10000] loss: 0.467\n",
      "[4, 12000] loss: 0.449\n",
      "[5,  2000] loss: 0.322\n",
      "[5,  4000] loss: 0.331\n",
      "[5,  6000] loss: 0.367\n",
      "[5,  8000] loss: 0.371\n",
      "[5, 10000] loss: 0.386\n",
      "[5, 12000] loss: 0.372\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=20, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                       # this is what you had\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.861\n",
      "[1,  4000] loss: 1.470\n",
      "[1,  6000] loss: 1.255\n",
      "[1,  8000] loss: 1.135\n",
      "[1, 10000] loss: 1.043\n",
      "[1, 12000] loss: 0.969\n",
      "[2,  2000] loss: 0.857\n",
      "[2,  4000] loss: 0.827\n",
      "[2,  6000] loss: 0.809\n",
      "[2,  8000] loss: 0.738\n",
      "[2, 10000] loss: 0.738\n",
      "[2, 12000] loss: 0.703\n",
      "[3,  2000] loss: 0.577\n",
      "[3,  4000] loss: 0.620\n",
      "[3,  6000] loss: 0.611\n",
      "[3,  8000] loss: 0.583\n",
      "[3, 10000] loss: 0.567\n",
      "[3, 12000] loss: 0.561\n",
      "[4,  2000] loss: 0.448\n",
      "[4,  4000] loss: 0.443\n",
      "[4,  6000] loss: 0.459\n",
      "[4,  8000] loss: 0.461\n",
      "[4, 10000] loss: 0.458\n",
      "[4, 12000] loss: 0.468\n",
      "[5,  2000] loss: 0.316\n",
      "[5,  4000] loss: 0.326\n",
      "[5,  6000] loss: 0.344\n",
      "[5,  8000] loss: 0.347\n",
      "[5, 10000] loss: 0.375\n",
      "[5, 12000] loss: 0.368\n",
      "[6,  2000] loss: 0.232\n",
      "[6,  4000] loss: 0.255\n",
      "[6,  6000] loss: 0.276\n",
      "[6,  8000] loss: 0.264\n",
      "[6, 10000] loss: 0.270\n",
      "[6, 12000] loss: 0.269\n",
      "[7,  2000] loss: 0.180\n",
      "[7,  4000] loss: 0.166\n",
      "[7,  6000] loss: 0.191\n",
      "[7,  8000] loss: 0.189\n",
      "[7, 10000] loss: 0.213\n",
      "[7, 12000] loss: 0.223\n",
      "[8,  2000] loss: 0.111\n",
      "[8,  4000] loss: 0.126\n",
      "[8,  6000] loss: 0.134\n",
      "[8,  8000] loss: 0.148\n",
      "[8, 10000] loss: 0.152\n",
      "[8, 12000] loss: 0.173\n",
      "[9,  2000] loss: 0.101\n",
      "[9,  4000] loss: 0.105\n",
      "[9,  6000] loss: 0.109\n",
      "[9,  8000] loss: 0.119\n",
      "[9, 10000] loss: 0.129\n",
      "[9, 12000] loss: 0.128\n",
      "[10,  2000] loss: 0.078\n",
      "[10,  4000] loss: 0.086\n",
      "[10,  6000] loss: 0.101\n",
      "[10,  8000] loss: 0.091\n",
      "[10, 10000] loss: 0.087\n",
      "[10, 12000] loss: 0.099\n",
      "[11,  2000] loss: 0.060\n",
      "[11,  4000] loss: 0.060\n",
      "[11,  6000] loss: 0.080\n",
      "[11,  8000] loss: 0.080\n",
      "[11, 10000] loss: 0.080\n",
      "[11, 12000] loss: 0.105\n",
      "[12,  2000] loss: 0.051\n",
      "[12,  4000] loss: 0.063\n",
      "[12,  6000] loss: 0.067\n",
      "[12,  8000] loss: 0.066\n",
      "[12, 10000] loss: 0.059\n",
      "[12, 12000] loss: 0.065\n",
      "[13,  2000] loss: 0.043\n",
      "[13,  4000] loss: 0.050\n",
      "[13,  6000] loss: 0.054\n",
      "[13,  8000] loss: 0.046\n",
      "[13, 10000] loss: 0.048\n",
      "[13, 12000] loss: 0.055\n",
      "[14,  2000] loss: 0.048\n",
      "[14,  4000] loss: 0.042\n",
      "[14,  6000] loss: 0.043\n",
      "[14,  8000] loss: 0.051\n",
      "[14, 10000] loss: 0.057\n",
      "[14, 12000] loss: 0.051\n",
      "[15,  2000] loss: 0.029\n",
      "[15,  4000] loss: 0.037\n",
      "[15,  6000] loss: 0.037\n",
      "[15,  8000] loss: 0.042\n",
      "[15, 10000] loss: 0.045\n",
      "[15, 12000] loss: 0.046\n",
      "[16,  2000] loss: 0.027\n",
      "[16,  4000] loss: 0.032\n",
      "[16,  6000] loss: 0.038\n",
      "[16,  8000] loss: 0.035\n",
      "[16, 10000] loss: 0.051\n",
      "[16, 12000] loss: 0.043\n",
      "[17,  2000] loss: 0.025\n",
      "[17,  4000] loss: 0.031\n",
      "[17,  6000] loss: 0.028\n",
      "[17,  8000] loss: 0.030\n",
      "[17, 10000] loss: 0.034\n",
      "[17, 12000] loss: 0.044\n",
      "[18,  2000] loss: 0.022\n",
      "[18,  4000] loss: 0.026\n",
      "[18,  6000] loss: 0.032\n",
      "[18,  8000] loss: 0.033\n",
      "[18, 10000] loss: 0.034\n",
      "[18, 12000] loss: 0.030\n",
      "[19,  2000] loss: 0.029\n",
      "[19,  4000] loss: 0.029\n",
      "[19,  6000] loss: 0.026\n",
      "[19,  8000] loss: 0.031\n",
      "[19, 10000] loss: 0.027\n",
      "[19, 12000] loss: 0.027\n",
      "[20,  2000] loss: 0.029\n",
      "[20,  4000] loss: 0.029\n",
      "[20,  6000] loss: 0.038\n",
      "[20,  8000] loss: 0.032\n",
      "[20, 10000] loss: 0.029\n",
      "[20, 12000] loss: 0.031\n",
      "[21,  2000] loss: 0.020\n",
      "[21,  4000] loss: 0.020\n",
      "[21,  6000] loss: 0.023\n",
      "[21,  8000] loss: 0.027\n",
      "[21, 10000] loss: 0.025\n",
      "[21, 12000] loss: 0.023\n",
      "[22,  2000] loss: 0.017\n",
      "[22,  4000] loss: 0.012\n",
      "[22,  6000] loss: 0.024\n",
      "[22,  8000] loss: 0.020\n",
      "[22, 10000] loss: 0.029\n",
      "[22, 12000] loss: 0.025\n",
      "[23,  2000] loss: 0.016\n",
      "[23,  4000] loss: 0.014\n",
      "[23,  6000] loss: 0.017\n",
      "[23,  8000] loss: 0.016\n",
      "[23, 10000] loss: 0.014\n",
      "[23, 12000] loss: 0.016\n",
      "[24,  2000] loss: 0.016\n",
      "[24,  4000] loss: 0.014\n",
      "[24,  6000] loss: 0.011\n",
      "[24,  8000] loss: 0.020\n",
      "[24, 10000] loss: 0.023\n",
      "[24, 12000] loss: 0.026\n",
      "[25,  2000] loss: 0.016\n",
      "[25,  4000] loss: 0.014\n",
      "[25,  6000] loss: 0.013\n",
      "[25,  8000] loss: 0.016\n",
      "[25, 10000] loss: 0.012\n",
      "[25, 12000] loss: 0.020\n",
      "[26,  2000] loss: 0.013\n",
      "[26,  4000] loss: 0.019\n",
      "[26,  6000] loss: 0.011\n",
      "[26,  8000] loss: 0.015\n",
      "[26, 10000] loss: 0.013\n",
      "[26, 12000] loss: 0.009\n",
      "[27,  2000] loss: 0.013\n",
      "[27,  4000] loss: 0.011\n",
      "[27,  6000] loss: 0.012\n",
      "[27,  8000] loss: 0.012\n",
      "[27, 10000] loss: 0.012\n",
      "[27, 12000] loss: 0.017\n",
      "[28,  2000] loss: 0.012\n",
      "[28,  4000] loss: 0.014\n",
      "[28,  6000] loss: 0.016\n",
      "[28,  8000] loss: 0.010\n",
      "[28, 10000] loss: 0.019\n",
      "[28, 12000] loss: 0.020\n",
      "[29,  2000] loss: 0.011\n",
      "[29,  4000] loss: 0.010\n",
      "[29,  6000] loss: 0.013\n",
      "[29,  8000] loss: 0.011\n",
      "[29, 10000] loss: 0.008\n",
      "[29, 12000] loss: 0.009\n",
      "[30,  2000] loss: 0.010\n",
      "[30,  4000] loss: 0.007\n",
      "[30,  6000] loss: 0.010\n",
      "[30,  8000] loss: 0.007\n",
      "[30, 10000] loss: 0.007\n",
      "[30, 12000] loss: 0.005\n",
      "[31,  2000] loss: 0.006\n",
      "[31,  4000] loss: 0.007\n",
      "[31,  6000] loss: 0.004\n",
      "[31,  8000] loss: 0.012\n",
      "[31, 10000] loss: 0.006\n",
      "[31, 12000] loss: 0.007\n",
      "[32,  2000] loss: 0.013\n",
      "[32,  4000] loss: 0.014\n",
      "[32,  6000] loss: 0.005\n",
      "[32,  8000] loss: 0.009\n",
      "[32, 10000] loss: 0.013\n",
      "[32, 12000] loss: 0.016\n",
      "[33,  2000] loss: 0.014\n",
      "[33,  4000] loss: 0.014\n",
      "[33,  6000] loss: 0.009\n",
      "[33,  8000] loss: 0.009\n",
      "[33, 10000] loss: 0.009\n",
      "[33, 12000] loss: 0.009\n",
      "[34,  2000] loss: 0.008\n",
      "[34,  4000] loss: 0.009\n",
      "[34,  6000] loss: 0.020\n",
      "[34,  8000] loss: 0.016\n",
      "[34, 10000] loss: 0.011\n",
      "[34, 12000] loss: 0.017\n",
      "[35,  2000] loss: 0.013\n",
      "[35,  4000] loss: 0.010\n",
      "[35,  6000] loss: 0.009\n",
      "[35,  8000] loss: 0.018\n",
      "[35, 10000] loss: 0.009\n",
      "[35, 12000] loss: 0.010\n",
      "[36,  2000] loss: 0.006\n",
      "[36,  4000] loss: 0.007\n",
      "[36,  6000] loss: 0.010\n",
      "[36,  8000] loss: 0.006\n",
      "[36, 10000] loss: 0.006\n",
      "[36, 12000] loss: 0.011\n",
      "[37,  2000] loss: 0.008\n",
      "[37,  4000] loss: 0.009\n",
      "[37,  6000] loss: 0.004\n",
      "[37,  8000] loss: 0.004\n",
      "[37, 10000] loss: 0.009\n",
      "[37, 12000] loss: 0.007\n",
      "[38,  2000] loss: 0.010\n",
      "[38,  4000] loss: 0.008\n",
      "[38,  6000] loss: 0.007\n",
      "[38,  8000] loss: 0.007\n",
      "[38, 10000] loss: 0.010\n",
      "[38, 12000] loss: 0.006\n",
      "[39,  2000] loss: 0.009\n",
      "[39,  4000] loss: 0.012\n",
      "[39,  6000] loss: 0.008\n",
      "[39,  8000] loss: 0.010\n",
      "[39, 10000] loss: 0.016\n",
      "[39, 12000] loss: 0.011\n",
      "[40,  2000] loss: 0.005\n",
      "[40,  4000] loss: 0.011\n",
      "[40,  6000] loss: 0.017\n",
      "[40,  8000] loss: 0.013\n",
      "[40, 10000] loss: 0.017\n",
      "[40, 12000] loss: 0.010\n",
      "[41,  2000] loss: 0.014\n",
      "[41,  4000] loss: 0.010\n",
      "[41,  6000] loss: 0.009\n",
      "[41,  8000] loss: 0.007\n",
      "[41, 10000] loss: 0.014\n",
      "[41, 12000] loss: 0.007\n",
      "[42,  2000] loss: 0.008\n",
      "[42,  4000] loss: 0.005\n",
      "[42,  6000] loss: 0.007\n",
      "[42,  8000] loss: 0.007\n",
      "[42, 10000] loss: 0.015\n",
      "[42, 12000] loss: 0.010\n",
      "[43,  2000] loss: 0.010\n",
      "[43,  4000] loss: 0.010\n",
      "[43,  6000] loss: 0.011\n",
      "[43,  8000] loss: 0.007\n",
      "[43, 10000] loss: 0.008\n",
      "[43, 12000] loss: 0.005\n",
      "[44,  2000] loss: 0.003\n",
      "[44,  4000] loss: 0.007\n",
      "[44,  6000] loss: 0.006\n",
      "[44,  8000] loss: 0.005\n",
      "[44, 10000] loss: 0.003\n",
      "[44, 12000] loss: 0.004\n",
      "[45,  2000] loss: 0.004\n",
      "[45,  4000] loss: 0.003\n",
      "[45,  6000] loss: 0.006\n",
      "[45,  8000] loss: 0.004\n",
      "[45, 10000] loss: 0.004\n",
      "[45, 12000] loss: 0.004\n",
      "[46,  2000] loss: 0.001\n",
      "[46,  4000] loss: 0.003\n",
      "[46,  6000] loss: 0.007\n",
      "[46,  8000] loss: 0.007\n",
      "[46, 10000] loss: 0.010\n",
      "[46, 12000] loss: 0.008\n",
      "[47,  2000] loss: 0.006\n",
      "[47,  4000] loss: 0.007\n",
      "[47,  6000] loss: 0.009\n",
      "[47,  8000] loss: 0.006\n",
      "[47, 10000] loss: 0.004\n",
      "[47, 12000] loss: 0.006\n",
      "[48,  2000] loss: 0.006\n",
      "[48,  4000] loss: 0.007\n",
      "[48,  6000] loss: 0.007\n",
      "[48,  8000] loss: 0.006\n",
      "[48, 10000] loss: 0.008\n",
      "[48, 12000] loss: 0.005\n",
      "[49,  2000] loss: 0.008\n",
      "[49,  4000] loss: 0.007\n",
      "[49,  6000] loss: 0.006\n",
      "[49,  8000] loss: 0.002\n",
      "[49, 10000] loss: 0.003\n",
      "[49, 12000] loss: 0.008\n",
      "[50,  2000] loss: 0.004\n",
      "[50,  4000] loss: 0.004\n",
      "[50,  6000] loss: 0.004\n",
      "[50,  8000] loss: 0.005\n",
      "[50, 10000] loss: 0.004\n",
      "[50, 12000] loss: 0.004\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                       # this is what you had\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.866\n",
      "[1,  4000] loss: 1.501\n",
      "[1,  6000] loss: 1.318\n",
      "[1,  8000] loss: 1.161\n",
      "[1, 10000] loss: 1.041\n",
      "[1, 12000] loss: 0.967\n",
      "[2,  2000] loss: 0.857\n",
      "[2,  4000] loss: 0.825\n",
      "[2,  6000] loss: 0.794\n",
      "[2,  8000] loss: 0.753\n",
      "[2, 10000] loss: 0.737\n",
      "[2, 12000] loss: 0.716\n",
      "[3,  2000] loss: 0.603\n",
      "[3,  4000] loss: 0.609\n",
      "[3,  6000] loss: 0.602\n",
      "[3,  8000] loss: 0.588\n",
      "[3, 10000] loss: 0.588\n",
      "[3, 12000] loss: 0.562\n",
      "[4,  2000] loss: 0.443\n",
      "[4,  4000] loss: 0.462\n",
      "[4,  6000] loss: 0.443\n",
      "[4,  8000] loss: 0.464\n",
      "[4, 10000] loss: 0.456\n",
      "[4, 12000] loss: 0.454\n",
      "[5,  2000] loss: 0.330\n",
      "[5,  4000] loss: 0.329\n",
      "[5,  6000] loss: 0.364\n",
      "[5,  8000] loss: 0.343\n",
      "[5, 10000] loss: 0.381\n",
      "[5, 12000] loss: 0.358\n",
      "[6,  2000] loss: 0.233\n",
      "[6,  4000] loss: 0.249\n",
      "[6,  6000] loss: 0.262\n",
      "[6,  8000] loss: 0.273\n",
      "[6, 10000] loss: 0.276\n",
      "[6, 12000] loss: 0.287\n",
      "[7,  2000] loss: 0.174\n",
      "[7,  4000] loss: 0.177\n",
      "[7,  6000] loss: 0.204\n",
      "[7,  8000] loss: 0.193\n",
      "[7, 10000] loss: 0.203\n",
      "[7, 12000] loss: 0.202\n",
      "[8,  2000] loss: 0.120\n",
      "[8,  4000] loss: 0.127\n",
      "[8,  6000] loss: 0.148\n",
      "[8,  8000] loss: 0.160\n",
      "[8, 10000] loss: 0.162\n",
      "[8, 12000] loss: 0.164\n",
      "[9,  2000] loss: 0.085\n",
      "[9,  4000] loss: 0.097\n",
      "[9,  6000] loss: 0.108\n",
      "[9,  8000] loss: 0.119\n",
      "[9, 10000] loss: 0.127\n",
      "[9, 12000] loss: 0.130\n",
      "[10,  2000] loss: 0.081\n",
      "[10,  4000] loss: 0.085\n",
      "[10,  6000] loss: 0.097\n",
      "[10,  8000] loss: 0.093\n",
      "[10, 10000] loss: 0.115\n",
      "[10, 12000] loss: 0.119\n",
      "[11,  2000] loss: 0.061\n",
      "[11,  4000] loss: 0.067\n",
      "[11,  6000] loss: 0.080\n",
      "[11,  8000] loss: 0.072\n",
      "[11, 10000] loss: 0.079\n",
      "[11, 12000] loss: 0.084\n",
      "[12,  2000] loss: 0.047\n",
      "[12,  4000] loss: 0.052\n",
      "[12,  6000] loss: 0.066\n",
      "[12,  8000] loss: 0.069\n",
      "[12, 10000] loss: 0.061\n",
      "[12, 12000] loss: 0.069\n",
      "[13,  2000] loss: 0.054\n",
      "[13,  4000] loss: 0.048\n",
      "[13,  6000] loss: 0.045\n",
      "[13,  8000] loss: 0.059\n",
      "[13, 10000] loss: 0.065\n",
      "[13, 12000] loss: 0.056\n",
      "[14,  2000] loss: 0.037\n",
      "[14,  4000] loss: 0.041\n",
      "[14,  6000] loss: 0.048\n",
      "[14,  8000] loss: 0.052\n",
      "[14, 10000] loss: 0.041\n",
      "[14, 12000] loss: 0.054\n",
      "[15,  2000] loss: 0.041\n",
      "[15,  4000] loss: 0.045\n",
      "[15,  6000] loss: 0.038\n",
      "[15,  8000] loss: 0.039\n",
      "[15, 10000] loss: 0.039\n",
      "[15, 12000] loss: 0.052\n",
      "[16,  2000] loss: 0.029\n",
      "[16,  4000] loss: 0.033\n",
      "[16,  6000] loss: 0.038\n",
      "[16,  8000] loss: 0.034\n",
      "[16, 10000] loss: 0.038\n",
      "[16, 12000] loss: 0.043\n",
      "[17,  2000] loss: 0.025\n",
      "[17,  4000] loss: 0.019\n",
      "[17,  6000] loss: 0.035\n",
      "[17,  8000] loss: 0.038\n",
      "[17, 10000] loss: 0.033\n",
      "[17, 12000] loss: 0.033\n",
      "[18,  2000] loss: 0.028\n",
      "[18,  4000] loss: 0.018\n",
      "[18,  6000] loss: 0.022\n",
      "[18,  8000] loss: 0.020\n",
      "[18, 10000] loss: 0.029\n",
      "[18, 12000] loss: 0.028\n",
      "[19,  2000] loss: 0.025\n",
      "[19,  4000] loss: 0.025\n",
      "[19,  6000] loss: 0.027\n",
      "[19,  8000] loss: 0.030\n",
      "[19, 10000] loss: 0.043\n",
      "[19, 12000] loss: 0.037\n",
      "[20,  2000] loss: 0.027\n",
      "[20,  4000] loss: 0.027\n",
      "[20,  6000] loss: 0.022\n",
      "[20,  8000] loss: 0.036\n",
      "[20, 10000] loss: 0.021\n",
      "[20, 12000] loss: 0.028\n",
      "[21,  2000] loss: 0.018\n",
      "[21,  4000] loss: 0.017\n",
      "[21,  6000] loss: 0.020\n",
      "[21,  8000] loss: 0.018\n",
      "[21, 10000] loss: 0.024\n",
      "[21, 12000] loss: 0.025\n",
      "[22,  2000] loss: 0.025\n",
      "[22,  4000] loss: 0.015\n",
      "[22,  6000] loss: 0.019\n",
      "[22,  8000] loss: 0.013\n",
      "[22, 10000] loss: 0.024\n",
      "[22, 12000] loss: 0.012\n",
      "[23,  2000] loss: 0.017\n",
      "[23,  4000] loss: 0.010\n",
      "[23,  6000] loss: 0.015\n",
      "[23,  8000] loss: 0.029\n",
      "[23, 10000] loss: 0.022\n",
      "[23, 12000] loss: 0.027\n",
      "[24,  2000] loss: 0.023\n",
      "[24,  4000] loss: 0.018\n",
      "[24,  6000] loss: 0.025\n",
      "[24,  8000] loss: 0.017\n",
      "[24, 10000] loss: 0.018\n",
      "[24, 12000] loss: 0.017\n",
      "[25,  2000] loss: 0.008\n",
      "[25,  4000] loss: 0.016\n",
      "[25,  6000] loss: 0.016\n",
      "[25,  8000] loss: 0.020\n",
      "[25, 10000] loss: 0.023\n",
      "[25, 12000] loss: 0.019\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=44, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(44),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=44, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                       # this is what you had\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        images =images.cuda()\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ba9ca613c00912bf2bb7336c6f7b766b0be232b7fbb6881178983a86316f18c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
