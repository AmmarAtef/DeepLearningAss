{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet20\n",
      "Total number of params 269722\n",
      "Total layers 20\n",
      "\n",
      "resnet32\n",
      "Total number of params 464154\n",
      "Total layers 32\n",
      "\n",
      "resnet44\n",
      "Total number of params 658586\n",
      "Total layers 44\n",
      "\n",
      "resnet56\n",
      "Total number of params 853018\n",
      "Total layers 56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56']\n",
    " \n",
    "def _weights_init(m):\n",
    "    \"\"\"\n",
    "        Initialization of CNN weights\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
    "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
    "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
    "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 experiment, ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
    "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
    "# The subsampling is performed by convolutions with a stride of 2.\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for net_name in __all__:\n",
    "        if net_name.startswith('resnet'):\n",
    "            print(net_name)\n",
    "            test(globals()[net_name]())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNetArgs:\n",
    "       \"\"\"Passing the hyperparameters to the model\"\"\"\n",
    "       def __init__(self, arch='resnet20' ,epochs=50, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
    "                 evaluate=0, pretrained=0, half=0, save_dir='save_temp', save_every=10):\n",
    "          \n",
    "          self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
    "          self.save_dir = save_dir #The directory used to save the trained models\n",
    "          self.half = half #use half-precision(16-bit)\n",
    "          self.evaluate = evaluate #evaluate model on the validation set\n",
    "          self.pretrained = pretrained #evaluate the pretrained model on the validation set\n",
    "          self.print_freq = print_freq #print frequency \n",
    "          self.weight_decay = weight_decay\n",
    "          self.momentum = momentum \n",
    "          self.lr = lr #Learning rate\n",
    "          self.batch_size = batch_size \n",
    "          self.start_epoch = start_epoch\n",
    "          self.epochs = epochs\n",
    "          self.arch = arch #ResNet model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-18                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-20            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-23                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-25             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-28                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-30             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-7                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-33                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-35            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-8                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-38                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-40             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-9                   [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-43                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-45             [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 269,722\n",
      "Trainable params: 269,722\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 41.09\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.88\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 3.92\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet20',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-28                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-30            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-33                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-35             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-38                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-40             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-43                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-45             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-50             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-11                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-53                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-55            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-12                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-58                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-60             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-13                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-63                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-65             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-14                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-68                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-70             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-15                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-73                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-75             [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 464,154\n",
      "Trainable params: 464,154\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 69.79\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.63\n",
      "Params size (MB): 1.77\n",
      "Estimated Total Size (MB): 6.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet32',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        if args.half:\n",
    "            input_var = input_var.half()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            if args.half:\n",
    "                input_var = input_var.half()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
    "          .format(top1=top1,error=100-top1.avg))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.th'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ship             cat           horse             car           truck           truck             car             cat\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 179,
       "width": 1150
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=128, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=128, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "plt.figure(figsize=(20,10)) \n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
    "# print labels\n",
    "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    \n",
    "    # Check the save_dir exists or not\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model =  globals()[args.arch]().to(device)\n",
    "    model.cuda()\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    if args.half:\n",
    "        print('half persicion is used.')\n",
    "        model.half()\n",
    "        criterion.half()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    if args.arch in ['resnet1202']:\n",
    "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
    "        # then switch back. In this setup it will correspond for first epoch.\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr*0.1\n",
    "\n",
    "\n",
    "    if args.evaluate:\n",
    "        print('evalution mode')\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    if args.pretrained:\n",
    "        print('evalution of pretrained model')\n",
    "        args.save_dir='pretrained_models'\n",
    "        pretrained_model= args.arch +'.th'\n",
    "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
    "        best_prec1 = validate(val_loader, model, criterion)\n",
    "        return best_prec1\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        # train for one epoch\n",
    "        print('Training {} model'.format(args.arch))\n",
    "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        if epoch > 0 and epoch % args.save_every == 0:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
    "        if is_best:\n",
    "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
    "\n",
    "    return best_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 3.5689 (3.5689)\tPrec@1 7.031 (7.031)\n",
      "Epoch: [0][55/391]\tLoss 1.8030 (2.1498)\tPrec@1 36.719 (23.521)\n",
      "Epoch: [0][110/391]\tLoss 1.7471 (1.9687)\tPrec@1 37.500 (27.717)\n",
      "Epoch: [0][165/391]\tLoss 1.4042 (1.8601)\tPrec@1 42.188 (31.208)\n",
      "Epoch: [0][220/391]\tLoss 1.4470 (1.7783)\tPrec@1 48.438 (34.159)\n",
      "Epoch: [0][275/391]\tLoss 1.3513 (1.7172)\tPrec@1 50.000 (36.450)\n",
      "Epoch: [0][330/391]\tLoss 1.4397 (1.6599)\tPrec@1 46.875 (38.687)\n",
      "Epoch: [0][385/391]\tLoss 1.1727 (1.6140)\tPrec@1 57.812 (40.459)\n",
      "Test\t  Prec@1: 46.470 (Err: 53.530 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.2773 (1.2773)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [1][55/391]\tLoss 1.3059 (1.2579)\tPrec@1 47.656 (53.697)\n",
      "Epoch: [1][110/391]\tLoss 1.1017 (1.2219)\tPrec@1 59.375 (55.279)\n",
      "Epoch: [1][165/391]\tLoss 1.0425 (1.1914)\tPrec@1 57.812 (56.255)\n",
      "Epoch: [1][220/391]\tLoss 1.1828 (1.1725)\tPrec@1 59.375 (56.999)\n",
      "Epoch: [1][275/391]\tLoss 1.2741 (1.1544)\tPrec@1 55.469 (57.801)\n",
      "Epoch: [1][330/391]\tLoss 1.0492 (1.1360)\tPrec@1 62.500 (58.665)\n",
      "Epoch: [1][385/391]\tLoss 0.8574 (1.1174)\tPrec@1 67.969 (59.505)\n",
      "Test\t  Prec@1: 63.240 (Err: 36.760 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 0.9489 (0.9489)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [2][55/391]\tLoss 0.9210 (0.9865)\tPrec@1 64.844 (64.774)\n",
      "Epoch: [2][110/391]\tLoss 0.7821 (0.9646)\tPrec@1 73.438 (65.407)\n",
      "Epoch: [2][165/391]\tLoss 0.8591 (0.9522)\tPrec@1 68.750 (65.983)\n",
      "Epoch: [2][220/391]\tLoss 0.8963 (0.9489)\tPrec@1 68.750 (66.184)\n",
      "Epoch: [2][275/391]\tLoss 0.8503 (0.9389)\tPrec@1 66.406 (66.585)\n",
      "Epoch: [2][330/391]\tLoss 0.9140 (0.9231)\tPrec@1 67.969 (67.119)\n",
      "Epoch: [2][385/391]\tLoss 0.7370 (0.9105)\tPrec@1 68.750 (67.542)\n",
      "Test\t  Prec@1: 68.570 (Err: 31.430 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 0.7664 (0.7664)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [3][55/391]\tLoss 0.7484 (0.8153)\tPrec@1 75.000 (71.247)\n",
      "Epoch: [3][110/391]\tLoss 0.8691 (0.7918)\tPrec@1 71.875 (72.438)\n",
      "Epoch: [3][165/391]\tLoss 0.7135 (0.7897)\tPrec@1 72.656 (72.581)\n",
      "Epoch: [3][220/391]\tLoss 0.8458 (0.7824)\tPrec@1 67.969 (72.663)\n",
      "Epoch: [3][275/391]\tLoss 0.6493 (0.7752)\tPrec@1 75.000 (72.866)\n",
      "Epoch: [3][330/391]\tLoss 0.7229 (0.7689)\tPrec@1 73.438 (73.043)\n",
      "Epoch: [3][385/391]\tLoss 0.7051 (0.7656)\tPrec@1 74.219 (73.189)\n",
      "Test\t  Prec@1: 68.810 (Err: 31.190 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.7984 (0.7984)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [4][55/391]\tLoss 0.6394 (0.7086)\tPrec@1 77.344 (75.167)\n",
      "Epoch: [4][110/391]\tLoss 0.7483 (0.7013)\tPrec@1 71.875 (75.436)\n",
      "Epoch: [4][165/391]\tLoss 0.5359 (0.6985)\tPrec@1 78.125 (75.485)\n",
      "Epoch: [4][220/391]\tLoss 0.6947 (0.6898)\tPrec@1 73.438 (76.004)\n",
      "Epoch: [4][275/391]\tLoss 0.7542 (0.6840)\tPrec@1 69.531 (76.084)\n",
      "Epoch: [4][330/391]\tLoss 0.8857 (0.6812)\tPrec@1 67.969 (76.095)\n",
      "Epoch: [4][385/391]\tLoss 0.5195 (0.6789)\tPrec@1 80.469 (76.158)\n",
      "Test\t  Prec@1: 74.740 (Err: 25.260 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.6025 (0.6025)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [5][55/391]\tLoss 0.6719 (0.6487)\tPrec@1 70.312 (77.706)\n",
      "Epoch: [5][110/391]\tLoss 0.8590 (0.6427)\tPrec@1 72.656 (77.956)\n",
      "Epoch: [5][165/391]\tLoss 0.5009 (0.6349)\tPrec@1 82.812 (78.064)\n",
      "Epoch: [5][220/391]\tLoss 0.5237 (0.6276)\tPrec@1 79.688 (78.330)\n",
      "Epoch: [5][275/391]\tLoss 0.6653 (0.6202)\tPrec@1 77.344 (78.595)\n",
      "Epoch: [5][330/391]\tLoss 0.5887 (0.6149)\tPrec@1 82.812 (78.722)\n",
      "Epoch: [5][385/391]\tLoss 0.5683 (0.6149)\tPrec@1 78.906 (78.773)\n",
      "Test\t  Prec@1: 73.660 (Err: 26.340 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.8142 (0.8142)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [6][55/391]\tLoss 0.5107 (0.5741)\tPrec@1 81.250 (79.980)\n",
      "Epoch: [6][110/391]\tLoss 0.7281 (0.5805)\tPrec@1 77.344 (79.913)\n",
      "Epoch: [6][165/391]\tLoss 0.5412 (0.5773)\tPrec@1 77.344 (80.045)\n",
      "Epoch: [6][220/391]\tLoss 0.5707 (0.5746)\tPrec@1 81.250 (80.045)\n",
      "Epoch: [6][275/391]\tLoss 0.4418 (0.5758)\tPrec@1 83.594 (79.911)\n",
      "Epoch: [6][330/391]\tLoss 0.4816 (0.5755)\tPrec@1 82.812 (79.919)\n",
      "Epoch: [6][385/391]\tLoss 0.5632 (0.5761)\tPrec@1 77.344 (79.868)\n",
      "Test\t  Prec@1: 74.130 (Err: 25.870 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.5797 (0.5797)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [7][55/391]\tLoss 0.5231 (0.5270)\tPrec@1 81.250 (81.627)\n",
      "Epoch: [7][110/391]\tLoss 0.5902 (0.5403)\tPrec@1 75.781 (81.356)\n",
      "Epoch: [7][165/391]\tLoss 0.6219 (0.5409)\tPrec@1 75.781 (81.151)\n",
      "Epoch: [7][220/391]\tLoss 0.4974 (0.5400)\tPrec@1 82.812 (81.331)\n",
      "Epoch: [7][275/391]\tLoss 0.4991 (0.5385)\tPrec@1 82.031 (81.428)\n",
      "Epoch: [7][330/391]\tLoss 0.5362 (0.5370)\tPrec@1 76.562 (81.432)\n",
      "Epoch: [7][385/391]\tLoss 0.4777 (0.5374)\tPrec@1 80.469 (81.432)\n",
      "Test\t  Prec@1: 77.930 (Err: 22.070 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5622 (0.5622)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [8][55/391]\tLoss 0.5844 (0.4986)\tPrec@1 82.031 (82.743)\n",
      "Epoch: [8][110/391]\tLoss 0.4246 (0.4954)\tPrec@1 82.812 (83.017)\n",
      "Epoch: [8][165/391]\tLoss 0.4946 (0.5020)\tPrec@1 82.812 (82.784)\n",
      "Epoch: [8][220/391]\tLoss 0.5272 (0.5028)\tPrec@1 82.031 (82.728)\n",
      "Epoch: [8][275/391]\tLoss 0.4017 (0.5022)\tPrec@1 87.500 (82.801)\n",
      "Epoch: [8][330/391]\tLoss 0.5002 (0.5053)\tPrec@1 83.594 (82.761)\n",
      "Epoch: [8][385/391]\tLoss 0.6581 (0.5058)\tPrec@1 78.125 (82.636)\n",
      "Test\t  Prec@1: 82.140 (Err: 17.860 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.3994 (0.3994)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [9][55/391]\tLoss 0.4892 (0.4692)\tPrec@1 82.031 (83.789)\n",
      "Epoch: [9][110/391]\tLoss 0.4655 (0.4773)\tPrec@1 81.250 (83.326)\n",
      "Epoch: [9][165/391]\tLoss 0.5915 (0.4885)\tPrec@1 80.469 (83.104)\n",
      "Epoch: [9][220/391]\tLoss 0.4470 (0.4871)\tPrec@1 83.594 (83.187)\n",
      "Epoch: [9][275/391]\tLoss 0.4344 (0.4838)\tPrec@1 85.156 (83.263)\n",
      "Epoch: [9][330/391]\tLoss 0.4399 (0.4858)\tPrec@1 82.812 (83.171)\n",
      "Epoch: [9][385/391]\tLoss 0.4604 (0.4849)\tPrec@1 82.031 (83.159)\n",
      "Test\t  Prec@1: 76.610 (Err: 23.390 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.3568 (0.3568)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.5295 (0.4687)\tPrec@1 81.250 (83.608)\n",
      "Epoch: [10][110/391]\tLoss 0.6595 (0.4715)\tPrec@1 79.688 (83.404)\n",
      "Epoch: [10][165/391]\tLoss 0.4585 (0.4663)\tPrec@1 84.375 (83.697)\n",
      "Epoch: [10][220/391]\tLoss 0.4157 (0.4677)\tPrec@1 85.156 (83.710)\n",
      "Epoch: [10][275/391]\tLoss 0.4001 (0.4683)\tPrec@1 87.500 (83.659)\n",
      "Epoch: [10][330/391]\tLoss 0.5180 (0.4691)\tPrec@1 85.156 (83.613)\n",
      "Epoch: [10][385/391]\tLoss 0.4549 (0.4674)\tPrec@1 80.469 (83.737)\n",
      "Test\t  Prec@1: 83.450 (Err: 16.550 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.3987 (0.3987)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [11][55/391]\tLoss 0.3767 (0.4284)\tPrec@1 86.719 (84.710)\n",
      "Epoch: [11][110/391]\tLoss 0.3893 (0.4373)\tPrec@1 85.938 (84.657)\n",
      "Epoch: [11][165/391]\tLoss 0.3808 (0.4366)\tPrec@1 88.281 (84.752)\n",
      "Epoch: [11][220/391]\tLoss 0.3655 (0.4369)\tPrec@1 89.062 (84.721)\n",
      "Epoch: [11][275/391]\tLoss 0.5566 (0.4404)\tPrec@1 78.125 (84.624)\n",
      "Epoch: [11][330/391]\tLoss 0.4883 (0.4411)\tPrec@1 84.375 (84.729)\n",
      "Epoch: [11][385/391]\tLoss 0.4295 (0.4432)\tPrec@1 85.156 (84.683)\n",
      "Test\t  Prec@1: 81.240 (Err: 18.760 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.3807 (0.3807)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [12][55/391]\tLoss 0.6148 (0.4244)\tPrec@1 81.250 (85.226)\n",
      "Epoch: [12][110/391]\tLoss 0.5343 (0.4123)\tPrec@1 82.812 (85.762)\n",
      "Epoch: [12][165/391]\tLoss 0.3424 (0.4202)\tPrec@1 88.281 (85.561)\n",
      "Epoch: [12][220/391]\tLoss 0.5015 (0.4251)\tPrec@1 80.469 (85.333)\n",
      "Epoch: [12][275/391]\tLoss 0.4952 (0.4234)\tPrec@1 85.938 (85.374)\n",
      "Epoch: [12][330/391]\tLoss 0.4658 (0.4285)\tPrec@1 82.031 (85.156)\n",
      "Epoch: [12][385/391]\tLoss 0.4675 (0.4298)\tPrec@1 82.812 (85.094)\n",
      "Test\t  Prec@1: 81.780 (Err: 18.220 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.4428 (0.4428)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [13][55/391]\tLoss 0.3838 (0.4078)\tPrec@1 86.719 (86.119)\n",
      "Epoch: [13][110/391]\tLoss 0.3502 (0.4117)\tPrec@1 86.719 (85.909)\n",
      "Epoch: [13][165/391]\tLoss 0.4770 (0.4141)\tPrec@1 87.500 (85.853)\n",
      "Epoch: [13][220/391]\tLoss 0.4510 (0.4150)\tPrec@1 81.250 (85.750)\n",
      "Epoch: [13][275/391]\tLoss 0.4052 (0.4132)\tPrec@1 86.719 (85.813)\n",
      "Epoch: [13][330/391]\tLoss 0.5383 (0.4142)\tPrec@1 82.031 (85.753)\n",
      "Epoch: [13][385/391]\tLoss 0.6368 (0.4160)\tPrec@1 80.469 (85.711)\n",
      "Test\t  Prec@1: 77.770 (Err: 22.230 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3677 (0.3677)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [14][55/391]\tLoss 0.4127 (0.4051)\tPrec@1 83.594 (85.993)\n",
      "Epoch: [14][110/391]\tLoss 0.4185 (0.4117)\tPrec@1 87.500 (85.797)\n",
      "Epoch: [14][165/391]\tLoss 0.4011 (0.4083)\tPrec@1 86.719 (85.952)\n",
      "Epoch: [14][220/391]\tLoss 0.3408 (0.4104)\tPrec@1 85.938 (85.757)\n",
      "Epoch: [14][275/391]\tLoss 0.5280 (0.4116)\tPrec@1 81.250 (85.728)\n",
      "Epoch: [14][330/391]\tLoss 0.4579 (0.4108)\tPrec@1 81.250 (85.772)\n",
      "Epoch: [14][385/391]\tLoss 0.3558 (0.4108)\tPrec@1 87.500 (85.753)\n",
      "Test\t  Prec@1: 81.710 (Err: 18.290 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.4395 (0.4395)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [15][55/391]\tLoss 0.4930 (0.4226)\tPrec@1 85.156 (85.491)\n",
      "Epoch: [15][110/391]\tLoss 0.3738 (0.4039)\tPrec@1 84.375 (85.923)\n",
      "Epoch: [15][165/391]\tLoss 0.5745 (0.4086)\tPrec@1 84.375 (85.721)\n",
      "Epoch: [15][220/391]\tLoss 0.4040 (0.4040)\tPrec@1 84.375 (86.022)\n",
      "Epoch: [15][275/391]\tLoss 0.3452 (0.4027)\tPrec@1 84.375 (86.020)\n",
      "Epoch: [15][330/391]\tLoss 0.3901 (0.3994)\tPrec@1 86.719 (86.218)\n",
      "Epoch: [15][385/391]\tLoss 0.2796 (0.3973)\tPrec@1 89.844 (86.247)\n",
      "Test\t  Prec@1: 82.700 (Err: 17.300 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.4757 (0.4757)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [16][55/391]\tLoss 0.3757 (0.3904)\tPrec@1 84.375 (86.063)\n",
      "Epoch: [16][110/391]\tLoss 0.3666 (0.3832)\tPrec@1 85.938 (86.677)\n",
      "Epoch: [16][165/391]\tLoss 0.4184 (0.3832)\tPrec@1 85.156 (86.672)\n",
      "Epoch: [16][220/391]\tLoss 0.3525 (0.3833)\tPrec@1 86.719 (86.698)\n",
      "Epoch: [16][275/391]\tLoss 0.2559 (0.3834)\tPrec@1 92.188 (86.671)\n",
      "Epoch: [16][330/391]\tLoss 0.2999 (0.3826)\tPrec@1 91.406 (86.709)\n",
      "Epoch: [16][385/391]\tLoss 0.4011 (0.3865)\tPrec@1 85.938 (86.601)\n",
      "Test\t  Prec@1: 82.910 (Err: 17.090 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.2835 (0.2835)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [17][55/391]\tLoss 0.3373 (0.3613)\tPrec@1 88.281 (87.374)\n",
      "Epoch: [17][110/391]\tLoss 0.3461 (0.3655)\tPrec@1 85.938 (87.310)\n",
      "Epoch: [17][165/391]\tLoss 0.3945 (0.3665)\tPrec@1 89.062 (87.293)\n",
      "Epoch: [17][220/391]\tLoss 0.3269 (0.3691)\tPrec@1 89.844 (87.175)\n",
      "Epoch: [17][275/391]\tLoss 0.2431 (0.3672)\tPrec@1 91.406 (87.291)\n",
      "Epoch: [17][330/391]\tLoss 0.4153 (0.3736)\tPrec@1 86.719 (87.137)\n",
      "Epoch: [17][385/391]\tLoss 0.3621 (0.3751)\tPrec@1 85.156 (87.047)\n",
      "Test\t  Prec@1: 84.350 (Err: 15.650 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3107 (0.3107)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [18][55/391]\tLoss 0.4292 (0.3574)\tPrec@1 86.719 (87.779)\n",
      "Epoch: [18][110/391]\tLoss 0.3607 (0.3588)\tPrec@1 85.938 (87.613)\n",
      "Epoch: [18][165/391]\tLoss 0.3895 (0.3609)\tPrec@1 85.156 (87.472)\n",
      "Epoch: [18][220/391]\tLoss 0.3360 (0.3611)\tPrec@1 88.281 (87.557)\n",
      "Epoch: [18][275/391]\tLoss 0.2975 (0.3603)\tPrec@1 89.062 (87.528)\n",
      "Epoch: [18][330/391]\tLoss 0.4372 (0.3614)\tPrec@1 84.375 (87.491)\n",
      "Epoch: [18][385/391]\tLoss 0.3984 (0.3657)\tPrec@1 86.719 (87.340)\n",
      "Test\t  Prec@1: 83.950 (Err: 16.050 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3073 (0.3073)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [19][55/391]\tLoss 0.3183 (0.3441)\tPrec@1 89.844 (88.128)\n",
      "Epoch: [19][110/391]\tLoss 0.3024 (0.3439)\tPrec@1 89.844 (87.922)\n",
      "Epoch: [19][165/391]\tLoss 0.2869 (0.3442)\tPrec@1 92.188 (87.914)\n",
      "Epoch: [19][220/391]\tLoss 0.3201 (0.3480)\tPrec@1 86.719 (87.723)\n",
      "Epoch: [19][275/391]\tLoss 0.4453 (0.3546)\tPrec@1 83.594 (87.531)\n",
      "Epoch: [19][330/391]\tLoss 0.3360 (0.3591)\tPrec@1 89.844 (87.441)\n",
      "Epoch: [19][385/391]\tLoss 0.3225 (0.3629)\tPrec@1 90.625 (87.362)\n",
      "Test\t  Prec@1: 75.220 (Err: 24.780 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.4031 (0.4031)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [20][55/391]\tLoss 0.3010 (0.3444)\tPrec@1 89.844 (88.030)\n",
      "Epoch: [20][110/391]\tLoss 0.2723 (0.3437)\tPrec@1 89.062 (88.042)\n",
      "Epoch: [20][165/391]\tLoss 0.4638 (0.3425)\tPrec@1 84.375 (88.093)\n",
      "Epoch: [20][220/391]\tLoss 0.4811 (0.3493)\tPrec@1 86.719 (87.892)\n",
      "Epoch: [20][275/391]\tLoss 0.2438 (0.3489)\tPrec@1 92.188 (87.843)\n",
      "Epoch: [20][330/391]\tLoss 0.3735 (0.3489)\tPrec@1 84.375 (87.795)\n",
      "Epoch: [20][385/391]\tLoss 0.3382 (0.3538)\tPrec@1 88.281 (87.674)\n",
      "Test\t  Prec@1: 81.890 (Err: 18.110 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.2055 (0.2055)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [21][55/391]\tLoss 0.3908 (0.3503)\tPrec@1 88.281 (87.849)\n",
      "Epoch: [21][110/391]\tLoss 0.2945 (0.3430)\tPrec@1 89.844 (87.950)\n",
      "Epoch: [21][165/391]\tLoss 0.4068 (0.3408)\tPrec@1 85.938 (88.074)\n",
      "Epoch: [21][220/391]\tLoss 0.3528 (0.3446)\tPrec@1 86.719 (87.917)\n",
      "Epoch: [21][275/391]\tLoss 0.3164 (0.3467)\tPrec@1 87.500 (87.780)\n",
      "Epoch: [21][330/391]\tLoss 0.3493 (0.3474)\tPrec@1 91.406 (87.856)\n",
      "Epoch: [21][385/391]\tLoss 0.3860 (0.3452)\tPrec@1 86.719 (87.945)\n",
      "Test\t  Prec@1: 82.440 (Err: 17.560 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.3873 (0.3873)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [22][55/391]\tLoss 0.4480 (0.3437)\tPrec@1 86.719 (88.100)\n",
      "Epoch: [22][110/391]\tLoss 0.3564 (0.3397)\tPrec@1 87.500 (88.112)\n",
      "Epoch: [22][165/391]\tLoss 0.3387 (0.3399)\tPrec@1 89.062 (88.145)\n",
      "Epoch: [22][220/391]\tLoss 0.3005 (0.3395)\tPrec@1 87.500 (88.136)\n",
      "Epoch: [22][275/391]\tLoss 0.4456 (0.3401)\tPrec@1 85.938 (88.131)\n",
      "Epoch: [22][330/391]\tLoss 0.4902 (0.3406)\tPrec@1 82.031 (88.050)\n",
      "Epoch: [22][385/391]\tLoss 0.2498 (0.3411)\tPrec@1 90.625 (88.107)\n",
      "Test\t  Prec@1: 85.170 (Err: 14.830 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2417 (0.2417)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [23][55/391]\tLoss 0.3186 (0.3182)\tPrec@1 88.281 (88.937)\n",
      "Epoch: [23][110/391]\tLoss 0.2955 (0.3165)\tPrec@1 92.188 (89.168)\n",
      "Epoch: [23][165/391]\tLoss 0.5401 (0.3257)\tPrec@1 82.812 (88.766)\n",
      "Epoch: [23][220/391]\tLoss 0.4309 (0.3292)\tPrec@1 86.719 (88.652)\n",
      "Epoch: [23][275/391]\tLoss 0.3749 (0.3310)\tPrec@1 86.719 (88.519)\n",
      "Epoch: [23][330/391]\tLoss 0.4910 (0.3358)\tPrec@1 83.594 (88.281)\n",
      "Epoch: [23][385/391]\tLoss 0.3150 (0.3372)\tPrec@1 89.844 (88.281)\n",
      "Test\t  Prec@1: 82.520 (Err: 17.480 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.3067 (0.3067)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [24][55/391]\tLoss 0.3308 (0.3250)\tPrec@1 88.281 (88.881)\n",
      "Epoch: [24][110/391]\tLoss 0.3420 (0.3256)\tPrec@1 86.719 (88.929)\n",
      "Epoch: [24][165/391]\tLoss 0.3027 (0.3274)\tPrec@1 90.625 (88.837)\n",
      "Epoch: [24][220/391]\tLoss 0.3665 (0.3257)\tPrec@1 88.281 (88.879)\n",
      "Epoch: [24][275/391]\tLoss 0.2296 (0.3274)\tPrec@1 92.969 (88.697)\n",
      "Epoch: [24][330/391]\tLoss 0.2829 (0.3259)\tPrec@1 88.281 (88.742)\n",
      "Epoch: [24][385/391]\tLoss 0.5488 (0.3292)\tPrec@1 82.812 (88.670)\n",
      "Test\t  Prec@1: 80.440 (Err: 19.560 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.3675 (0.3675)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [25][55/391]\tLoss 0.3133 (0.3211)\tPrec@1 86.719 (88.825)\n",
      "Epoch: [25][110/391]\tLoss 0.3875 (0.3223)\tPrec@1 86.719 (88.795)\n",
      "Epoch: [25][165/391]\tLoss 0.4008 (0.3261)\tPrec@1 85.938 (88.535)\n",
      "Epoch: [25][220/391]\tLoss 0.3932 (0.3256)\tPrec@1 87.500 (88.631)\n",
      "Epoch: [25][275/391]\tLoss 0.3408 (0.3262)\tPrec@1 88.281 (88.638)\n",
      "Epoch: [25][330/391]\tLoss 0.3999 (0.3288)\tPrec@1 84.375 (88.508)\n",
      "Epoch: [25][385/391]\tLoss 0.3615 (0.3310)\tPrec@1 88.281 (88.453)\n",
      "Test\t  Prec@1: 86.120 (Err: 13.880 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.3158 (0.3158)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [26][55/391]\tLoss 0.4130 (0.2820)\tPrec@1 86.719 (90.234)\n",
      "Epoch: [26][110/391]\tLoss 0.2709 (0.3036)\tPrec@1 90.625 (89.358)\n",
      "Epoch: [26][165/391]\tLoss 0.3091 (0.3080)\tPrec@1 88.281 (89.180)\n",
      "Epoch: [26][220/391]\tLoss 0.2806 (0.3084)\tPrec@1 90.625 (89.186)\n",
      "Epoch: [26][275/391]\tLoss 0.3651 (0.3111)\tPrec@1 86.719 (89.062)\n",
      "Epoch: [26][330/391]\tLoss 0.3532 (0.3134)\tPrec@1 85.938 (88.980)\n",
      "Epoch: [26][385/391]\tLoss 0.3429 (0.3152)\tPrec@1 89.844 (88.892)\n",
      "Test\t  Prec@1: 83.130 (Err: 16.870 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.2888 (0.2888)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [27][55/391]\tLoss 0.3214 (0.2960)\tPrec@1 88.281 (89.886)\n",
      "Epoch: [27][110/391]\tLoss 0.3418 (0.3044)\tPrec@1 88.281 (89.562)\n",
      "Epoch: [27][165/391]\tLoss 0.2159 (0.3082)\tPrec@1 92.188 (89.284)\n",
      "Epoch: [27][220/391]\tLoss 0.2813 (0.3113)\tPrec@1 91.406 (89.169)\n",
      "Epoch: [27][275/391]\tLoss 0.2023 (0.3102)\tPrec@1 93.750 (89.252)\n",
      "Epoch: [27][330/391]\tLoss 0.3516 (0.3118)\tPrec@1 87.500 (89.178)\n",
      "Epoch: [27][385/391]\tLoss 0.3942 (0.3164)\tPrec@1 89.844 (88.998)\n",
      "Test\t  Prec@1: 83.300 (Err: 16.700 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.3009 (0.3009)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [28][55/391]\tLoss 0.2045 (0.2964)\tPrec@1 91.406 (89.495)\n",
      "Epoch: [28][110/391]\tLoss 0.3458 (0.2990)\tPrec@1 86.719 (89.316)\n",
      "Epoch: [28][165/391]\tLoss 0.2541 (0.3034)\tPrec@1 90.625 (89.270)\n",
      "Epoch: [28][220/391]\tLoss 0.2524 (0.3040)\tPrec@1 89.844 (89.345)\n",
      "Epoch: [28][275/391]\tLoss 0.2298 (0.3067)\tPrec@1 92.188 (89.323)\n",
      "Epoch: [28][330/391]\tLoss 0.2150 (0.3105)\tPrec@1 92.188 (89.256)\n",
      "Epoch: [28][385/391]\tLoss 0.2933 (0.3132)\tPrec@1 90.625 (89.186)\n",
      "Test\t  Prec@1: 84.100 (Err: 15.900 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2677 (0.2677)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [29][55/391]\tLoss 0.3773 (0.2900)\tPrec@1 87.500 (89.914)\n",
      "Epoch: [29][110/391]\tLoss 0.2626 (0.2975)\tPrec@1 91.406 (89.780)\n",
      "Epoch: [29][165/391]\tLoss 0.3762 (0.3011)\tPrec@1 87.500 (89.623)\n",
      "Epoch: [29][220/391]\tLoss 0.4282 (0.3003)\tPrec@1 83.594 (89.540)\n",
      "Epoch: [29][275/391]\tLoss 0.2745 (0.3048)\tPrec@1 92.188 (89.334)\n",
      "Epoch: [29][330/391]\tLoss 0.2011 (0.3093)\tPrec@1 92.188 (89.114)\n",
      "Epoch: [29][385/391]\tLoss 0.2742 (0.3105)\tPrec@1 89.844 (89.081)\n",
      "Test\t  Prec@1: 85.050 (Err: 14.950 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2548 (0.2548)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [30][55/391]\tLoss 0.1422 (0.2766)\tPrec@1 94.531 (90.430)\n",
      "Epoch: [30][110/391]\tLoss 0.3146 (0.2905)\tPrec@1 89.062 (89.999)\n",
      "Epoch: [30][165/391]\tLoss 0.3877 (0.2944)\tPrec@1 85.938 (89.839)\n",
      "Epoch: [30][220/391]\tLoss 0.2397 (0.2946)\tPrec@1 92.188 (89.840)\n",
      "Epoch: [30][275/391]\tLoss 0.4618 (0.2987)\tPrec@1 82.812 (89.671)\n",
      "Epoch: [30][330/391]\tLoss 0.2343 (0.3007)\tPrec@1 91.406 (89.631)\n",
      "Epoch: [30][385/391]\tLoss 0.3444 (0.3063)\tPrec@1 88.281 (89.429)\n",
      "Test\t  Prec@1: 85.940 (Err: 14.060 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.3689 (0.3689)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [31][55/391]\tLoss 0.1947 (0.2950)\tPrec@1 92.188 (89.858)\n",
      "Epoch: [31][110/391]\tLoss 0.3254 (0.2988)\tPrec@1 89.844 (89.604)\n",
      "Epoch: [31][165/391]\tLoss 0.2158 (0.2935)\tPrec@1 91.406 (89.811)\n",
      "Epoch: [31][220/391]\tLoss 0.4515 (0.2962)\tPrec@1 84.375 (89.646)\n",
      "Epoch: [31][275/391]\tLoss 0.3743 (0.2992)\tPrec@1 87.500 (89.518)\n",
      "Epoch: [31][330/391]\tLoss 0.3167 (0.3010)\tPrec@1 89.844 (89.499)\n",
      "Epoch: [31][385/391]\tLoss 0.2668 (0.3010)\tPrec@1 91.406 (89.520)\n",
      "Test\t  Prec@1: 87.260 (Err: 12.740 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.2647 (0.2647)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [32][55/391]\tLoss 0.3998 (0.2815)\tPrec@1 83.594 (90.025)\n",
      "Epoch: [32][110/391]\tLoss 0.1999 (0.2938)\tPrec@1 92.969 (89.661)\n",
      "Epoch: [32][165/391]\tLoss 0.3356 (0.2966)\tPrec@1 89.844 (89.641)\n",
      "Epoch: [32][220/391]\tLoss 0.2504 (0.3018)\tPrec@1 91.406 (89.384)\n",
      "Epoch: [32][275/391]\tLoss 0.2584 (0.2970)\tPrec@1 91.406 (89.589)\n",
      "Epoch: [32][330/391]\tLoss 0.2337 (0.2953)\tPrec@1 92.188 (89.641)\n",
      "Epoch: [32][385/391]\tLoss 0.4041 (0.2949)\tPrec@1 83.594 (89.664)\n",
      "Test\t  Prec@1: 86.410 (Err: 13.590 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.3303 (0.3303)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [33][55/391]\tLoss 0.3430 (0.2902)\tPrec@1 88.281 (89.634)\n",
      "Epoch: [33][110/391]\tLoss 0.2337 (0.2980)\tPrec@1 91.406 (89.633)\n",
      "Epoch: [33][165/391]\tLoss 0.3210 (0.2988)\tPrec@1 86.719 (89.575)\n",
      "Epoch: [33][220/391]\tLoss 0.3998 (0.2999)\tPrec@1 88.281 (89.494)\n",
      "Epoch: [33][275/391]\tLoss 0.2206 (0.2997)\tPrec@1 92.969 (89.558)\n",
      "Epoch: [33][330/391]\tLoss 0.3468 (0.2989)\tPrec@1 89.844 (89.594)\n",
      "Epoch: [33][385/391]\tLoss 0.3262 (0.2982)\tPrec@1 86.719 (89.603)\n",
      "Test\t  Prec@1: 83.480 (Err: 16.520 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.3326 (0.3326)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [34][55/391]\tLoss 0.3934 (0.2680)\tPrec@1 82.031 (90.695)\n",
      "Epoch: [34][110/391]\tLoss 0.2438 (0.2749)\tPrec@1 92.188 (90.358)\n",
      "Epoch: [34][165/391]\tLoss 0.2857 (0.2840)\tPrec@1 89.844 (90.140)\n",
      "Epoch: [34][220/391]\tLoss 0.3011 (0.2857)\tPrec@1 88.281 (90.024)\n",
      "Epoch: [34][275/391]\tLoss 0.2749 (0.2853)\tPrec@1 90.625 (90.090)\n",
      "Epoch: [34][330/391]\tLoss 0.3788 (0.2831)\tPrec@1 87.500 (90.210)\n",
      "Epoch: [34][385/391]\tLoss 0.2728 (0.2866)\tPrec@1 90.625 (90.097)\n",
      "Test\t  Prec@1: 83.160 (Err: 16.840 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.1987 (0.1987)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [35][55/391]\tLoss 0.1520 (0.2916)\tPrec@1 96.094 (89.579)\n",
      "Epoch: [35][110/391]\tLoss 0.2863 (0.2833)\tPrec@1 89.062 (89.865)\n",
      "Epoch: [35][165/391]\tLoss 0.3673 (0.2831)\tPrec@1 88.281 (89.914)\n",
      "Epoch: [35][220/391]\tLoss 0.1778 (0.2857)\tPrec@1 92.969 (89.833)\n",
      "Epoch: [35][275/391]\tLoss 0.3048 (0.2835)\tPrec@1 91.406 (89.948)\n",
      "Epoch: [35][330/391]\tLoss 0.2017 (0.2863)\tPrec@1 94.531 (89.889)\n",
      "Epoch: [35][385/391]\tLoss 0.2605 (0.2894)\tPrec@1 91.406 (89.819)\n",
      "Test\t  Prec@1: 81.000 (Err: 19.000 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2734 (0.2734)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [36][55/391]\tLoss 0.2620 (0.2664)\tPrec@1 87.500 (90.653)\n",
      "Epoch: [36][110/391]\tLoss 0.3891 (0.2756)\tPrec@1 85.938 (90.210)\n",
      "Epoch: [36][165/391]\tLoss 0.3283 (0.2781)\tPrec@1 89.844 (90.239)\n",
      "Epoch: [36][220/391]\tLoss 0.2738 (0.2776)\tPrec@1 88.281 (90.233)\n",
      "Epoch: [36][275/391]\tLoss 0.4013 (0.2781)\tPrec@1 83.594 (90.135)\n",
      "Epoch: [36][330/391]\tLoss 0.3685 (0.2811)\tPrec@1 85.156 (90.092)\n",
      "Epoch: [36][385/391]\tLoss 0.4142 (0.2832)\tPrec@1 83.594 (89.998)\n",
      "Test\t  Prec@1: 82.630 (Err: 17.370 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.3453 (0.3453)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [37][55/391]\tLoss 0.2686 (0.2727)\tPrec@1 90.625 (90.318)\n",
      "Epoch: [37][110/391]\tLoss 0.3350 (0.2729)\tPrec@1 87.500 (90.329)\n",
      "Epoch: [37][165/391]\tLoss 0.2703 (0.2768)\tPrec@1 90.625 (90.154)\n",
      "Epoch: [37][220/391]\tLoss 0.1696 (0.2780)\tPrec@1 93.750 (90.144)\n",
      "Epoch: [37][275/391]\tLoss 0.4113 (0.2800)\tPrec@1 86.719 (90.164)\n",
      "Epoch: [37][330/391]\tLoss 0.3527 (0.2847)\tPrec@1 88.281 (90.077)\n",
      "Epoch: [37][385/391]\tLoss 0.2027 (0.2856)\tPrec@1 93.750 (90.066)\n",
      "Test\t  Prec@1: 86.190 (Err: 13.810 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.3085 (0.3085)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.1438 (0.2812)\tPrec@1 96.094 (90.290)\n",
      "Epoch: [38][110/391]\tLoss 0.3418 (0.2750)\tPrec@1 89.844 (90.569)\n",
      "Epoch: [38][165/391]\tLoss 0.2674 (0.2760)\tPrec@1 89.844 (90.465)\n",
      "Epoch: [38][220/391]\tLoss 0.1836 (0.2743)\tPrec@1 92.969 (90.455)\n",
      "Epoch: [38][275/391]\tLoss 0.2268 (0.2754)\tPrec@1 91.406 (90.427)\n",
      "Epoch: [38][330/391]\tLoss 0.3263 (0.2773)\tPrec@1 89.062 (90.295)\n",
      "Epoch: [38][385/391]\tLoss 0.3144 (0.2811)\tPrec@1 88.281 (90.168)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2527 (0.2527)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [39][55/391]\tLoss 0.2892 (0.2715)\tPrec@1 91.406 (90.332)\n",
      "Epoch: [39][110/391]\tLoss 0.2308 (0.2706)\tPrec@1 92.969 (90.435)\n",
      "Epoch: [39][165/391]\tLoss 0.1669 (0.2696)\tPrec@1 93.750 (90.559)\n",
      "Epoch: [39][220/391]\tLoss 0.4453 (0.2738)\tPrec@1 82.031 (90.332)\n",
      "Epoch: [39][275/391]\tLoss 0.2237 (0.2774)\tPrec@1 92.188 (90.220)\n",
      "Epoch: [39][330/391]\tLoss 0.3421 (0.2789)\tPrec@1 86.719 (90.238)\n",
      "Epoch: [39][385/391]\tLoss 0.2236 (0.2764)\tPrec@1 92.969 (90.344)\n",
      "Test\t  Prec@1: 85.560 (Err: 14.440 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.4054 (0.4054)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [40][55/391]\tLoss 0.3599 (0.2686)\tPrec@1 88.281 (90.527)\n",
      "Epoch: [40][110/391]\tLoss 0.2875 (0.2725)\tPrec@1 89.844 (90.456)\n",
      "Epoch: [40][165/391]\tLoss 0.2276 (0.2656)\tPrec@1 93.750 (90.667)\n",
      "Epoch: [40][220/391]\tLoss 0.3266 (0.2683)\tPrec@1 86.719 (90.544)\n",
      "Epoch: [40][275/391]\tLoss 0.3427 (0.2717)\tPrec@1 85.938 (90.441)\n",
      "Epoch: [40][330/391]\tLoss 0.3148 (0.2712)\tPrec@1 89.062 (90.446)\n",
      "Epoch: [40][385/391]\tLoss 0.2254 (0.2737)\tPrec@1 89.844 (90.330)\n",
      "Test\t  Prec@1: 85.830 (Err: 14.170 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2654 (0.2654)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [41][55/391]\tLoss 0.2290 (0.2610)\tPrec@1 91.406 (91.309)\n",
      "Epoch: [41][110/391]\tLoss 0.2183 (0.2582)\tPrec@1 91.406 (91.181)\n",
      "Epoch: [41][165/391]\tLoss 0.2112 (0.2647)\tPrec@1 90.625 (90.954)\n",
      "Epoch: [41][220/391]\tLoss 0.2830 (0.2662)\tPrec@1 89.062 (90.777)\n",
      "Epoch: [41][275/391]\tLoss 0.3432 (0.2706)\tPrec@1 85.938 (90.588)\n",
      "Epoch: [41][330/391]\tLoss 0.3344 (0.2710)\tPrec@1 88.281 (90.526)\n",
      "Epoch: [41][385/391]\tLoss 0.3709 (0.2754)\tPrec@1 85.938 (90.374)\n",
      "Test\t  Prec@1: 82.280 (Err: 17.720 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.3190 (0.3190)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [42][55/391]\tLoss 0.4283 (0.2817)\tPrec@1 84.375 (90.416)\n",
      "Epoch: [42][110/391]\tLoss 0.2294 (0.2671)\tPrec@1 90.625 (90.892)\n",
      "Epoch: [42][165/391]\tLoss 0.2626 (0.2706)\tPrec@1 92.188 (90.672)\n",
      "Epoch: [42][220/391]\tLoss 0.2926 (0.2723)\tPrec@1 89.062 (90.593)\n",
      "Epoch: [42][275/391]\tLoss 0.2404 (0.2723)\tPrec@1 90.625 (90.577)\n",
      "Epoch: [42][330/391]\tLoss 0.2785 (0.2715)\tPrec@1 90.625 (90.594)\n",
      "Epoch: [42][385/391]\tLoss 0.2708 (0.2730)\tPrec@1 92.188 (90.518)\n",
      "Test\t  Prec@1: 85.320 (Err: 14.680 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.1439 (0.1439)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [43][55/391]\tLoss 0.2210 (0.2634)\tPrec@1 93.750 (90.778)\n",
      "Epoch: [43][110/391]\tLoss 0.2801 (0.2643)\tPrec@1 89.844 (90.731)\n",
      "Epoch: [43][165/391]\tLoss 0.2711 (0.2667)\tPrec@1 90.625 (90.729)\n",
      "Epoch: [43][220/391]\tLoss 0.1890 (0.2688)\tPrec@1 93.750 (90.770)\n",
      "Epoch: [43][275/391]\tLoss 0.2519 (0.2710)\tPrec@1 91.406 (90.707)\n",
      "Epoch: [43][330/391]\tLoss 0.3431 (0.2708)\tPrec@1 88.281 (90.691)\n",
      "Epoch: [43][385/391]\tLoss 0.1796 (0.2709)\tPrec@1 94.531 (90.670)\n",
      "Test\t  Prec@1: 85.550 (Err: 14.450 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2111 (0.2111)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [44][55/391]\tLoss 0.3024 (0.2318)\tPrec@1 89.062 (92.034)\n",
      "Epoch: [44][110/391]\tLoss 0.2772 (0.2388)\tPrec@1 88.281 (91.624)\n",
      "Epoch: [44][165/391]\tLoss 0.2293 (0.2447)\tPrec@1 91.406 (91.491)\n",
      "Epoch: [44][220/391]\tLoss 0.3806 (0.2571)\tPrec@1 90.625 (91.067)\n",
      "Epoch: [44][275/391]\tLoss 0.2053 (0.2621)\tPrec@1 90.625 (90.863)\n",
      "Epoch: [44][330/391]\tLoss 0.3181 (0.2633)\tPrec@1 86.719 (90.752)\n",
      "Epoch: [44][385/391]\tLoss 0.3750 (0.2676)\tPrec@1 85.156 (90.635)\n",
      "Test\t  Prec@1: 85.530 (Err: 14.470 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.4227 (0.4227)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [45][55/391]\tLoss 0.2472 (0.2454)\tPrec@1 87.500 (90.932)\n",
      "Epoch: [45][110/391]\tLoss 0.2870 (0.2564)\tPrec@1 87.500 (90.759)\n",
      "Epoch: [45][165/391]\tLoss 0.2308 (0.2602)\tPrec@1 89.844 (90.653)\n",
      "Epoch: [45][220/391]\tLoss 0.2970 (0.2654)\tPrec@1 90.625 (90.558)\n",
      "Epoch: [45][275/391]\tLoss 0.2825 (0.2653)\tPrec@1 91.406 (90.614)\n",
      "Epoch: [45][330/391]\tLoss 0.2552 (0.2656)\tPrec@1 93.750 (90.625)\n",
      "Epoch: [45][385/391]\tLoss 0.2417 (0.2660)\tPrec@1 89.062 (90.647)\n",
      "Test\t  Prec@1: 84.230 (Err: 15.770 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2240 (0.2240)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [46][55/391]\tLoss 0.1927 (0.2509)\tPrec@1 92.188 (91.071)\n",
      "Epoch: [46][110/391]\tLoss 0.2645 (0.2584)\tPrec@1 90.625 (90.949)\n",
      "Epoch: [46][165/391]\tLoss 0.2087 (0.2605)\tPrec@1 92.188 (90.874)\n",
      "Epoch: [46][220/391]\tLoss 0.4249 (0.2617)\tPrec@1 87.500 (90.925)\n",
      "Epoch: [46][275/391]\tLoss 0.3751 (0.2643)\tPrec@1 88.281 (90.815)\n",
      "Epoch: [46][330/391]\tLoss 0.2034 (0.2654)\tPrec@1 93.750 (90.710)\n",
      "Epoch: [46][385/391]\tLoss 0.2014 (0.2700)\tPrec@1 92.969 (90.506)\n",
      "Test\t  Prec@1: 85.320 (Err: 14.680 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.2246 (0.2246)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [47][55/391]\tLoss 0.1906 (0.2339)\tPrec@1 96.094 (91.755)\n",
      "Epoch: [47][110/391]\tLoss 0.2062 (0.2437)\tPrec@1 92.969 (91.456)\n",
      "Epoch: [47][165/391]\tLoss 0.3185 (0.2437)\tPrec@1 89.062 (91.416)\n",
      "Epoch: [47][220/391]\tLoss 0.3483 (0.2491)\tPrec@1 87.500 (91.307)\n",
      "Epoch: [47][275/391]\tLoss 0.2415 (0.2531)\tPrec@1 92.188 (91.140)\n",
      "Epoch: [47][330/391]\tLoss 0.2821 (0.2543)\tPrec@1 90.625 (91.118)\n",
      "Epoch: [47][385/391]\tLoss 0.2325 (0.2564)\tPrec@1 91.406 (91.062)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.2797 (0.2797)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [48][55/391]\tLoss 0.3074 (0.2589)\tPrec@1 89.844 (91.295)\n",
      "Epoch: [48][110/391]\tLoss 0.3188 (0.2553)\tPrec@1 89.844 (91.202)\n",
      "Epoch: [48][165/391]\tLoss 0.2203 (0.2530)\tPrec@1 93.750 (91.237)\n",
      "Epoch: [48][220/391]\tLoss 0.2161 (0.2525)\tPrec@1 92.969 (91.208)\n",
      "Epoch: [48][275/391]\tLoss 0.2372 (0.2540)\tPrec@1 94.531 (91.126)\n",
      "Epoch: [48][330/391]\tLoss 0.3137 (0.2538)\tPrec@1 86.719 (91.104)\n",
      "Epoch: [48][385/391]\tLoss 0.2614 (0.2565)\tPrec@1 89.844 (90.983)\n",
      "Test\t  Prec@1: 84.500 (Err: 15.500 )\n",
      "\n",
      "Training resnet20 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2033 (0.2033)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [49][55/391]\tLoss 0.2537 (0.2507)\tPrec@1 91.406 (91.532)\n",
      "Epoch: [49][110/391]\tLoss 0.1372 (0.2515)\tPrec@1 96.094 (91.322)\n",
      "Epoch: [49][165/391]\tLoss 0.2633 (0.2591)\tPrec@1 89.062 (91.002)\n",
      "Epoch: [49][220/391]\tLoss 0.2551 (0.2608)\tPrec@1 88.281 (90.876)\n",
      "Epoch: [49][275/391]\tLoss 0.2148 (0.2612)\tPrec@1 92.188 (90.823)\n",
      "Epoch: [49][330/391]\tLoss 0.2165 (0.2630)\tPrec@1 92.969 (90.729)\n",
      "Epoch: [49][385/391]\tLoss 0.3681 (0.2668)\tPrec@1 85.938 (90.631)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "The lowest error from resnet20 model after 50 epochs is 12.740\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 4.8126 (4.8126)\tPrec@1 10.938 (10.938)\n",
      "Epoch: [0][55/391]\tLoss 2.0015 (3.2928)\tPrec@1 22.656 (13.560)\n",
      "Epoch: [0][110/391]\tLoss 1.8453 (2.6238)\tPrec@1 31.250 (19.264)\n",
      "Epoch: [0][165/391]\tLoss 1.7414 (2.3557)\tPrec@1 35.938 (22.948)\n",
      "Epoch: [0][220/391]\tLoss 1.6716 (2.1945)\tPrec@1 37.500 (26.057)\n",
      "Epoch: [0][275/391]\tLoss 1.7556 (2.0908)\tPrec@1 33.594 (28.054)\n",
      "Epoch: [0][330/391]\tLoss 1.6340 (2.0123)\tPrec@1 39.062 (30.060)\n",
      "Epoch: [0][385/391]\tLoss 1.5781 (1.9461)\tPrec@1 40.625 (31.835)\n",
      "Test\t  Prec@1: 40.840 (Err: 59.160 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.4036 (1.4036)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [1][55/391]\tLoss 1.3088 (1.4894)\tPrec@1 57.031 (44.587)\n",
      "Epoch: [1][110/391]\tLoss 1.4552 (1.4871)\tPrec@1 46.875 (44.728)\n",
      "Epoch: [1][165/391]\tLoss 1.4664 (1.4694)\tPrec@1 42.188 (45.628)\n",
      "Epoch: [1][220/391]\tLoss 1.5268 (1.4522)\tPrec@1 46.094 (46.394)\n",
      "Epoch: [1][275/391]\tLoss 1.3794 (1.4371)\tPrec@1 50.781 (47.169)\n",
      "Epoch: [1][330/391]\tLoss 1.3241 (1.4179)\tPrec@1 50.000 (47.935)\n",
      "Epoch: [1][385/391]\tLoss 1.2386 (1.3965)\tPrec@1 53.125 (48.725)\n",
      "Test\t  Prec@1: 49.320 (Err: 50.680 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.2987 (1.2987)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [2][55/391]\tLoss 1.0876 (1.2454)\tPrec@1 65.625 (54.925)\n",
      "Epoch: [2][110/391]\tLoss 1.2100 (1.2182)\tPrec@1 52.344 (56.046)\n",
      "Epoch: [2][165/391]\tLoss 1.1827 (1.2003)\tPrec@1 59.375 (56.631)\n",
      "Epoch: [2][220/391]\tLoss 1.1834 (1.1936)\tPrec@1 54.688 (56.890)\n",
      "Epoch: [2][275/391]\tLoss 1.2743 (1.1758)\tPrec@1 55.469 (57.657)\n",
      "Epoch: [2][330/391]\tLoss 1.2347 (1.1608)\tPrec@1 58.594 (58.405)\n",
      "Epoch: [2][385/391]\tLoss 1.0890 (1.1431)\tPrec@1 63.281 (59.092)\n",
      "Test\t  Prec@1: 61.340 (Err: 38.660 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.1142 (1.1142)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [3][55/391]\tLoss 1.0879 (1.0154)\tPrec@1 61.719 (64.216)\n",
      "Epoch: [3][110/391]\tLoss 1.1128 (0.9979)\tPrec@1 60.938 (64.773)\n",
      "Epoch: [3][165/391]\tLoss 1.0278 (0.9848)\tPrec@1 64.844 (65.281)\n",
      "Epoch: [3][220/391]\tLoss 1.0935 (0.9792)\tPrec@1 59.375 (65.480)\n",
      "Epoch: [3][275/391]\tLoss 0.8038 (0.9701)\tPrec@1 68.750 (65.789)\n",
      "Epoch: [3][330/391]\tLoss 1.0201 (0.9609)\tPrec@1 63.281 (66.121)\n",
      "Epoch: [3][385/391]\tLoss 1.0441 (0.9524)\tPrec@1 60.938 (66.420)\n",
      "Test\t  Prec@1: 60.890 (Err: 39.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.7176 (0.7176)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [4][55/391]\tLoss 0.8368 (0.8382)\tPrec@1 71.875 (70.187)\n",
      "Epoch: [4][110/391]\tLoss 0.8790 (0.8329)\tPrec@1 67.969 (70.601)\n",
      "Epoch: [4][165/391]\tLoss 0.9154 (0.8316)\tPrec@1 67.969 (70.731)\n",
      "Epoch: [4][220/391]\tLoss 0.8406 (0.8258)\tPrec@1 69.531 (70.885)\n",
      "Epoch: [4][275/391]\tLoss 0.8671 (0.8214)\tPrec@1 64.844 (71.071)\n",
      "Epoch: [4][330/391]\tLoss 0.7385 (0.8134)\tPrec@1 72.656 (71.403)\n",
      "Epoch: [4][385/391]\tLoss 0.7401 (0.8057)\tPrec@1 75.781 (71.701)\n",
      "Test\t  Prec@1: 72.180 (Err: 27.820 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.8160 (0.8160)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [5][55/391]\tLoss 0.7483 (0.7196)\tPrec@1 74.219 (74.874)\n",
      "Epoch: [5][110/391]\tLoss 0.6509 (0.7300)\tPrec@1 73.438 (75.014)\n",
      "Epoch: [5][165/391]\tLoss 0.7174 (0.7269)\tPrec@1 74.219 (74.976)\n",
      "Epoch: [5][220/391]\tLoss 0.8339 (0.7257)\tPrec@1 66.406 (74.982)\n",
      "Epoch: [5][275/391]\tLoss 0.6924 (0.7204)\tPrec@1 74.219 (75.187)\n",
      "Epoch: [5][330/391]\tLoss 0.5433 (0.7194)\tPrec@1 82.031 (75.220)\n",
      "Epoch: [5][385/391]\tLoss 0.6450 (0.7137)\tPrec@1 77.344 (75.393)\n",
      "Test\t  Prec@1: 76.110 (Err: 23.890 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.7274 (0.7274)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [6][55/391]\tLoss 0.6281 (0.6571)\tPrec@1 77.344 (77.218)\n",
      "Epoch: [6][110/391]\tLoss 0.7972 (0.6554)\tPrec@1 71.094 (77.280)\n",
      "Epoch: [6][165/391]\tLoss 0.6072 (0.6542)\tPrec@1 80.469 (77.367)\n",
      "Epoch: [6][220/391]\tLoss 0.5183 (0.6486)\tPrec@1 84.375 (77.669)\n",
      "Epoch: [6][275/391]\tLoss 0.6642 (0.6515)\tPrec@1 75.781 (77.505)\n",
      "Epoch: [6][330/391]\tLoss 0.7471 (0.6513)\tPrec@1 71.094 (77.511)\n",
      "Epoch: [6][385/391]\tLoss 0.6755 (0.6443)\tPrec@1 78.906 (77.769)\n",
      "Test\t  Prec@1: 75.540 (Err: 24.460 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.5567 (0.5567)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [7][55/391]\tLoss 0.5100 (0.6094)\tPrec@1 87.500 (78.823)\n",
      "Epoch: [7][110/391]\tLoss 0.6434 (0.6018)\tPrec@1 78.125 (78.836)\n",
      "Epoch: [7][165/391]\tLoss 0.7396 (0.6067)\tPrec@1 75.000 (78.789)\n",
      "Epoch: [7][220/391]\tLoss 0.4108 (0.6043)\tPrec@1 84.375 (78.959)\n",
      "Epoch: [7][275/391]\tLoss 0.4062 (0.6008)\tPrec@1 85.156 (79.178)\n",
      "Epoch: [7][330/391]\tLoss 0.6229 (0.5963)\tPrec@1 78.906 (79.315)\n",
      "Epoch: [7][385/391]\tLoss 0.5523 (0.5940)\tPrec@1 85.156 (79.453)\n",
      "Test\t  Prec@1: 76.830 (Err: 23.170 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.4507 (0.4507)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [8][55/391]\tLoss 0.5130 (0.5541)\tPrec@1 79.688 (80.706)\n",
      "Epoch: [8][110/391]\tLoss 0.6295 (0.5531)\tPrec@1 77.344 (80.652)\n",
      "Epoch: [8][165/391]\tLoss 0.4232 (0.5525)\tPrec@1 82.812 (80.681)\n",
      "Epoch: [8][220/391]\tLoss 0.5395 (0.5513)\tPrec@1 82.812 (80.677)\n",
      "Epoch: [8][275/391]\tLoss 0.3687 (0.5528)\tPrec@1 85.938 (80.675)\n",
      "Epoch: [8][330/391]\tLoss 0.6759 (0.5521)\tPrec@1 76.562 (80.776)\n",
      "Epoch: [8][385/391]\tLoss 0.4308 (0.5498)\tPrec@1 82.031 (80.855)\n",
      "Test\t  Prec@1: 79.900 (Err: 20.100 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.4559 (0.4559)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [9][55/391]\tLoss 0.5552 (0.5066)\tPrec@1 79.688 (82.617)\n",
      "Epoch: [9][110/391]\tLoss 0.5377 (0.5162)\tPrec@1 79.688 (82.320)\n",
      "Epoch: [9][165/391]\tLoss 0.4313 (0.5187)\tPrec@1 84.375 (82.154)\n",
      "Epoch: [9][220/391]\tLoss 0.6368 (0.5145)\tPrec@1 75.000 (82.293)\n",
      "Epoch: [9][275/391]\tLoss 0.5262 (0.5163)\tPrec@1 81.250 (82.167)\n",
      "Epoch: [9][330/391]\tLoss 0.5620 (0.5168)\tPrec@1 82.031 (82.156)\n",
      "Epoch: [9][385/391]\tLoss 0.4808 (0.5153)\tPrec@1 82.812 (82.222)\n",
      "Test\t  Prec@1: 77.130 (Err: 22.870 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4580 (0.4580)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.4131 (0.4766)\tPrec@1 87.500 (83.733)\n",
      "Epoch: [10][110/391]\tLoss 0.4089 (0.4833)\tPrec@1 85.938 (83.249)\n",
      "Epoch: [10][165/391]\tLoss 0.5146 (0.4871)\tPrec@1 83.594 (82.973)\n",
      "Epoch: [10][220/391]\tLoss 0.5676 (0.4873)\tPrec@1 78.125 (82.855)\n",
      "Epoch: [10][275/391]\tLoss 0.5460 (0.4855)\tPrec@1 82.812 (82.963)\n",
      "Epoch: [10][330/391]\tLoss 0.4916 (0.4877)\tPrec@1 82.812 (82.940)\n",
      "Epoch: [10][385/391]\tLoss 0.4909 (0.4876)\tPrec@1 82.031 (82.976)\n",
      "Test\t  Prec@1: 78.330 (Err: 21.670 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.5866 (0.5866)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [11][55/391]\tLoss 0.6363 (0.4541)\tPrec@1 78.906 (84.124)\n",
      "Epoch: [11][110/391]\tLoss 0.5787 (0.4550)\tPrec@1 81.250 (84.347)\n",
      "Epoch: [11][165/391]\tLoss 0.4992 (0.4619)\tPrec@1 85.938 (84.055)\n",
      "Epoch: [11][220/391]\tLoss 0.4627 (0.4630)\tPrec@1 79.688 (83.947)\n",
      "Epoch: [11][275/391]\tLoss 0.4798 (0.4665)\tPrec@1 83.594 (83.812)\n",
      "Epoch: [11][330/391]\tLoss 0.3549 (0.4640)\tPrec@1 88.281 (83.903)\n",
      "Epoch: [11][385/391]\tLoss 0.5278 (0.4631)\tPrec@1 81.250 (83.934)\n",
      "Test\t  Prec@1: 79.890 (Err: 20.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.4564 (0.4564)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [12][55/391]\tLoss 0.3342 (0.4441)\tPrec@1 87.500 (84.124)\n",
      "Epoch: [12][110/391]\tLoss 0.4611 (0.4480)\tPrec@1 85.938 (84.213)\n",
      "Epoch: [12][165/391]\tLoss 0.4350 (0.4546)\tPrec@1 82.812 (84.003)\n",
      "Epoch: [12][220/391]\tLoss 0.5660 (0.4594)\tPrec@1 78.125 (83.958)\n",
      "Epoch: [12][275/391]\tLoss 0.5048 (0.4574)\tPrec@1 81.250 (84.047)\n",
      "Epoch: [12][330/391]\tLoss 0.4104 (0.4538)\tPrec@1 85.938 (84.215)\n",
      "Epoch: [12][385/391]\tLoss 0.4093 (0.4503)\tPrec@1 85.938 (84.349)\n",
      "Test\t  Prec@1: 81.220 (Err: 18.780 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.4098 (0.4098)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [13][55/391]\tLoss 0.4210 (0.4245)\tPrec@1 85.938 (85.407)\n",
      "Epoch: [13][110/391]\tLoss 0.3823 (0.4246)\tPrec@1 86.719 (85.410)\n",
      "Epoch: [13][165/391]\tLoss 0.4994 (0.4217)\tPrec@1 85.156 (85.472)\n",
      "Epoch: [13][220/391]\tLoss 0.5407 (0.4209)\tPrec@1 82.031 (85.393)\n",
      "Epoch: [13][275/391]\tLoss 0.4742 (0.4208)\tPrec@1 80.469 (85.422)\n",
      "Epoch: [13][330/391]\tLoss 0.4703 (0.4229)\tPrec@1 82.812 (85.350)\n",
      "Epoch: [13][385/391]\tLoss 0.3737 (0.4257)\tPrec@1 85.156 (85.209)\n",
      "Test\t  Prec@1: 79.910 (Err: 20.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3278 (0.3278)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [14][55/391]\tLoss 0.3389 (0.4046)\tPrec@1 88.281 (86.147)\n",
      "Epoch: [14][110/391]\tLoss 0.5695 (0.4007)\tPrec@1 82.812 (86.289)\n",
      "Epoch: [14][165/391]\tLoss 0.5230 (0.4034)\tPrec@1 81.250 (86.069)\n",
      "Epoch: [14][220/391]\tLoss 0.4928 (0.4064)\tPrec@1 81.250 (85.938)\n",
      "Epoch: [14][275/391]\tLoss 0.4870 (0.4095)\tPrec@1 81.250 (85.804)\n",
      "Epoch: [14][330/391]\tLoss 0.3981 (0.4096)\tPrec@1 83.594 (85.784)\n",
      "Epoch: [14][385/391]\tLoss 0.4779 (0.4123)\tPrec@1 79.688 (85.689)\n",
      "Test\t  Prec@1: 83.040 (Err: 16.960 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3665 (0.3665)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [15][55/391]\tLoss 0.2819 (0.3882)\tPrec@1 88.281 (86.663)\n",
      "Epoch: [15][110/391]\tLoss 0.4127 (0.3871)\tPrec@1 85.156 (86.719)\n",
      "Epoch: [15][165/391]\tLoss 0.5504 (0.3855)\tPrec@1 78.125 (86.611)\n",
      "Epoch: [15][220/391]\tLoss 0.6283 (0.3910)\tPrec@1 78.125 (86.362)\n",
      "Epoch: [15][275/391]\tLoss 0.3656 (0.3934)\tPrec@1 86.719 (86.280)\n",
      "Epoch: [15][330/391]\tLoss 0.3560 (0.3939)\tPrec@1 86.719 (86.303)\n",
      "Epoch: [15][385/391]\tLoss 0.3565 (0.3986)\tPrec@1 86.719 (86.207)\n",
      "Test\t  Prec@1: 82.480 (Err: 17.520 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.2887 (0.2887)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [16][55/391]\tLoss 0.3306 (0.3697)\tPrec@1 85.938 (86.956)\n",
      "Epoch: [16][110/391]\tLoss 0.4107 (0.3710)\tPrec@1 85.938 (86.937)\n",
      "Epoch: [16][165/391]\tLoss 0.2606 (0.3700)\tPrec@1 91.406 (86.963)\n",
      "Epoch: [16][220/391]\tLoss 0.5280 (0.3737)\tPrec@1 82.031 (86.860)\n",
      "Epoch: [16][275/391]\tLoss 0.4083 (0.3779)\tPrec@1 87.500 (86.741)\n",
      "Epoch: [16][330/391]\tLoss 0.4773 (0.3817)\tPrec@1 86.719 (86.648)\n",
      "Epoch: [16][385/391]\tLoss 0.3000 (0.3844)\tPrec@1 92.188 (86.551)\n",
      "Test\t  Prec@1: 81.590 (Err: 18.410 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.3533 (0.3533)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [17][55/391]\tLoss 0.4820 (0.3742)\tPrec@1 83.594 (87.040)\n",
      "Epoch: [17][110/391]\tLoss 0.3640 (0.3586)\tPrec@1 88.281 (87.507)\n",
      "Epoch: [17][165/391]\tLoss 0.2846 (0.3649)\tPrec@1 89.062 (87.232)\n",
      "Epoch: [17][220/391]\tLoss 0.4575 (0.3724)\tPrec@1 82.812 (86.949)\n",
      "Epoch: [17][275/391]\tLoss 0.5135 (0.3742)\tPrec@1 82.031 (86.920)\n",
      "Epoch: [17][330/391]\tLoss 0.2747 (0.3744)\tPrec@1 92.188 (86.934)\n",
      "Epoch: [17][385/391]\tLoss 0.4132 (0.3760)\tPrec@1 88.281 (86.907)\n",
      "Test\t  Prec@1: 80.690 (Err: 19.310 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3336 (0.3336)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [18][55/391]\tLoss 0.4274 (0.3660)\tPrec@1 87.500 (87.333)\n",
      "Epoch: [18][110/391]\tLoss 0.3925 (0.3681)\tPrec@1 88.281 (87.190)\n",
      "Epoch: [18][165/391]\tLoss 0.4131 (0.3663)\tPrec@1 82.812 (87.307)\n",
      "Epoch: [18][220/391]\tLoss 0.3891 (0.3658)\tPrec@1 83.594 (87.203)\n",
      "Epoch: [18][275/391]\tLoss 0.3550 (0.3663)\tPrec@1 88.281 (87.225)\n",
      "Epoch: [18][330/391]\tLoss 0.3345 (0.3663)\tPrec@1 86.719 (87.198)\n",
      "Epoch: [18][385/391]\tLoss 0.5526 (0.3691)\tPrec@1 80.469 (87.182)\n",
      "Test\t  Prec@1: 84.460 (Err: 15.540 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3528 (0.3528)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [19][55/391]\tLoss 0.4343 (0.3373)\tPrec@1 88.281 (88.421)\n",
      "Epoch: [19][110/391]\tLoss 0.4140 (0.3472)\tPrec@1 86.719 (87.986)\n",
      "Epoch: [19][165/391]\tLoss 0.3150 (0.3456)\tPrec@1 85.938 (88.065)\n",
      "Epoch: [19][220/391]\tLoss 0.4725 (0.3462)\tPrec@1 82.031 (88.002)\n",
      "Epoch: [19][275/391]\tLoss 0.3387 (0.3470)\tPrec@1 89.062 (88.029)\n",
      "Epoch: [19][330/391]\tLoss 0.3307 (0.3487)\tPrec@1 88.281 (87.939)\n",
      "Epoch: [19][385/391]\tLoss 0.5151 (0.3527)\tPrec@1 82.812 (87.775)\n",
      "Test\t  Prec@1: 82.790 (Err: 17.210 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.3050 (0.3050)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [20][55/391]\tLoss 0.3422 (0.3405)\tPrec@1 85.938 (88.267)\n",
      "Epoch: [20][110/391]\tLoss 0.3407 (0.3353)\tPrec@1 88.281 (88.471)\n",
      "Epoch: [20][165/391]\tLoss 0.4035 (0.3337)\tPrec@1 85.156 (88.432)\n",
      "Epoch: [20][220/391]\tLoss 0.3023 (0.3364)\tPrec@1 87.500 (88.218)\n",
      "Epoch: [20][275/391]\tLoss 0.3773 (0.3399)\tPrec@1 84.375 (88.134)\n",
      "Epoch: [20][330/391]\tLoss 0.3168 (0.3409)\tPrec@1 88.281 (88.085)\n",
      "Epoch: [20][385/391]\tLoss 0.5271 (0.3426)\tPrec@1 79.688 (88.024)\n",
      "Test\t  Prec@1: 84.480 (Err: 15.520 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.2837 (0.2837)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [21][55/391]\tLoss 0.3296 (0.3289)\tPrec@1 86.719 (88.351)\n",
      "Epoch: [21][110/391]\tLoss 0.2607 (0.3390)\tPrec@1 93.750 (88.309)\n",
      "Epoch: [21][165/391]\tLoss 0.3776 (0.3362)\tPrec@1 85.156 (88.352)\n",
      "Epoch: [21][220/391]\tLoss 0.4336 (0.3324)\tPrec@1 85.156 (88.529)\n",
      "Epoch: [21][275/391]\tLoss 0.2853 (0.3336)\tPrec@1 91.406 (88.434)\n",
      "Epoch: [21][330/391]\tLoss 0.4111 (0.3351)\tPrec@1 83.594 (88.319)\n",
      "Epoch: [21][385/391]\tLoss 0.4387 (0.3388)\tPrec@1 85.938 (88.216)\n",
      "Test\t  Prec@1: 85.160 (Err: 14.840 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.2355 (0.2355)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [22][55/391]\tLoss 0.2654 (0.2996)\tPrec@1 90.625 (89.593)\n",
      "Epoch: [22][110/391]\tLoss 0.4556 (0.3090)\tPrec@1 82.812 (89.070)\n",
      "Epoch: [22][165/391]\tLoss 0.3823 (0.3140)\tPrec@1 85.156 (89.001)\n",
      "Epoch: [22][220/391]\tLoss 0.3570 (0.3242)\tPrec@1 89.062 (88.755)\n",
      "Epoch: [22][275/391]\tLoss 0.2721 (0.3273)\tPrec@1 91.406 (88.578)\n",
      "Epoch: [22][330/391]\tLoss 0.2607 (0.3308)\tPrec@1 92.188 (88.432)\n",
      "Epoch: [22][385/391]\tLoss 0.2564 (0.3301)\tPrec@1 89.062 (88.469)\n",
      "Test\t  Prec@1: 83.170 (Err: 16.830 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2444 (0.2444)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [23][55/391]\tLoss 0.2255 (0.3132)\tPrec@1 92.969 (89.369)\n",
      "Epoch: [23][110/391]\tLoss 0.2375 (0.3151)\tPrec@1 91.406 (89.084)\n",
      "Epoch: [23][165/391]\tLoss 0.2702 (0.3128)\tPrec@1 89.844 (89.147)\n",
      "Epoch: [23][220/391]\tLoss 0.3446 (0.3168)\tPrec@1 85.938 (89.020)\n",
      "Epoch: [23][275/391]\tLoss 0.3189 (0.3182)\tPrec@1 85.156 (88.918)\n",
      "Epoch: [23][330/391]\tLoss 0.3107 (0.3187)\tPrec@1 91.406 (88.954)\n",
      "Epoch: [23][385/391]\tLoss 0.4165 (0.3171)\tPrec@1 85.156 (88.907)\n",
      "Test\t  Prec@1: 83.910 (Err: 16.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.4183 (0.4183)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [24][55/391]\tLoss 0.2707 (0.3234)\tPrec@1 89.062 (88.574)\n",
      "Epoch: [24][110/391]\tLoss 0.3493 (0.3214)\tPrec@1 88.281 (88.718)\n",
      "Epoch: [24][165/391]\tLoss 0.3356 (0.3190)\tPrec@1 89.062 (89.034)\n",
      "Epoch: [24][220/391]\tLoss 0.3152 (0.3199)\tPrec@1 89.844 (89.045)\n",
      "Epoch: [24][275/391]\tLoss 0.3269 (0.3200)\tPrec@1 89.062 (88.935)\n",
      "Epoch: [24][330/391]\tLoss 0.2824 (0.3196)\tPrec@1 93.750 (88.923)\n",
      "Epoch: [24][385/391]\tLoss 0.3513 (0.3219)\tPrec@1 85.938 (88.828)\n",
      "Test\t  Prec@1: 85.760 (Err: 14.240 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2743 (0.2743)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [25][55/391]\tLoss 0.2968 (0.2961)\tPrec@1 89.844 (89.704)\n",
      "Epoch: [25][110/391]\tLoss 0.5021 (0.2986)\tPrec@1 76.562 (89.583)\n",
      "Epoch: [25][165/391]\tLoss 0.2730 (0.3024)\tPrec@1 91.406 (89.575)\n",
      "Epoch: [25][220/391]\tLoss 0.3772 (0.3050)\tPrec@1 85.938 (89.359)\n",
      "Epoch: [25][275/391]\tLoss 0.2961 (0.3081)\tPrec@1 89.844 (89.275)\n",
      "Epoch: [25][330/391]\tLoss 0.2972 (0.3098)\tPrec@1 91.406 (89.195)\n",
      "Epoch: [25][385/391]\tLoss 0.3666 (0.3108)\tPrec@1 88.281 (89.176)\n",
      "Test\t  Prec@1: 82.870 (Err: 17.130 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.3593 (0.3593)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [26][55/391]\tLoss 0.2150 (0.2808)\tPrec@1 92.969 (90.109)\n",
      "Epoch: [26][110/391]\tLoss 0.3442 (0.2926)\tPrec@1 89.844 (89.569)\n",
      "Epoch: [26][165/391]\tLoss 0.2720 (0.3011)\tPrec@1 91.406 (89.406)\n",
      "Epoch: [26][220/391]\tLoss 0.2443 (0.3016)\tPrec@1 90.625 (89.412)\n",
      "Epoch: [26][275/391]\tLoss 0.2732 (0.3024)\tPrec@1 89.062 (89.365)\n",
      "Epoch: [26][330/391]\tLoss 0.2852 (0.3051)\tPrec@1 89.062 (89.313)\n",
      "Epoch: [26][385/391]\tLoss 0.4476 (0.3043)\tPrec@1 85.156 (89.328)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.2404 (0.2404)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [27][55/391]\tLoss 0.3229 (0.2820)\tPrec@1 90.625 (90.137)\n",
      "Epoch: [27][110/391]\tLoss 0.3757 (0.2844)\tPrec@1 84.375 (90.020)\n",
      "Epoch: [27][165/391]\tLoss 0.2538 (0.2871)\tPrec@1 90.625 (89.867)\n",
      "Epoch: [27][220/391]\tLoss 0.3616 (0.2921)\tPrec@1 85.156 (89.671)\n",
      "Epoch: [27][275/391]\tLoss 0.2944 (0.2969)\tPrec@1 90.625 (89.524)\n",
      "Epoch: [27][330/391]\tLoss 0.3420 (0.2972)\tPrec@1 89.062 (89.577)\n",
      "Epoch: [27][385/391]\tLoss 0.2880 (0.2969)\tPrec@1 91.406 (89.591)\n",
      "Test\t  Prec@1: 84.580 (Err: 15.420 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.1668 (0.1668)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [28][55/391]\tLoss 0.3427 (0.2851)\tPrec@1 87.500 (90.179)\n",
      "Epoch: [28][110/391]\tLoss 0.2398 (0.2829)\tPrec@1 92.969 (90.259)\n",
      "Epoch: [28][165/391]\tLoss 0.2818 (0.2812)\tPrec@1 90.625 (90.310)\n",
      "Epoch: [28][220/391]\tLoss 0.1845 (0.2867)\tPrec@1 93.750 (90.084)\n",
      "Epoch: [28][275/391]\tLoss 0.2826 (0.2881)\tPrec@1 89.844 (90.056)\n",
      "Epoch: [28][330/391]\tLoss 0.3952 (0.2916)\tPrec@1 87.500 (89.974)\n",
      "Epoch: [28][385/391]\tLoss 0.3115 (0.2935)\tPrec@1 88.281 (89.890)\n",
      "Test\t  Prec@1: 84.980 (Err: 15.020 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2833 (0.2833)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [29][55/391]\tLoss 0.3609 (0.2804)\tPrec@1 84.375 (90.318)\n",
      "Epoch: [29][110/391]\tLoss 0.2890 (0.2812)\tPrec@1 89.844 (90.132)\n",
      "Epoch: [29][165/391]\tLoss 0.3362 (0.2783)\tPrec@1 89.062 (90.239)\n",
      "Epoch: [29][220/391]\tLoss 0.3588 (0.2856)\tPrec@1 85.938 (90.059)\n",
      "Epoch: [29][275/391]\tLoss 0.2723 (0.2901)\tPrec@1 92.188 (90.002)\n",
      "Epoch: [29][330/391]\tLoss 0.3303 (0.2886)\tPrec@1 87.500 (90.040)\n",
      "Epoch: [29][385/391]\tLoss 0.4446 (0.2905)\tPrec@1 85.156 (89.967)\n",
      "Test\t  Prec@1: 84.120 (Err: 15.880 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2660 (0.2660)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [30][55/391]\tLoss 0.2787 (0.2518)\tPrec@1 89.062 (91.309)\n",
      "Epoch: [30][110/391]\tLoss 0.2638 (0.2657)\tPrec@1 92.188 (90.653)\n",
      "Epoch: [30][165/391]\tLoss 0.2418 (0.2706)\tPrec@1 89.844 (90.550)\n",
      "Epoch: [30][220/391]\tLoss 0.3204 (0.2730)\tPrec@1 89.844 (90.487)\n",
      "Epoch: [30][275/391]\tLoss 0.1999 (0.2777)\tPrec@1 93.750 (90.285)\n",
      "Epoch: [30][330/391]\tLoss 0.3177 (0.2782)\tPrec@1 86.719 (90.287)\n",
      "Epoch: [30][385/391]\tLoss 0.2560 (0.2792)\tPrec@1 92.188 (90.279)\n",
      "Test\t  Prec@1: 84.470 (Err: 15.530 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.3828 (0.3828)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [31][55/391]\tLoss 0.3352 (0.2556)\tPrec@1 88.281 (91.030)\n",
      "Epoch: [31][110/391]\tLoss 0.2698 (0.2599)\tPrec@1 92.969 (90.759)\n",
      "Epoch: [31][165/391]\tLoss 0.4100 (0.2667)\tPrec@1 85.938 (90.644)\n",
      "Epoch: [31][220/391]\tLoss 0.2630 (0.2674)\tPrec@1 89.062 (90.568)\n",
      "Epoch: [31][275/391]\tLoss 0.2953 (0.2715)\tPrec@1 89.844 (90.492)\n",
      "Epoch: [31][330/391]\tLoss 0.1450 (0.2766)\tPrec@1 96.094 (90.370)\n",
      "Epoch: [31][385/391]\tLoss 0.4297 (0.2779)\tPrec@1 82.031 (90.384)\n",
      "Test\t  Prec@1: 84.040 (Err: 15.960 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.2916 (0.2916)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [32][55/391]\tLoss 0.1985 (0.2570)\tPrec@1 92.188 (90.834)\n",
      "Epoch: [32][110/391]\tLoss 0.3156 (0.2735)\tPrec@1 87.500 (90.484)\n",
      "Epoch: [32][165/391]\tLoss 0.3249 (0.2786)\tPrec@1 89.844 (90.258)\n",
      "Epoch: [32][220/391]\tLoss 0.2175 (0.2775)\tPrec@1 93.750 (90.339)\n",
      "Epoch: [32][275/391]\tLoss 0.2457 (0.2776)\tPrec@1 92.188 (90.243)\n",
      "Epoch: [32][330/391]\tLoss 0.3579 (0.2775)\tPrec@1 86.719 (90.245)\n",
      "Epoch: [32][385/391]\tLoss 0.2162 (0.2788)\tPrec@1 92.969 (90.180)\n",
      "Test\t  Prec@1: 83.870 (Err: 16.130 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.1720 (0.1720)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [33][55/391]\tLoss 0.2362 (0.2658)\tPrec@1 91.406 (90.639)\n",
      "Epoch: [33][110/391]\tLoss 0.2795 (0.2699)\tPrec@1 89.062 (90.393)\n",
      "Epoch: [33][165/391]\tLoss 0.2216 (0.2648)\tPrec@1 91.406 (90.606)\n",
      "Epoch: [33][220/391]\tLoss 0.2972 (0.2685)\tPrec@1 89.844 (90.586)\n",
      "Epoch: [33][275/391]\tLoss 0.2439 (0.2739)\tPrec@1 90.625 (90.481)\n",
      "Epoch: [33][330/391]\tLoss 0.2340 (0.2770)\tPrec@1 90.625 (90.365)\n",
      "Epoch: [33][385/391]\tLoss 0.3742 (0.2756)\tPrec@1 89.844 (90.459)\n",
      "Test\t  Prec@1: 83.530 (Err: 16.470 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.1902 (0.1902)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [34][55/391]\tLoss 0.2408 (0.2632)\tPrec@1 88.281 (90.569)\n",
      "Epoch: [34][110/391]\tLoss 0.2308 (0.2614)\tPrec@1 95.312 (90.738)\n",
      "Epoch: [34][165/391]\tLoss 0.2392 (0.2674)\tPrec@1 90.625 (90.536)\n",
      "Epoch: [34][220/391]\tLoss 0.2846 (0.2749)\tPrec@1 86.719 (90.296)\n",
      "Epoch: [34][275/391]\tLoss 0.2598 (0.2728)\tPrec@1 89.844 (90.384)\n",
      "Epoch: [34][330/391]\tLoss 0.3095 (0.2740)\tPrec@1 89.062 (90.417)\n",
      "Epoch: [34][385/391]\tLoss 0.2931 (0.2729)\tPrec@1 88.281 (90.487)\n",
      "Test\t  Prec@1: 83.910 (Err: 16.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.1858 (0.1858)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [35][55/391]\tLoss 0.3438 (0.2499)\tPrec@1 91.406 (91.420)\n",
      "Epoch: [35][110/391]\tLoss 0.1806 (0.2426)\tPrec@1 93.750 (91.716)\n",
      "Epoch: [35][165/391]\tLoss 0.2293 (0.2426)\tPrec@1 92.969 (91.665)\n",
      "Epoch: [35][220/391]\tLoss 0.1717 (0.2494)\tPrec@1 94.531 (91.374)\n",
      "Epoch: [35][275/391]\tLoss 0.2433 (0.2533)\tPrec@1 92.188 (91.168)\n",
      "Epoch: [35][330/391]\tLoss 0.2417 (0.2571)\tPrec@1 87.500 (91.010)\n",
      "Epoch: [35][385/391]\tLoss 0.1561 (0.2591)\tPrec@1 96.094 (90.941)\n",
      "Test\t  Prec@1: 85.590 (Err: 14.410 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2007 (0.2007)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [36][55/391]\tLoss 0.2534 (0.2391)\tPrec@1 89.062 (91.741)\n",
      "Epoch: [36][110/391]\tLoss 0.1648 (0.2402)\tPrec@1 94.531 (91.681)\n",
      "Epoch: [36][165/391]\tLoss 0.2590 (0.2491)\tPrec@1 91.406 (91.350)\n",
      "Epoch: [36][220/391]\tLoss 0.2312 (0.2514)\tPrec@1 92.969 (91.332)\n",
      "Epoch: [36][275/391]\tLoss 0.3297 (0.2539)\tPrec@1 85.938 (91.217)\n",
      "Epoch: [36][330/391]\tLoss 0.3065 (0.2570)\tPrec@1 91.406 (91.090)\n",
      "Epoch: [36][385/391]\tLoss 0.3012 (0.2599)\tPrec@1 89.062 (90.985)\n",
      "Test\t  Prec@1: 86.750 (Err: 13.250 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.2990 (0.2990)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [37][55/391]\tLoss 0.2792 (0.2474)\tPrec@1 89.062 (91.546)\n",
      "Epoch: [37][110/391]\tLoss 0.2289 (0.2457)\tPrec@1 92.188 (91.484)\n",
      "Epoch: [37][165/391]\tLoss 0.2309 (0.2513)\tPrec@1 90.625 (91.331)\n",
      "Epoch: [37][220/391]\tLoss 0.2618 (0.2540)\tPrec@1 89.062 (91.251)\n",
      "Epoch: [37][275/391]\tLoss 0.2606 (0.2544)\tPrec@1 90.625 (91.177)\n",
      "Epoch: [37][330/391]\tLoss 0.2255 (0.2548)\tPrec@1 90.625 (91.196)\n",
      "Epoch: [37][385/391]\tLoss 0.2016 (0.2545)\tPrec@1 93.750 (91.182)\n",
      "Test\t  Prec@1: 82.360 (Err: 17.640 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.3108 (0.3108)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.2647 (0.2416)\tPrec@1 90.625 (91.546)\n",
      "Epoch: [38][110/391]\tLoss 0.2728 (0.2589)\tPrec@1 88.281 (90.970)\n",
      "Epoch: [38][165/391]\tLoss 0.1842 (0.2606)\tPrec@1 95.312 (90.945)\n",
      "Epoch: [38][220/391]\tLoss 0.2224 (0.2575)\tPrec@1 93.750 (91.067)\n",
      "Epoch: [38][275/391]\tLoss 0.2391 (0.2586)\tPrec@1 92.188 (91.098)\n",
      "Epoch: [38][330/391]\tLoss 0.2548 (0.2609)\tPrec@1 91.406 (90.996)\n",
      "Epoch: [38][385/391]\tLoss 0.3158 (0.2621)\tPrec@1 84.375 (90.933)\n",
      "Test\t  Prec@1: 86.930 (Err: 13.070 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2072 (0.2072)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [39][55/391]\tLoss 0.1355 (0.2352)\tPrec@1 96.875 (91.685)\n",
      "Epoch: [39][110/391]\tLoss 0.2806 (0.2412)\tPrec@1 89.062 (91.498)\n",
      "Epoch: [39][165/391]\tLoss 0.3172 (0.2445)\tPrec@1 89.844 (91.467)\n",
      "Epoch: [39][220/391]\tLoss 0.2318 (0.2511)\tPrec@1 90.625 (91.268)\n",
      "Epoch: [39][275/391]\tLoss 0.1982 (0.2492)\tPrec@1 92.188 (91.319)\n",
      "Epoch: [39][330/391]\tLoss 0.1809 (0.2500)\tPrec@1 93.750 (91.302)\n",
      "Epoch: [39][385/391]\tLoss 0.2133 (0.2501)\tPrec@1 92.969 (91.283)\n",
      "Test\t  Prec@1: 84.180 (Err: 15.820 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.3044 (0.3044)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [40][55/391]\tLoss 0.2094 (0.2322)\tPrec@1 92.969 (91.895)\n",
      "Epoch: [40][110/391]\tLoss 0.2516 (0.2277)\tPrec@1 92.188 (92.033)\n",
      "Epoch: [40][165/391]\tLoss 0.2230 (0.2310)\tPrec@1 90.625 (91.839)\n",
      "Epoch: [40][220/391]\tLoss 0.1951 (0.2392)\tPrec@1 92.969 (91.583)\n",
      "Epoch: [40][275/391]\tLoss 0.2129 (0.2408)\tPrec@1 92.188 (91.502)\n",
      "Epoch: [40][330/391]\tLoss 0.3048 (0.2452)\tPrec@1 87.500 (91.449)\n",
      "Epoch: [40][385/391]\tLoss 0.2698 (0.2488)\tPrec@1 90.625 (91.315)\n",
      "Test\t  Prec@1: 84.800 (Err: 15.200 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2236 (0.2236)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [41][55/391]\tLoss 0.3017 (0.2338)\tPrec@1 92.188 (91.950)\n",
      "Epoch: [41][110/391]\tLoss 0.3068 (0.2397)\tPrec@1 88.281 (91.512)\n",
      "Epoch: [41][165/391]\tLoss 0.2561 (0.2449)\tPrec@1 92.188 (91.307)\n",
      "Epoch: [41][220/391]\tLoss 0.2031 (0.2409)\tPrec@1 93.750 (91.484)\n",
      "Epoch: [41][275/391]\tLoss 0.3320 (0.2416)\tPrec@1 87.500 (91.460)\n",
      "Epoch: [41][330/391]\tLoss 0.2945 (0.2438)\tPrec@1 91.406 (91.409)\n",
      "Epoch: [41][385/391]\tLoss 0.2903 (0.2495)\tPrec@1 90.625 (91.250)\n",
      "Test\t  Prec@1: 85.910 (Err: 14.090 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2072 (0.2072)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [42][55/391]\tLoss 0.3465 (0.2213)\tPrec@1 89.844 (92.550)\n",
      "Epoch: [42][110/391]\tLoss 0.1676 (0.2217)\tPrec@1 94.531 (92.490)\n",
      "Epoch: [42][165/391]\tLoss 0.2569 (0.2321)\tPrec@1 89.844 (92.037)\n",
      "Epoch: [42][220/391]\tLoss 0.2057 (0.2426)\tPrec@1 92.969 (91.678)\n",
      "Epoch: [42][275/391]\tLoss 0.1595 (0.2413)\tPrec@1 92.188 (91.689)\n",
      "Epoch: [42][330/391]\tLoss 0.2607 (0.2432)\tPrec@1 89.062 (91.548)\n",
      "Epoch: [42][385/391]\tLoss 0.2274 (0.2464)\tPrec@1 93.750 (91.410)\n",
      "Test\t  Prec@1: 84.680 (Err: 15.320 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.2664 (0.2664)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [43][55/391]\tLoss 0.2508 (0.2341)\tPrec@1 89.844 (91.867)\n",
      "Epoch: [43][110/391]\tLoss 0.1900 (0.2355)\tPrec@1 93.750 (91.857)\n",
      "Epoch: [43][165/391]\tLoss 0.3290 (0.2349)\tPrec@1 89.062 (91.783)\n",
      "Epoch: [43][220/391]\tLoss 0.1675 (0.2400)\tPrec@1 92.969 (91.632)\n",
      "Epoch: [43][275/391]\tLoss 0.1968 (0.2444)\tPrec@1 92.969 (91.449)\n",
      "Epoch: [43][330/391]\tLoss 0.1969 (0.2443)\tPrec@1 93.750 (91.449)\n",
      "Epoch: [43][385/391]\tLoss 0.2923 (0.2440)\tPrec@1 89.844 (91.475)\n",
      "Test\t  Prec@1: 86.380 (Err: 13.620 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2677 (0.2677)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [44][55/391]\tLoss 0.1824 (0.2211)\tPrec@1 92.969 (92.118)\n",
      "Epoch: [44][110/391]\tLoss 0.2484 (0.2271)\tPrec@1 92.188 (92.033)\n",
      "Epoch: [44][165/391]\tLoss 0.3183 (0.2266)\tPrec@1 87.500 (92.051)\n",
      "Epoch: [44][220/391]\tLoss 0.2638 (0.2284)\tPrec@1 93.750 (91.972)\n",
      "Epoch: [44][275/391]\tLoss 0.2220 (0.2309)\tPrec@1 93.750 (91.927)\n",
      "Epoch: [44][330/391]\tLoss 0.2546 (0.2311)\tPrec@1 91.406 (91.897)\n",
      "Epoch: [44][385/391]\tLoss 0.2876 (0.2368)\tPrec@1 89.062 (91.698)\n",
      "Test\t  Prec@1: 81.620 (Err: 18.380 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.2217 (0.2217)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [45][55/391]\tLoss 0.1266 (0.2225)\tPrec@1 95.312 (92.076)\n",
      "Epoch: [45][110/391]\tLoss 0.2137 (0.2326)\tPrec@1 92.969 (91.695)\n",
      "Epoch: [45][165/391]\tLoss 0.1988 (0.2343)\tPrec@1 92.969 (91.656)\n",
      "Epoch: [45][220/391]\tLoss 0.3000 (0.2323)\tPrec@1 88.281 (91.717)\n",
      "Epoch: [45][275/391]\tLoss 0.2673 (0.2360)\tPrec@1 89.062 (91.599)\n",
      "Epoch: [45][330/391]\tLoss 0.2324 (0.2388)\tPrec@1 91.406 (91.522)\n",
      "Epoch: [45][385/391]\tLoss 0.2630 (0.2423)\tPrec@1 90.625 (91.408)\n",
      "Test\t  Prec@1: 86.890 (Err: 13.110 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.1203 (0.1203)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [46][55/391]\tLoss 0.1982 (0.2355)\tPrec@1 96.094 (91.992)\n",
      "Epoch: [46][110/391]\tLoss 0.2415 (0.2286)\tPrec@1 91.406 (91.927)\n",
      "Epoch: [46][165/391]\tLoss 0.3533 (0.2346)\tPrec@1 88.281 (91.675)\n",
      "Epoch: [46][220/391]\tLoss 0.2089 (0.2379)\tPrec@1 93.750 (91.558)\n",
      "Epoch: [46][275/391]\tLoss 0.1431 (0.2381)\tPrec@1 94.531 (91.585)\n",
      "Epoch: [46][330/391]\tLoss 0.2135 (0.2402)\tPrec@1 92.969 (91.503)\n",
      "Epoch: [46][385/391]\tLoss 0.3752 (0.2408)\tPrec@1 87.500 (91.469)\n",
      "Test\t  Prec@1: 84.600 (Err: 15.400 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.1969 (0.1969)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [47][55/391]\tLoss 0.1866 (0.2273)\tPrec@1 92.188 (92.229)\n",
      "Epoch: [47][110/391]\tLoss 0.2881 (0.2145)\tPrec@1 87.500 (92.575)\n",
      "Epoch: [47][165/391]\tLoss 0.2783 (0.2113)\tPrec@1 88.281 (92.578)\n",
      "Epoch: [47][220/391]\tLoss 0.2750 (0.2172)\tPrec@1 92.188 (92.431)\n",
      "Epoch: [47][275/391]\tLoss 0.1707 (0.2247)\tPrec@1 94.531 (92.159)\n",
      "Epoch: [47][330/391]\tLoss 0.2911 (0.2291)\tPrec@1 91.406 (91.982)\n",
      "Epoch: [47][385/391]\tLoss 0.3029 (0.2347)\tPrec@1 89.062 (91.744)\n",
      "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1869 (0.1869)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [48][55/391]\tLoss 0.2704 (0.2177)\tPrec@1 90.625 (92.467)\n",
      "Epoch: [48][110/391]\tLoss 0.3262 (0.2244)\tPrec@1 88.281 (92.216)\n",
      "Epoch: [48][165/391]\tLoss 0.2648 (0.2283)\tPrec@1 90.625 (92.046)\n",
      "Epoch: [48][220/391]\tLoss 0.1558 (0.2293)\tPrec@1 94.531 (92.011)\n",
      "Epoch: [48][275/391]\tLoss 0.2287 (0.2323)\tPrec@1 92.969 (91.921)\n",
      "Epoch: [48][330/391]\tLoss 0.1961 (0.2329)\tPrec@1 89.062 (91.859)\n",
      "Epoch: [48][385/391]\tLoss 0.2742 (0.2326)\tPrec@1 89.062 (91.892)\n",
      "Test\t  Prec@1: 86.020 (Err: 13.980 )\n",
      "\n",
      "Training resnet32 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.1959 (0.1959)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [49][55/391]\tLoss 0.1866 (0.1995)\tPrec@1 90.625 (93.164)\n",
      "Epoch: [49][110/391]\tLoss 0.1301 (0.2152)\tPrec@1 96.094 (92.469)\n",
      "Epoch: [49][165/391]\tLoss 0.2097 (0.2207)\tPrec@1 92.188 (92.376)\n",
      "Epoch: [49][220/391]\tLoss 0.1557 (0.2272)\tPrec@1 94.531 (92.173)\n",
      "Epoch: [49][275/391]\tLoss 0.2441 (0.2266)\tPrec@1 89.844 (92.193)\n",
      "Epoch: [49][330/391]\tLoss 0.2388 (0.2257)\tPrec@1 92.188 (92.225)\n",
      "Epoch: [49][385/391]\tLoss 0.2812 (0.2265)\tPrec@1 92.188 (92.198)\n",
      "Test\t  Prec@1: 86.660 (Err: 13.340 )\n",
      "\n",
      "The lowest error from resnet32 model after 50 epochs is 13.070\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet32';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-28                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-30             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-33                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-35             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-38                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-40            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-43                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-45             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-50             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-11                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-53                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-55             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-12                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-58                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-60             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-13                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-63                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-65             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-14                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-68                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-70             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-15                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-73                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-75            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-16                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-76                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-77            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-78                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-79            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-80             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-17                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-81                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-82            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-83                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-84            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-85             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-18                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-86                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-87            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-88                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-89            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-90             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-19                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-91                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-92            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-93                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-94            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-95             [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-20                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-96                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-97            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-98                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-99            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-100            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-21                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-101                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-102           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-103                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-104           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-105            [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 658,586\n",
      "Trainable params: 658,586\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 98.49\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.38\n",
      "Params size (MB): 2.51\n",
      "Estimated Total Size (MB): 8.90\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet44',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 4.8021 (4.8021)\tPrec@1 10.156 (10.156)\n",
      "Epoch: [0][55/391]\tLoss 2.2239 (2.8684)\tPrec@1 10.156 (13.742)\n",
      "Epoch: [0][110/391]\tLoss 2.0390 (2.4984)\tPrec@1 23.438 (16.990)\n",
      "Epoch: [0][165/391]\tLoss 1.9911 (2.3236)\tPrec@1 24.219 (19.658)\n",
      "Epoch: [0][220/391]\tLoss 1.7376 (2.2007)\tPrec@1 32.812 (22.607)\n",
      "Epoch: [0][275/391]\tLoss 1.8931 (2.1164)\tPrec@1 32.031 (24.791)\n",
      "Epoch: [0][330/391]\tLoss 1.7547 (2.0475)\tPrec@1 32.812 (26.770)\n",
      "Epoch: [0][385/391]\tLoss 1.7116 (1.9911)\tPrec@1 43.750 (28.366)\n",
      "Test\t  Prec@1: 42.030 (Err: 57.970 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.6105 (1.6105)\tPrec@1 34.375 (34.375)\n",
      "Epoch: [1][55/391]\tLoss 1.5858 (1.5602)\tPrec@1 42.188 (42.983)\n",
      "Epoch: [1][110/391]\tLoss 1.3609 (1.5442)\tPrec@1 53.906 (43.117)\n",
      "Epoch: [1][165/391]\tLoss 1.3607 (1.5230)\tPrec@1 53.906 (43.901)\n",
      "Epoch: [1][220/391]\tLoss 1.3761 (1.4953)\tPrec@1 49.219 (45.065)\n",
      "Epoch: [1][275/391]\tLoss 1.0935 (1.4653)\tPrec@1 61.719 (46.224)\n",
      "Epoch: [1][330/391]\tLoss 1.3556 (1.4402)\tPrec@1 53.125 (47.364)\n",
      "Epoch: [1][385/391]\tLoss 1.3262 (1.4182)\tPrec@1 50.000 (48.106)\n",
      "Test\t  Prec@1: 53.790 (Err: 46.210 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.4915 (1.4915)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [2][55/391]\tLoss 1.2442 (1.2504)\tPrec@1 57.031 (54.590)\n",
      "Epoch: [2][110/391]\tLoss 0.9827 (1.2131)\tPrec@1 65.625 (56.018)\n",
      "Epoch: [2][165/391]\tLoss 1.1296 (1.1903)\tPrec@1 57.812 (57.271)\n",
      "Epoch: [2][220/391]\tLoss 0.9224 (1.1770)\tPrec@1 71.875 (57.770)\n",
      "Epoch: [2][275/391]\tLoss 1.0451 (1.1586)\tPrec@1 66.406 (58.387)\n",
      "Epoch: [2][330/391]\tLoss 0.9998 (1.1420)\tPrec@1 67.188 (58.948)\n",
      "Epoch: [2][385/391]\tLoss 1.2194 (1.1268)\tPrec@1 56.250 (59.537)\n",
      "Test\t  Prec@1: 62.230 (Err: 37.770 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.0802 (1.0802)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [3][55/391]\tLoss 0.8908 (0.9834)\tPrec@1 67.969 (64.955)\n",
      "Epoch: [3][110/391]\tLoss 0.9630 (0.9604)\tPrec@1 67.188 (66.019)\n",
      "Epoch: [3][165/391]\tLoss 0.8075 (0.9567)\tPrec@1 70.312 (66.171)\n",
      "Epoch: [3][220/391]\tLoss 0.9327 (0.9507)\tPrec@1 63.281 (66.374)\n",
      "Epoch: [3][275/391]\tLoss 0.6512 (0.9298)\tPrec@1 77.344 (67.105)\n",
      "Epoch: [3][330/391]\tLoss 0.9026 (0.9208)\tPrec@1 69.531 (67.405)\n",
      "Epoch: [3][385/391]\tLoss 0.8253 (0.9072)\tPrec@1 75.000 (67.946)\n",
      "Test\t  Prec@1: 70.500 (Err: 29.500 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.8535 (0.8535)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [4][55/391]\tLoss 0.7412 (0.8112)\tPrec@1 71.094 (71.680)\n",
      "Epoch: [4][110/391]\tLoss 0.8383 (0.7939)\tPrec@1 70.312 (72.206)\n",
      "Epoch: [4][165/391]\tLoss 0.8803 (0.7902)\tPrec@1 68.750 (72.426)\n",
      "Epoch: [4][220/391]\tLoss 0.9111 (0.7840)\tPrec@1 65.625 (72.688)\n",
      "Epoch: [4][275/391]\tLoss 0.7084 (0.7801)\tPrec@1 74.219 (72.781)\n",
      "Epoch: [4][330/391]\tLoss 0.7545 (0.7756)\tPrec@1 73.438 (72.793)\n",
      "Epoch: [4][385/391]\tLoss 0.7549 (0.7705)\tPrec@1 72.656 (73.079)\n",
      "Test\t  Prec@1: 73.470 (Err: 26.530 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.6686 (0.6686)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [5][55/391]\tLoss 0.7871 (0.7242)\tPrec@1 71.094 (74.400)\n",
      "Epoch: [5][110/391]\tLoss 0.8794 (0.7072)\tPrec@1 66.406 (75.127)\n",
      "Epoch: [5][165/391]\tLoss 0.6950 (0.7021)\tPrec@1 76.562 (75.513)\n",
      "Epoch: [5][220/391]\tLoss 0.7191 (0.6984)\tPrec@1 71.875 (75.629)\n",
      "Epoch: [5][275/391]\tLoss 0.7643 (0.6994)\tPrec@1 71.875 (75.620)\n",
      "Epoch: [5][330/391]\tLoss 0.6645 (0.6944)\tPrec@1 72.656 (75.810)\n",
      "Epoch: [5][385/391]\tLoss 0.7480 (0.6909)\tPrec@1 74.219 (75.976)\n",
      "Test\t  Prec@1: 73.700 (Err: 26.300 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.5433 (0.5433)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [6][55/391]\tLoss 0.7246 (0.6331)\tPrec@1 74.219 (78.041)\n",
      "Epoch: [6][110/391]\tLoss 0.7662 (0.6280)\tPrec@1 69.531 (78.026)\n",
      "Epoch: [6][165/391]\tLoss 0.7491 (0.6269)\tPrec@1 70.312 (78.003)\n",
      "Epoch: [6][220/391]\tLoss 0.5569 (0.6215)\tPrec@1 80.469 (78.235)\n",
      "Epoch: [6][275/391]\tLoss 0.5248 (0.6180)\tPrec@1 82.031 (78.397)\n",
      "Epoch: [6][330/391]\tLoss 0.6027 (0.6140)\tPrec@1 79.688 (78.533)\n",
      "Epoch: [6][385/391]\tLoss 0.6268 (0.6122)\tPrec@1 78.125 (78.647)\n",
      "Test\t  Prec@1: 74.810 (Err: 25.190 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.7311 (0.7311)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [7][55/391]\tLoss 0.5391 (0.5401)\tPrec@1 80.469 (81.445)\n",
      "Epoch: [7][110/391]\tLoss 0.6313 (0.5671)\tPrec@1 78.125 (80.384)\n",
      "Epoch: [7][165/391]\tLoss 0.6267 (0.5660)\tPrec@1 76.562 (80.243)\n",
      "Epoch: [7][220/391]\tLoss 0.6873 (0.5626)\tPrec@1 69.531 (80.380)\n",
      "Epoch: [7][275/391]\tLoss 0.5607 (0.5643)\tPrec@1 79.688 (80.412)\n",
      "Epoch: [7][330/391]\tLoss 0.5933 (0.5607)\tPrec@1 77.344 (80.599)\n",
      "Epoch: [7][385/391]\tLoss 0.5147 (0.5607)\tPrec@1 84.375 (80.594)\n",
      "Test\t  Prec@1: 78.540 (Err: 21.460 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5235 (0.5235)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [8][55/391]\tLoss 0.5453 (0.5286)\tPrec@1 82.812 (81.627)\n",
      "Epoch: [8][110/391]\tLoss 0.5270 (0.5246)\tPrec@1 82.031 (81.827)\n",
      "Epoch: [8][165/391]\tLoss 0.6518 (0.5237)\tPrec@1 77.344 (81.909)\n",
      "Epoch: [8][220/391]\tLoss 0.4718 (0.5213)\tPrec@1 78.906 (81.943)\n",
      "Epoch: [8][275/391]\tLoss 0.5061 (0.5195)\tPrec@1 82.812 (82.003)\n",
      "Epoch: [8][330/391]\tLoss 0.4842 (0.5184)\tPrec@1 83.594 (82.017)\n",
      "Epoch: [8][385/391]\tLoss 0.5547 (0.5174)\tPrec@1 80.469 (82.068)\n",
      "Test\t  Prec@1: 81.370 (Err: 18.630 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.4233 (0.4233)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [9][55/391]\tLoss 0.5588 (0.4997)\tPrec@1 76.562 (82.617)\n",
      "Epoch: [9][110/391]\tLoss 0.5436 (0.4884)\tPrec@1 78.906 (82.932)\n",
      "Epoch: [9][165/391]\tLoss 0.4423 (0.4822)\tPrec@1 86.719 (83.180)\n",
      "Epoch: [9][220/391]\tLoss 0.5535 (0.4843)\tPrec@1 82.031 (83.063)\n",
      "Epoch: [9][275/391]\tLoss 0.4431 (0.4879)\tPrec@1 85.156 (83.039)\n",
      "Epoch: [9][330/391]\tLoss 0.5799 (0.4904)\tPrec@1 78.125 (83.027)\n",
      "Epoch: [9][385/391]\tLoss 0.3519 (0.4886)\tPrec@1 89.062 (83.140)\n",
      "Test\t  Prec@1: 81.450 (Err: 18.550 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4608 (0.4608)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [10][55/391]\tLoss 0.5817 (0.4682)\tPrec@1 77.344 (83.315)\n",
      "Epoch: [10][110/391]\tLoss 0.4547 (0.4591)\tPrec@1 83.594 (83.685)\n",
      "Epoch: [10][165/391]\tLoss 0.4956 (0.4504)\tPrec@1 79.688 (83.989)\n",
      "Epoch: [10][220/391]\tLoss 0.3983 (0.4540)\tPrec@1 89.062 (83.933)\n",
      "Epoch: [10][275/391]\tLoss 0.4408 (0.4534)\tPrec@1 85.938 (84.041)\n",
      "Epoch: [10][330/391]\tLoss 0.5211 (0.4574)\tPrec@1 84.375 (83.964)\n",
      "Epoch: [10][385/391]\tLoss 0.5707 (0.4580)\tPrec@1 78.125 (83.912)\n",
      "Test\t  Prec@1: 80.640 (Err: 19.360 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.4506 (0.4506)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [11][55/391]\tLoss 0.3336 (0.4238)\tPrec@1 89.062 (85.686)\n",
      "Epoch: [11][110/391]\tLoss 0.3340 (0.4290)\tPrec@1 84.375 (85.353)\n",
      "Epoch: [11][165/391]\tLoss 0.5251 (0.4300)\tPrec@1 79.688 (85.137)\n",
      "Epoch: [11][220/391]\tLoss 0.3868 (0.4309)\tPrec@1 86.719 (85.177)\n",
      "Epoch: [11][275/391]\tLoss 0.4737 (0.4323)\tPrec@1 85.156 (85.148)\n",
      "Epoch: [11][330/391]\tLoss 0.5950 (0.4342)\tPrec@1 75.000 (84.975)\n",
      "Epoch: [11][385/391]\tLoss 0.5086 (0.4347)\tPrec@1 83.594 (85.013)\n",
      "Test\t  Prec@1: 80.610 (Err: 19.390 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.5126 (0.5126)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [12][55/391]\tLoss 0.3385 (0.4178)\tPrec@1 90.625 (85.212)\n",
      "Epoch: [12][110/391]\tLoss 0.6274 (0.4199)\tPrec@1 78.906 (85.262)\n",
      "Epoch: [12][165/391]\tLoss 0.2857 (0.4141)\tPrec@1 91.406 (85.608)\n",
      "Epoch: [12][220/391]\tLoss 0.5249 (0.4140)\tPrec@1 80.469 (85.517)\n",
      "Epoch: [12][275/391]\tLoss 0.4175 (0.4197)\tPrec@1 85.156 (85.360)\n",
      "Epoch: [12][330/391]\tLoss 0.4270 (0.4184)\tPrec@1 84.375 (85.310)\n",
      "Epoch: [12][385/391]\tLoss 0.4360 (0.4194)\tPrec@1 82.031 (85.286)\n",
      "Test\t  Prec@1: 80.060 (Err: 19.940 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.3116 (0.3116)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [13][55/391]\tLoss 0.5773 (0.4010)\tPrec@1 81.250 (85.770)\n",
      "Epoch: [13][110/391]\tLoss 0.2593 (0.4049)\tPrec@1 91.406 (86.022)\n",
      "Epoch: [13][165/391]\tLoss 0.4111 (0.4052)\tPrec@1 83.594 (85.895)\n",
      "Epoch: [13][220/391]\tLoss 0.4351 (0.4061)\tPrec@1 81.250 (85.874)\n",
      "Epoch: [13][275/391]\tLoss 0.3964 (0.4056)\tPrec@1 87.500 (85.966)\n",
      "Epoch: [13][330/391]\tLoss 0.3820 (0.4045)\tPrec@1 86.719 (85.947)\n",
      "Epoch: [13][385/391]\tLoss 0.4951 (0.4043)\tPrec@1 82.031 (85.929)\n",
      "Test\t  Prec@1: 82.630 (Err: 17.370 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.3894 (0.3894)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [14][55/391]\tLoss 0.3432 (0.3835)\tPrec@1 88.281 (86.761)\n",
      "Epoch: [14][110/391]\tLoss 0.3823 (0.3771)\tPrec@1 88.281 (87.050)\n",
      "Epoch: [14][165/391]\tLoss 0.3703 (0.3893)\tPrec@1 86.719 (86.568)\n",
      "Epoch: [14][220/391]\tLoss 0.2732 (0.3915)\tPrec@1 90.625 (86.425)\n",
      "Epoch: [14][275/391]\tLoss 0.3153 (0.3899)\tPrec@1 91.406 (86.439)\n",
      "Epoch: [14][330/391]\tLoss 0.4573 (0.3884)\tPrec@1 85.156 (86.502)\n",
      "Epoch: [14][385/391]\tLoss 0.5046 (0.3891)\tPrec@1 83.594 (86.460)\n",
      "Test\t  Prec@1: 79.540 (Err: 20.460 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3027 (0.3027)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [15][55/391]\tLoss 0.3996 (0.3671)\tPrec@1 85.938 (87.165)\n",
      "Epoch: [15][110/391]\tLoss 0.4883 (0.3672)\tPrec@1 85.156 (87.247)\n",
      "Epoch: [15][165/391]\tLoss 0.4018 (0.3715)\tPrec@1 85.938 (87.062)\n",
      "Epoch: [15][220/391]\tLoss 0.3642 (0.3729)\tPrec@1 87.500 (87.090)\n",
      "Epoch: [15][275/391]\tLoss 0.4025 (0.3734)\tPrec@1 84.375 (87.141)\n",
      "Epoch: [15][330/391]\tLoss 0.1942 (0.3741)\tPrec@1 95.312 (87.148)\n",
      "Epoch: [15][385/391]\tLoss 0.3381 (0.3736)\tPrec@1 89.062 (87.146)\n",
      "Test\t  Prec@1: 81.030 (Err: 18.970 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.2199 (0.2199)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [16][55/391]\tLoss 0.2792 (0.3240)\tPrec@1 89.062 (88.979)\n",
      "Epoch: [16][110/391]\tLoss 0.3957 (0.3441)\tPrec@1 88.281 (88.260)\n",
      "Epoch: [16][165/391]\tLoss 0.2582 (0.3560)\tPrec@1 90.625 (87.825)\n",
      "Epoch: [16][220/391]\tLoss 0.3526 (0.3570)\tPrec@1 88.281 (87.747)\n",
      "Epoch: [16][275/391]\tLoss 0.3897 (0.3591)\tPrec@1 86.719 (87.698)\n",
      "Epoch: [16][330/391]\tLoss 0.4239 (0.3565)\tPrec@1 86.719 (87.819)\n",
      "Epoch: [16][385/391]\tLoss 0.3861 (0.3575)\tPrec@1 85.938 (87.769)\n",
      "Test\t  Prec@1: 81.360 (Err: 18.640 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.2657 (0.2657)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [17][55/391]\tLoss 0.3795 (0.3202)\tPrec@1 85.156 (88.770)\n",
      "Epoch: [17][110/391]\tLoss 0.3120 (0.3371)\tPrec@1 91.406 (88.323)\n",
      "Epoch: [17][165/391]\tLoss 0.4242 (0.3425)\tPrec@1 80.469 (88.008)\n",
      "Epoch: [17][220/391]\tLoss 0.3201 (0.3395)\tPrec@1 86.719 (88.175)\n",
      "Epoch: [17][275/391]\tLoss 0.3389 (0.3436)\tPrec@1 89.844 (88.168)\n",
      "Epoch: [17][330/391]\tLoss 0.3331 (0.3474)\tPrec@1 89.844 (88.057)\n",
      "Epoch: [17][385/391]\tLoss 0.3086 (0.3484)\tPrec@1 88.281 (87.970)\n",
      "Test\t  Prec@1: 84.070 (Err: 15.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.4066 (0.4066)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [18][55/391]\tLoss 0.3136 (0.3357)\tPrec@1 89.844 (88.421)\n",
      "Epoch: [18][110/391]\tLoss 0.2989 (0.3249)\tPrec@1 89.844 (88.901)\n",
      "Epoch: [18][165/391]\tLoss 0.2688 (0.3247)\tPrec@1 89.062 (88.785)\n",
      "Epoch: [18][220/391]\tLoss 0.4799 (0.3288)\tPrec@1 85.156 (88.674)\n",
      "Epoch: [18][275/391]\tLoss 0.3537 (0.3345)\tPrec@1 89.062 (88.505)\n",
      "Epoch: [18][330/391]\tLoss 0.4821 (0.3365)\tPrec@1 88.281 (88.439)\n",
      "Epoch: [18][385/391]\tLoss 0.4160 (0.3374)\tPrec@1 83.594 (88.338)\n",
      "Test\t  Prec@1: 84.800 (Err: 15.200 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.2601 (0.2601)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [19][55/391]\tLoss 0.2843 (0.3034)\tPrec@1 86.719 (89.160)\n",
      "Epoch: [19][110/391]\tLoss 0.2480 (0.3098)\tPrec@1 93.750 (89.048)\n",
      "Epoch: [19][165/391]\tLoss 0.4476 (0.3202)\tPrec@1 85.156 (88.926)\n",
      "Epoch: [19][220/391]\tLoss 0.1577 (0.3227)\tPrec@1 96.094 (88.804)\n",
      "Epoch: [19][275/391]\tLoss 0.3102 (0.3257)\tPrec@1 85.156 (88.624)\n",
      "Epoch: [19][330/391]\tLoss 0.3695 (0.3298)\tPrec@1 86.719 (88.487)\n",
      "Epoch: [19][385/391]\tLoss 0.3079 (0.3300)\tPrec@1 89.844 (88.522)\n",
      "Test\t  Prec@1: 79.450 (Err: 20.550 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.3316 (0.3316)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [20][55/391]\tLoss 0.4719 (0.3041)\tPrec@1 84.375 (89.244)\n",
      "Epoch: [20][110/391]\tLoss 0.2588 (0.3079)\tPrec@1 89.844 (89.133)\n",
      "Epoch: [20][165/391]\tLoss 0.2833 (0.3183)\tPrec@1 89.062 (88.804)\n",
      "Epoch: [20][220/391]\tLoss 0.4932 (0.3226)\tPrec@1 83.594 (88.663)\n",
      "Epoch: [20][275/391]\tLoss 0.3337 (0.3231)\tPrec@1 86.719 (88.692)\n",
      "Epoch: [20][330/391]\tLoss 0.4314 (0.3227)\tPrec@1 85.156 (88.723)\n",
      "Epoch: [20][385/391]\tLoss 0.3884 (0.3246)\tPrec@1 88.281 (88.621)\n",
      "Test\t  Prec@1: 85.000 (Err: 15.000 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.1725 (0.1725)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [21][55/391]\tLoss 0.3708 (0.2979)\tPrec@1 87.500 (89.481)\n",
      "Epoch: [21][110/391]\tLoss 0.4284 (0.3139)\tPrec@1 88.281 (88.830)\n",
      "Epoch: [21][165/391]\tLoss 0.2997 (0.3112)\tPrec@1 89.844 (89.044)\n",
      "Epoch: [21][220/391]\tLoss 0.2982 (0.3097)\tPrec@1 89.844 (89.119)\n",
      "Epoch: [21][275/391]\tLoss 0.3204 (0.3118)\tPrec@1 90.625 (89.179)\n",
      "Epoch: [21][330/391]\tLoss 0.3373 (0.3099)\tPrec@1 90.625 (89.230)\n",
      "Epoch: [21][385/391]\tLoss 0.2268 (0.3123)\tPrec@1 91.406 (89.139)\n",
      "Test\t  Prec@1: 83.120 (Err: 16.880 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.3568 (0.3568)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [22][55/391]\tLoss 0.3883 (0.2950)\tPrec@1 84.375 (89.676)\n",
      "Epoch: [22][110/391]\tLoss 0.2757 (0.2923)\tPrec@1 89.844 (89.661)\n",
      "Epoch: [22][165/391]\tLoss 0.2516 (0.2970)\tPrec@1 89.844 (89.571)\n",
      "Epoch: [22][220/391]\tLoss 0.3849 (0.3041)\tPrec@1 88.281 (89.292)\n",
      "Epoch: [22][275/391]\tLoss 0.3474 (0.3052)\tPrec@1 86.719 (89.210)\n",
      "Epoch: [22][330/391]\tLoss 0.3294 (0.3104)\tPrec@1 91.406 (89.088)\n",
      "Epoch: [22][385/391]\tLoss 0.3598 (0.3115)\tPrec@1 91.406 (89.081)\n",
      "Test\t  Prec@1: 84.990 (Err: 15.010 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.3950 (0.3950)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [23][55/391]\tLoss 0.2907 (0.2898)\tPrec@1 88.281 (89.872)\n",
      "Epoch: [23][110/391]\tLoss 0.2205 (0.2875)\tPrec@1 90.625 (89.963)\n",
      "Epoch: [23][165/391]\tLoss 0.2010 (0.2897)\tPrec@1 90.625 (89.811)\n",
      "Epoch: [23][220/391]\tLoss 0.2769 (0.2929)\tPrec@1 89.844 (89.688)\n",
      "Epoch: [23][275/391]\tLoss 0.2351 (0.2972)\tPrec@1 91.406 (89.583)\n",
      "Epoch: [23][330/391]\tLoss 0.3049 (0.2990)\tPrec@1 88.281 (89.572)\n",
      "Epoch: [23][385/391]\tLoss 0.3412 (0.3014)\tPrec@1 89.062 (89.455)\n",
      "Test\t  Prec@1: 82.050 (Err: 17.950 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.2123 (0.2123)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [24][55/391]\tLoss 0.2666 (0.2924)\tPrec@1 90.625 (89.565)\n",
      "Epoch: [24][110/391]\tLoss 0.2222 (0.2868)\tPrec@1 92.969 (89.921)\n",
      "Epoch: [24][165/391]\tLoss 0.2939 (0.2858)\tPrec@1 89.062 (89.980)\n",
      "Epoch: [24][220/391]\tLoss 0.3260 (0.2861)\tPrec@1 89.062 (90.017)\n",
      "Epoch: [24][275/391]\tLoss 0.3211 (0.2922)\tPrec@1 87.500 (89.878)\n",
      "Epoch: [24][330/391]\tLoss 0.2278 (0.2946)\tPrec@1 92.188 (89.815)\n",
      "Epoch: [24][385/391]\tLoss 0.3080 (0.2948)\tPrec@1 87.500 (89.755)\n",
      "Test\t  Prec@1: 84.470 (Err: 15.530 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2056 (0.2056)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [25][55/391]\tLoss 0.2470 (0.2588)\tPrec@1 90.625 (90.988)\n",
      "Epoch: [25][110/391]\tLoss 0.2613 (0.2676)\tPrec@1 89.844 (90.646)\n",
      "Epoch: [25][165/391]\tLoss 0.1907 (0.2776)\tPrec@1 96.875 (90.343)\n",
      "Epoch: [25][220/391]\tLoss 0.4183 (0.2823)\tPrec@1 85.156 (90.271)\n",
      "Epoch: [25][275/391]\tLoss 0.3540 (0.2878)\tPrec@1 92.188 (90.022)\n",
      "Epoch: [25][330/391]\tLoss 0.1268 (0.2877)\tPrec@1 96.094 (89.978)\n",
      "Epoch: [25][385/391]\tLoss 0.2304 (0.2888)\tPrec@1 93.750 (89.991)\n",
      "Test\t  Prec@1: 85.230 (Err: 14.770 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.2976 (0.2976)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [26][55/391]\tLoss 0.3066 (0.2701)\tPrec@1 88.281 (90.318)\n",
      "Epoch: [26][110/391]\tLoss 0.3079 (0.2808)\tPrec@1 87.500 (90.006)\n",
      "Epoch: [26][165/391]\tLoss 0.2415 (0.2845)\tPrec@1 90.625 (89.881)\n",
      "Epoch: [26][220/391]\tLoss 0.2244 (0.2840)\tPrec@1 92.969 (89.936)\n",
      "Epoch: [26][275/391]\tLoss 0.2755 (0.2836)\tPrec@1 89.844 (89.909)\n",
      "Epoch: [26][330/391]\tLoss 0.3433 (0.2840)\tPrec@1 89.062 (89.896)\n",
      "Epoch: [26][385/391]\tLoss 0.2669 (0.2826)\tPrec@1 89.844 (89.981)\n",
      "Test\t  Prec@1: 80.460 (Err: 19.540 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.3224 (0.3224)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [27][55/391]\tLoss 0.4115 (0.2591)\tPrec@1 86.719 (91.211)\n",
      "Epoch: [27][110/391]\tLoss 0.2445 (0.2596)\tPrec@1 91.406 (91.054)\n",
      "Epoch: [27][165/391]\tLoss 0.4451 (0.2652)\tPrec@1 86.719 (90.790)\n",
      "Epoch: [27][220/391]\tLoss 0.3531 (0.2698)\tPrec@1 87.500 (90.572)\n",
      "Epoch: [27][275/391]\tLoss 0.1972 (0.2762)\tPrec@1 93.750 (90.311)\n",
      "Epoch: [27][330/391]\tLoss 0.1779 (0.2769)\tPrec@1 96.094 (90.351)\n",
      "Epoch: [27][385/391]\tLoss 0.1913 (0.2779)\tPrec@1 94.531 (90.311)\n",
      "Test\t  Prec@1: 85.790 (Err: 14.210 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.1697 (0.1697)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [28][55/391]\tLoss 0.3470 (0.2477)\tPrec@1 89.062 (91.490)\n",
      "Epoch: [28][110/391]\tLoss 0.2453 (0.2603)\tPrec@1 90.625 (90.871)\n",
      "Epoch: [28][165/391]\tLoss 0.2751 (0.2655)\tPrec@1 89.844 (90.761)\n",
      "Epoch: [28][220/391]\tLoss 0.3246 (0.2683)\tPrec@1 85.938 (90.703)\n",
      "Epoch: [28][275/391]\tLoss 0.2554 (0.2735)\tPrec@1 92.188 (90.455)\n",
      "Epoch: [28][330/391]\tLoss 0.3975 (0.2743)\tPrec@1 89.844 (90.457)\n",
      "Epoch: [28][385/391]\tLoss 0.3156 (0.2768)\tPrec@1 86.719 (90.390)\n",
      "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2907 (0.2907)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [29][55/391]\tLoss 0.3011 (0.2662)\tPrec@1 92.188 (90.485)\n",
      "Epoch: [29][110/391]\tLoss 0.1683 (0.2713)\tPrec@1 92.969 (90.569)\n",
      "Epoch: [29][165/391]\tLoss 0.2370 (0.2646)\tPrec@1 93.750 (90.823)\n",
      "Epoch: [29][220/391]\tLoss 0.2395 (0.2689)\tPrec@1 92.969 (90.682)\n",
      "Epoch: [29][275/391]\tLoss 0.4194 (0.2728)\tPrec@1 84.375 (90.534)\n",
      "Epoch: [29][330/391]\tLoss 0.4264 (0.2718)\tPrec@1 83.594 (90.599)\n",
      "Epoch: [29][385/391]\tLoss 0.2281 (0.2703)\tPrec@1 94.531 (90.659)\n",
      "Test\t  Prec@1: 86.000 (Err: 14.000 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2062 (0.2062)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [30][55/391]\tLoss 0.3552 (0.2437)\tPrec@1 87.500 (91.727)\n",
      "Epoch: [30][110/391]\tLoss 0.2147 (0.2475)\tPrec@1 92.969 (91.441)\n",
      "Epoch: [30][165/391]\tLoss 0.2338 (0.2622)\tPrec@1 92.969 (90.898)\n",
      "Epoch: [30][220/391]\tLoss 0.2197 (0.2621)\tPrec@1 92.969 (90.908)\n",
      "Epoch: [30][275/391]\tLoss 0.1739 (0.2616)\tPrec@1 92.969 (90.953)\n",
      "Epoch: [30][330/391]\tLoss 0.2287 (0.2632)\tPrec@1 92.188 (90.896)\n",
      "Epoch: [30][385/391]\tLoss 0.2275 (0.2655)\tPrec@1 92.188 (90.819)\n",
      "Test\t  Prec@1: 86.530 (Err: 13.470 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.1713 (0.1713)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [31][55/391]\tLoss 0.2623 (0.2426)\tPrec@1 92.188 (91.769)\n",
      "Epoch: [31][110/391]\tLoss 0.3087 (0.2499)\tPrec@1 89.844 (91.596)\n",
      "Epoch: [31][165/391]\tLoss 0.2201 (0.2459)\tPrec@1 91.406 (91.524)\n",
      "Epoch: [31][220/391]\tLoss 0.2664 (0.2497)\tPrec@1 92.188 (91.321)\n",
      "Epoch: [31][275/391]\tLoss 0.2016 (0.2520)\tPrec@1 93.750 (91.205)\n",
      "Epoch: [31][330/391]\tLoss 0.2495 (0.2553)\tPrec@1 89.062 (91.099)\n",
      "Epoch: [31][385/391]\tLoss 0.2895 (0.2573)\tPrec@1 89.062 (91.050)\n",
      "Test\t  Prec@1: 84.710 (Err: 15.290 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.1623 (0.1623)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [32][55/391]\tLoss 0.4153 (0.2453)\tPrec@1 86.719 (91.085)\n",
      "Epoch: [32][110/391]\tLoss 0.2244 (0.2545)\tPrec@1 92.969 (90.963)\n",
      "Epoch: [32][165/391]\tLoss 0.3342 (0.2597)\tPrec@1 86.719 (90.841)\n",
      "Epoch: [32][220/391]\tLoss 0.2424 (0.2580)\tPrec@1 89.062 (90.844)\n",
      "Epoch: [32][275/391]\tLoss 0.2632 (0.2539)\tPrec@1 89.062 (91.004)\n",
      "Epoch: [32][330/391]\tLoss 0.2328 (0.2552)\tPrec@1 93.750 (91.005)\n",
      "Epoch: [32][385/391]\tLoss 0.3124 (0.2572)\tPrec@1 89.844 (90.983)\n",
      "Test\t  Prec@1: 86.070 (Err: 13.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.3594 (0.3594)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [33][55/391]\tLoss 0.2133 (0.2389)\tPrec@1 92.969 (91.671)\n",
      "Epoch: [33][110/391]\tLoss 0.1693 (0.2237)\tPrec@1 95.312 (92.131)\n",
      "Epoch: [33][165/391]\tLoss 0.2641 (0.2352)\tPrec@1 91.406 (91.802)\n",
      "Epoch: [33][220/391]\tLoss 0.2256 (0.2418)\tPrec@1 91.406 (91.572)\n",
      "Epoch: [33][275/391]\tLoss 0.3876 (0.2458)\tPrec@1 88.281 (91.389)\n",
      "Epoch: [33][330/391]\tLoss 0.1995 (0.2514)\tPrec@1 92.969 (91.201)\n",
      "Epoch: [33][385/391]\tLoss 0.1904 (0.2554)\tPrec@1 94.531 (91.036)\n",
      "Test\t  Prec@1: 86.020 (Err: 13.980 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.2659 (0.2659)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [34][55/391]\tLoss 0.2136 (0.2513)\tPrec@1 90.625 (91.295)\n",
      "Epoch: [34][110/391]\tLoss 0.3586 (0.2478)\tPrec@1 88.281 (91.434)\n",
      "Epoch: [34][165/391]\tLoss 0.2775 (0.2409)\tPrec@1 92.188 (91.698)\n",
      "Epoch: [34][220/391]\tLoss 0.2725 (0.2374)\tPrec@1 92.188 (91.753)\n",
      "Epoch: [34][275/391]\tLoss 0.2363 (0.2395)\tPrec@1 92.969 (91.667)\n",
      "Epoch: [34][330/391]\tLoss 0.3035 (0.2416)\tPrec@1 90.625 (91.579)\n",
      "Epoch: [34][385/391]\tLoss 0.3244 (0.2449)\tPrec@1 89.062 (91.487)\n",
      "Test\t  Prec@1: 86.260 (Err: 13.740 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.2251 (0.2251)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [35][55/391]\tLoss 0.3635 (0.2326)\tPrec@1 86.719 (91.713)\n",
      "Epoch: [35][110/391]\tLoss 0.2393 (0.2391)\tPrec@1 92.188 (91.695)\n",
      "Epoch: [35][165/391]\tLoss 0.1555 (0.2372)\tPrec@1 96.094 (91.787)\n",
      "Epoch: [35][220/391]\tLoss 0.2513 (0.2341)\tPrec@1 89.062 (91.894)\n",
      "Epoch: [35][275/391]\tLoss 0.2738 (0.2357)\tPrec@1 91.406 (91.800)\n",
      "Epoch: [35][330/391]\tLoss 0.2603 (0.2365)\tPrec@1 89.844 (91.796)\n",
      "Epoch: [35][385/391]\tLoss 0.2364 (0.2381)\tPrec@1 91.406 (91.708)\n",
      "Test\t  Prec@1: 84.290 (Err: 15.710 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.2051 (0.2051)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [36][55/391]\tLoss 0.1439 (0.2386)\tPrec@1 94.531 (92.006)\n",
      "Epoch: [36][110/391]\tLoss 0.3080 (0.2378)\tPrec@1 92.188 (91.864)\n",
      "Epoch: [36][165/391]\tLoss 0.3144 (0.2431)\tPrec@1 91.406 (91.510)\n",
      "Epoch: [36][220/391]\tLoss 0.2279 (0.2467)\tPrec@1 91.406 (91.389)\n",
      "Epoch: [36][275/391]\tLoss 0.2385 (0.2450)\tPrec@1 92.969 (91.429)\n",
      "Epoch: [36][330/391]\tLoss 0.1760 (0.2445)\tPrec@1 94.531 (91.376)\n",
      "Epoch: [36][385/391]\tLoss 0.3383 (0.2459)\tPrec@1 87.500 (91.299)\n",
      "Test\t  Prec@1: 85.370 (Err: 14.630 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.1299 (0.1299)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [37][55/391]\tLoss 0.2616 (0.2399)\tPrec@1 90.625 (91.546)\n",
      "Epoch: [37][110/391]\tLoss 0.3041 (0.2381)\tPrec@1 89.062 (91.681)\n",
      "Epoch: [37][165/391]\tLoss 0.1687 (0.2369)\tPrec@1 96.094 (91.722)\n",
      "Epoch: [37][220/391]\tLoss 0.2209 (0.2371)\tPrec@1 93.750 (91.629)\n",
      "Epoch: [37][275/391]\tLoss 0.1329 (0.2368)\tPrec@1 93.750 (91.590)\n",
      "Epoch: [37][330/391]\tLoss 0.2459 (0.2396)\tPrec@1 91.406 (91.517)\n",
      "Epoch: [37][385/391]\tLoss 0.2636 (0.2413)\tPrec@1 92.188 (91.491)\n",
      "Test\t  Prec@1: 85.580 (Err: 14.420 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.2059 (0.2059)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [38][55/391]\tLoss 0.1621 (0.2153)\tPrec@1 95.312 (92.327)\n",
      "Epoch: [38][110/391]\tLoss 0.2643 (0.2211)\tPrec@1 91.406 (92.244)\n",
      "Epoch: [38][165/391]\tLoss 0.2850 (0.2287)\tPrec@1 90.625 (91.919)\n",
      "Epoch: [38][220/391]\tLoss 0.1281 (0.2306)\tPrec@1 96.094 (91.947)\n",
      "Epoch: [38][275/391]\tLoss 0.2961 (0.2340)\tPrec@1 89.062 (91.803)\n",
      "Epoch: [38][330/391]\tLoss 0.2649 (0.2379)\tPrec@1 89.062 (91.699)\n",
      "Epoch: [38][385/391]\tLoss 0.2358 (0.2396)\tPrec@1 90.625 (91.645)\n",
      "Test\t  Prec@1: 87.340 (Err: 12.660 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.2283 (0.2283)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [39][55/391]\tLoss 0.2587 (0.2251)\tPrec@1 92.969 (92.104)\n",
      "Epoch: [39][110/391]\tLoss 0.1646 (0.2244)\tPrec@1 92.969 (92.188)\n",
      "Epoch: [39][165/391]\tLoss 0.2598 (0.2245)\tPrec@1 92.188 (92.084)\n",
      "Epoch: [39][220/391]\tLoss 0.3051 (0.2290)\tPrec@1 89.844 (91.933)\n",
      "Epoch: [39][275/391]\tLoss 0.1799 (0.2344)\tPrec@1 93.750 (91.794)\n",
      "Epoch: [39][330/391]\tLoss 0.2767 (0.2363)\tPrec@1 89.844 (91.687)\n",
      "Epoch: [39][385/391]\tLoss 0.2726 (0.2394)\tPrec@1 91.406 (91.558)\n",
      "Test\t  Prec@1: 85.720 (Err: 14.280 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.1427 (0.1427)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [40][55/391]\tLoss 0.3350 (0.2256)\tPrec@1 87.500 (91.895)\n",
      "Epoch: [40][110/391]\tLoss 0.1772 (0.2256)\tPrec@1 92.188 (92.026)\n",
      "Epoch: [40][165/391]\tLoss 0.2331 (0.2289)\tPrec@1 93.750 (91.891)\n",
      "Epoch: [40][220/391]\tLoss 0.1391 (0.2284)\tPrec@1 95.312 (91.958)\n",
      "Epoch: [40][275/391]\tLoss 0.2026 (0.2291)\tPrec@1 94.531 (91.921)\n",
      "Epoch: [40][330/391]\tLoss 0.2572 (0.2305)\tPrec@1 90.625 (91.831)\n",
      "Epoch: [40][385/391]\tLoss 0.2496 (0.2337)\tPrec@1 91.406 (91.781)\n",
      "Test\t  Prec@1: 87.040 (Err: 12.960 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2934 (0.2934)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [41][55/391]\tLoss 0.2125 (0.2171)\tPrec@1 92.969 (92.439)\n",
      "Epoch: [41][110/391]\tLoss 0.2572 (0.2137)\tPrec@1 88.281 (92.420)\n",
      "Epoch: [41][165/391]\tLoss 0.1859 (0.2130)\tPrec@1 94.531 (92.564)\n",
      "Epoch: [41][220/391]\tLoss 0.2931 (0.2195)\tPrec@1 89.062 (92.301)\n",
      "Epoch: [41][275/391]\tLoss 0.1471 (0.2216)\tPrec@1 95.312 (92.250)\n",
      "Epoch: [41][330/391]\tLoss 0.1781 (0.2253)\tPrec@1 93.750 (92.133)\n",
      "Epoch: [41][385/391]\tLoss 0.2158 (0.2299)\tPrec@1 94.531 (91.999)\n",
      "Test\t  Prec@1: 84.400 (Err: 15.600 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2071 (0.2071)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [42][55/391]\tLoss 0.2993 (0.2290)\tPrec@1 91.406 (92.243)\n",
      "Epoch: [42][110/391]\tLoss 0.1396 (0.2230)\tPrec@1 96.094 (92.237)\n",
      "Epoch: [42][165/391]\tLoss 0.3654 (0.2218)\tPrec@1 89.062 (92.173)\n",
      "Epoch: [42][220/391]\tLoss 0.3122 (0.2208)\tPrec@1 90.625 (92.216)\n",
      "Epoch: [42][275/391]\tLoss 0.3138 (0.2247)\tPrec@1 93.750 (92.097)\n",
      "Epoch: [42][330/391]\tLoss 0.2978 (0.2265)\tPrec@1 90.625 (92.069)\n",
      "Epoch: [42][385/391]\tLoss 0.1421 (0.2286)\tPrec@1 96.094 (92.026)\n",
      "Test\t  Prec@1: 85.820 (Err: 14.180 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.3258 (0.3258)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [43][55/391]\tLoss 0.0756 (0.1976)\tPrec@1 97.656 (93.471)\n",
      "Epoch: [43][110/391]\tLoss 0.1837 (0.2096)\tPrec@1 92.188 (92.842)\n",
      "Epoch: [43][165/391]\tLoss 0.2639 (0.2109)\tPrec@1 90.625 (92.795)\n",
      "Epoch: [43][220/391]\tLoss 0.2359 (0.2159)\tPrec@1 91.406 (92.559)\n",
      "Epoch: [43][275/391]\tLoss 0.2587 (0.2157)\tPrec@1 90.625 (92.499)\n",
      "Epoch: [43][330/391]\tLoss 0.2802 (0.2200)\tPrec@1 89.062 (92.280)\n",
      "Epoch: [43][385/391]\tLoss 0.2868 (0.2217)\tPrec@1 89.844 (92.214)\n",
      "Test\t  Prec@1: 84.820 (Err: 15.180 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.2629 (0.2629)\tPrec@1 88.281 (88.281)\n",
      "Epoch: [44][55/391]\tLoss 0.2643 (0.2002)\tPrec@1 90.625 (92.899)\n",
      "Epoch: [44][110/391]\tLoss 0.2007 (0.2096)\tPrec@1 93.750 (92.666)\n",
      "Epoch: [44][165/391]\tLoss 0.3009 (0.2155)\tPrec@1 89.062 (92.479)\n",
      "Epoch: [44][220/391]\tLoss 0.2442 (0.2167)\tPrec@1 91.406 (92.396)\n",
      "Epoch: [44][275/391]\tLoss 0.2196 (0.2219)\tPrec@1 92.969 (92.244)\n",
      "Epoch: [44][330/391]\tLoss 0.2458 (0.2238)\tPrec@1 90.625 (92.157)\n",
      "Epoch: [44][385/391]\tLoss 0.2634 (0.2254)\tPrec@1 92.188 (92.105)\n",
      "Test\t  Prec@1: 87.090 (Err: 12.910 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.2276 (0.2276)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [45][55/391]\tLoss 0.2505 (0.2090)\tPrec@1 87.500 (92.704)\n",
      "Epoch: [45][110/391]\tLoss 0.1427 (0.2081)\tPrec@1 93.750 (92.582)\n",
      "Epoch: [45][165/391]\tLoss 0.1798 (0.2065)\tPrec@1 94.531 (92.597)\n",
      "Epoch: [45][220/391]\tLoss 0.1994 (0.2111)\tPrec@1 93.750 (92.495)\n",
      "Epoch: [45][275/391]\tLoss 0.2020 (0.2147)\tPrec@1 90.625 (92.422)\n",
      "Epoch: [45][330/391]\tLoss 0.2593 (0.2190)\tPrec@1 89.844 (92.265)\n",
      "Epoch: [45][385/391]\tLoss 0.2941 (0.2204)\tPrec@1 90.625 (92.230)\n",
      "Test\t  Prec@1: 85.070 (Err: 14.930 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2082 (0.2082)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [46][55/391]\tLoss 0.1661 (0.2118)\tPrec@1 94.531 (92.662)\n",
      "Epoch: [46][110/391]\tLoss 0.2239 (0.2105)\tPrec@1 91.406 (92.758)\n",
      "Epoch: [46][165/391]\tLoss 0.1287 (0.2124)\tPrec@1 95.312 (92.611)\n",
      "Epoch: [46][220/391]\tLoss 0.1502 (0.2141)\tPrec@1 94.531 (92.527)\n",
      "Epoch: [46][275/391]\tLoss 0.1484 (0.2172)\tPrec@1 96.875 (92.405)\n",
      "Epoch: [46][330/391]\tLoss 0.2732 (0.2165)\tPrec@1 90.625 (92.400)\n",
      "Epoch: [46][385/391]\tLoss 0.2827 (0.2188)\tPrec@1 91.406 (92.345)\n",
      "Test\t  Prec@1: 85.340 (Err: 14.660 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.1899 (0.1899)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [47][55/391]\tLoss 0.2224 (0.2077)\tPrec@1 92.969 (92.564)\n",
      "Epoch: [47][110/391]\tLoss 0.1932 (0.2094)\tPrec@1 92.188 (92.469)\n",
      "Epoch: [47][165/391]\tLoss 0.1616 (0.2093)\tPrec@1 91.406 (92.503)\n",
      "Epoch: [47][220/391]\tLoss 0.1645 (0.2104)\tPrec@1 94.531 (92.537)\n",
      "Epoch: [47][275/391]\tLoss 0.2292 (0.2139)\tPrec@1 93.750 (92.411)\n",
      "Epoch: [47][330/391]\tLoss 0.2111 (0.2167)\tPrec@1 92.188 (92.357)\n",
      "Epoch: [47][385/391]\tLoss 0.3319 (0.2187)\tPrec@1 89.062 (92.283)\n",
      "Test\t  Prec@1: 86.390 (Err: 13.610 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1079 (0.1079)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [48][55/391]\tLoss 0.2553 (0.1845)\tPrec@1 90.625 (93.401)\n",
      "Epoch: [48][110/391]\tLoss 0.1870 (0.1928)\tPrec@1 92.188 (93.053)\n",
      "Epoch: [48][165/391]\tLoss 0.3447 (0.1994)\tPrec@1 90.625 (92.893)\n",
      "Epoch: [48][220/391]\tLoss 0.3116 (0.2049)\tPrec@1 86.719 (92.735)\n",
      "Epoch: [48][275/391]\tLoss 0.1288 (0.2079)\tPrec@1 95.312 (92.649)\n",
      "Epoch: [48][330/391]\tLoss 0.1765 (0.2122)\tPrec@1 94.531 (92.556)\n",
      "Epoch: [48][385/391]\tLoss 0.1911 (0.2143)\tPrec@1 94.531 (92.536)\n",
      "Test\t  Prec@1: 86.960 (Err: 13.040 )\n",
      "\n",
      "Training resnet44 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2145 (0.2145)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [49][55/391]\tLoss 0.1953 (0.1940)\tPrec@1 91.406 (93.150)\n",
      "Epoch: [49][110/391]\tLoss 0.1936 (0.1972)\tPrec@1 92.969 (93.074)\n",
      "Epoch: [49][165/391]\tLoss 0.1645 (0.2002)\tPrec@1 96.094 (93.030)\n",
      "Epoch: [49][220/391]\tLoss 0.3274 (0.2048)\tPrec@1 89.062 (92.877)\n",
      "Epoch: [49][275/391]\tLoss 0.2865 (0.2108)\tPrec@1 88.281 (92.632)\n",
      "Epoch: [49][330/391]\tLoss 0.1742 (0.2116)\tPrec@1 95.312 (92.565)\n",
      "Epoch: [49][385/391]\tLoss 0.1899 (0.2125)\tPrec@1 94.531 (92.556)\n",
      "Test\t  Prec@1: 84.790 (Err: 15.210 )\n",
      "\n",
      "The lowest error from resnet44 model after 50 epochs is 12.660\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet44';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 32, 32]          432\n",
      "├─BatchNorm2d: 1-2                       [-1, 16, 32, 32]          32\n",
      "├─Sequential: 1-3                        [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-1                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-3                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-4             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-5              [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-2                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-8                  [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-9             [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-10             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-3                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-13                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-15             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-4                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-16                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-18                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-19            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-20             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-5                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-23                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-24            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-25             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-6                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-26                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-27            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-28                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-30             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-7                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-33                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-35             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-8                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-38                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-39            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-40             [-1, 16, 32, 32]          --\n",
      "|    └─BasicBlock: 2-9                   [-1, 16, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-41                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-42            [-1, 16, 32, 32]          32\n",
      "|    |    └─Conv2d: 3-43                 [-1, 16, 32, 32]          2,304\n",
      "|    |    └─BatchNorm2d: 3-44            [-1, 16, 32, 32]          32\n",
      "|    |    └─Sequential: 3-45             [-1, 16, 32, 32]          --\n",
      "├─Sequential: 1-4                        [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-10                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-46                 [-1, 32, 16, 16]          4,608\n",
      "|    |    └─BatchNorm2d: 3-47            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-48                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-49            [-1, 32, 16, 16]          64\n",
      "|    |    └─LambdaLayer: 3-50            [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-11                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-51                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-52            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-53                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-55             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-12                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-56                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-57            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-58                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-59            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-60             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-13                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-61                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-62            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-63                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-64            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-65             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-14                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-66                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-67            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-68                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-69            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-70             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-15                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-71                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-72            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-73                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-74            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-75             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-16                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-76                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-77            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-78                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-79            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-80             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-17                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-81                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-82            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-83                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-84            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-85             [-1, 32, 16, 16]          --\n",
      "|    └─BasicBlock: 2-18                  [-1, 32, 16, 16]          --\n",
      "|    |    └─Conv2d: 3-86                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-87            [-1, 32, 16, 16]          64\n",
      "|    |    └─Conv2d: 3-88                 [-1, 32, 16, 16]          9,216\n",
      "|    |    └─BatchNorm2d: 3-89            [-1, 32, 16, 16]          64\n",
      "|    |    └─Sequential: 3-90             [-1, 32, 16, 16]          --\n",
      "├─Sequential: 1-5                        [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-19                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-91                 [-1, 64, 8, 8]            18,432\n",
      "|    |    └─BatchNorm2d: 3-92            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-93                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-94            [-1, 64, 8, 8]            128\n",
      "|    |    └─LambdaLayer: 3-95            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-20                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-96                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-97            [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-98                 [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-99            [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-100            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-21                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-101                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-102           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-103                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-104           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-105            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-22                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-106                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-107           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-108                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-109           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-110            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-23                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-111                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-112           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-113                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-114           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-115            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-24                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-116                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-117           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-118                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-119           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-120            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-25                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-121                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-122           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-123                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-124           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-125            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-26                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-126                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-127           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-128                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-129           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-130            [-1, 64, 8, 8]            --\n",
      "|    └─BasicBlock: 2-27                  [-1, 64, 8, 8]            --\n",
      "|    |    └─Conv2d: 3-131                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-132           [-1, 64, 8, 8]            128\n",
      "|    |    └─Conv2d: 3-133                [-1, 64, 8, 8]            36,864\n",
      "|    |    └─BatchNorm2d: 3-134           [-1, 64, 8, 8]            128\n",
      "|    |    └─Sequential: 3-135            [-1, 64, 8, 8]            --\n",
      "├─Linear: 1-6                            [-1, 10]                  650\n",
      "==========================================================================================\n",
      "Total params: 853,018\n",
      "Trainable params: 853,018\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 127.19\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.13\n",
      "Params size (MB): 3.25\n",
      "Estimated Total Size (MB): 11.39\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "args=MyResNetArgs('resnet56',pretrained=0)\n",
    "#model = resnet.__dict__[args.arch]()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  globals()[args.arch]().to(device)\n",
    "summary(model, (3,32,32))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/391]\tLoss 6.0584 (6.0584)\tPrec@1 12.500 (12.500)\n",
      "Epoch: [0][55/391]\tLoss 2.2840 (3.3470)\tPrec@1 11.719 (11.691)\n",
      "Epoch: [0][110/391]\tLoss 2.2261 (2.8167)\tPrec@1 10.156 (12.324)\n",
      "Epoch: [0][165/391]\tLoss 2.1303 (2.6186)\tPrec@1 19.531 (13.578)\n",
      "Epoch: [0][220/391]\tLoss 2.0265 (2.4902)\tPrec@1 26.562 (15.367)\n",
      "Epoch: [0][275/391]\tLoss 1.9590 (2.3996)\tPrec@1 20.312 (16.667)\n",
      "Epoch: [0][330/391]\tLoss 1.9950 (2.3280)\tPrec@1 26.562 (18.035)\n",
      "Epoch: [0][385/391]\tLoss 1.9009 (2.2683)\tPrec@1 28.906 (19.379)\n",
      "Test\t  Prec@1: 23.250 (Err: 76.750 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/391]\tLoss 1.9612 (1.9612)\tPrec@1 21.094 (21.094)\n",
      "Epoch: [1][55/391]\tLoss 1.8317 (1.8575)\tPrec@1 28.906 (29.785)\n",
      "Epoch: [1][110/391]\tLoss 1.9071 (1.8169)\tPrec@1 22.656 (31.292)\n",
      "Epoch: [1][165/391]\tLoss 1.8355 (1.7868)\tPrec@1 28.125 (32.380)\n",
      "Epoch: [1][220/391]\tLoss 1.6024 (1.7593)\tPrec@1 45.312 (33.686)\n",
      "Epoch: [1][275/391]\tLoss 1.6463 (1.7296)\tPrec@1 35.938 (34.817)\n",
      "Epoch: [1][330/391]\tLoss 1.4902 (1.6991)\tPrec@1 44.531 (36.013)\n",
      "Epoch: [1][385/391]\tLoss 1.4417 (1.6715)\tPrec@1 45.312 (37.192)\n",
      "Test\t  Prec@1: 40.520 (Err: 59.480 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/391]\tLoss 1.6868 (1.6868)\tPrec@1 41.406 (41.406)\n",
      "Epoch: [2][55/391]\tLoss 1.5263 (1.4867)\tPrec@1 39.062 (45.592)\n",
      "Epoch: [2][110/391]\tLoss 1.3661 (1.4569)\tPrec@1 46.094 (46.699)\n",
      "Epoch: [2][165/391]\tLoss 1.3989 (1.4365)\tPrec@1 50.781 (47.553)\n",
      "Epoch: [2][220/391]\tLoss 1.3215 (1.4153)\tPrec@1 54.688 (48.554)\n",
      "Epoch: [2][275/391]\tLoss 1.1204 (1.3968)\tPrec@1 57.031 (49.114)\n",
      "Epoch: [2][330/391]\tLoss 1.1337 (1.3804)\tPrec@1 64.062 (49.641)\n",
      "Epoch: [2][385/391]\tLoss 1.0638 (1.3625)\tPrec@1 61.719 (50.366)\n",
      "Test\t  Prec@1: 53.860 (Err: 46.140 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/391]\tLoss 1.3143 (1.3143)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [3][55/391]\tLoss 1.0588 (1.2303)\tPrec@1 59.375 (55.608)\n",
      "Epoch: [3][110/391]\tLoss 1.0876 (1.2060)\tPrec@1 60.156 (56.553)\n",
      "Epoch: [3][165/391]\tLoss 1.0867 (1.1900)\tPrec@1 61.719 (57.055)\n",
      "Epoch: [3][220/391]\tLoss 1.1053 (1.1791)\tPrec@1 62.500 (57.600)\n",
      "Epoch: [3][275/391]\tLoss 1.1003 (1.1684)\tPrec@1 62.500 (58.070)\n",
      "Epoch: [3][330/391]\tLoss 1.2116 (1.1513)\tPrec@1 57.812 (58.606)\n",
      "Epoch: [3][385/391]\tLoss 1.1349 (1.1377)\tPrec@1 60.156 (59.177)\n",
      "Test\t  Prec@1: 60.550 (Err: 39.450 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/391]\tLoss 0.9854 (0.9854)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [4][55/391]\tLoss 1.0267 (1.0309)\tPrec@1 64.062 (64.035)\n",
      "Epoch: [4][110/391]\tLoss 0.9296 (1.0093)\tPrec@1 66.406 (64.196)\n",
      "Epoch: [4][165/391]\tLoss 0.8793 (0.9917)\tPrec@1 74.219 (64.966)\n",
      "Epoch: [4][220/391]\tLoss 0.9051 (0.9847)\tPrec@1 71.875 (65.162)\n",
      "Epoch: [4][275/391]\tLoss 0.9972 (0.9742)\tPrec@1 64.844 (65.486)\n",
      "Epoch: [4][330/391]\tLoss 0.8913 (0.9668)\tPrec@1 68.750 (65.760)\n",
      "Epoch: [4][385/391]\tLoss 0.9080 (0.9563)\tPrec@1 64.844 (66.135)\n",
      "Test\t  Prec@1: 65.010 (Err: 34.990 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/391]\tLoss 0.8435 (0.8435)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [5][55/391]\tLoss 0.8574 (0.8770)\tPrec@1 67.969 (69.713)\n",
      "Epoch: [5][110/391]\tLoss 0.9046 (0.8635)\tPrec@1 67.969 (69.658)\n",
      "Epoch: [5][165/391]\tLoss 0.7131 (0.8594)\tPrec@1 72.656 (69.757)\n",
      "Epoch: [5][220/391]\tLoss 0.7081 (0.8538)\tPrec@1 71.875 (69.874)\n",
      "Epoch: [5][275/391]\tLoss 0.9120 (0.8469)\tPrec@1 65.625 (70.194)\n",
      "Epoch: [5][330/391]\tLoss 0.9334 (0.8385)\tPrec@1 63.281 (70.454)\n",
      "Epoch: [5][385/391]\tLoss 0.8366 (0.8299)\tPrec@1 73.438 (70.760)\n",
      "Test\t  Prec@1: 67.870 (Err: 32.130 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/391]\tLoss 0.7236 (0.7236)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [6][55/391]\tLoss 0.6750 (0.7529)\tPrec@1 75.000 (73.507)\n",
      "Epoch: [6][110/391]\tLoss 0.5884 (0.7598)\tPrec@1 81.250 (73.297)\n",
      "Epoch: [6][165/391]\tLoss 0.8053 (0.7620)\tPrec@1 74.219 (73.188)\n",
      "Epoch: [6][220/391]\tLoss 0.7685 (0.7500)\tPrec@1 75.781 (73.533)\n",
      "Epoch: [6][275/391]\tLoss 0.5806 (0.7476)\tPrec@1 77.344 (73.610)\n",
      "Epoch: [6][330/391]\tLoss 0.5991 (0.7448)\tPrec@1 83.594 (73.865)\n",
      "Epoch: [6][385/391]\tLoss 0.6846 (0.7412)\tPrec@1 73.438 (74.026)\n",
      "Test\t  Prec@1: 64.290 (Err: 35.710 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/391]\tLoss 0.6300 (0.6300)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [7][55/391]\tLoss 0.8036 (0.7022)\tPrec@1 71.875 (76.004)\n",
      "Epoch: [7][110/391]\tLoss 0.7042 (0.6889)\tPrec@1 77.344 (76.105)\n",
      "Epoch: [7][165/391]\tLoss 0.7730 (0.6916)\tPrec@1 73.438 (75.941)\n",
      "Epoch: [7][220/391]\tLoss 0.8316 (0.6859)\tPrec@1 69.531 (76.064)\n",
      "Epoch: [7][275/391]\tLoss 0.6445 (0.6804)\tPrec@1 78.125 (76.296)\n",
      "Epoch: [7][330/391]\tLoss 0.5630 (0.6759)\tPrec@1 78.125 (76.527)\n",
      "Epoch: [7][385/391]\tLoss 0.7481 (0.6746)\tPrec@1 75.781 (76.589)\n",
      "Test\t  Prec@1: 77.490 (Err: 22.510 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/391]\tLoss 0.5543 (0.5543)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [8][55/391]\tLoss 0.7613 (0.6075)\tPrec@1 72.656 (79.143)\n",
      "Epoch: [8][110/391]\tLoss 0.6415 (0.6197)\tPrec@1 75.781 (78.780)\n",
      "Epoch: [8][165/391]\tLoss 0.7319 (0.6255)\tPrec@1 72.656 (78.304)\n",
      "Epoch: [8][220/391]\tLoss 0.6064 (0.6275)\tPrec@1 78.906 (78.210)\n",
      "Epoch: [8][275/391]\tLoss 0.7380 (0.6263)\tPrec@1 72.656 (78.303)\n",
      "Epoch: [8][330/391]\tLoss 0.7162 (0.6230)\tPrec@1 78.125 (78.434)\n",
      "Epoch: [8][385/391]\tLoss 0.5745 (0.6245)\tPrec@1 76.562 (78.340)\n",
      "Test\t  Prec@1: 71.620 (Err: 28.380 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/391]\tLoss 0.6026 (0.6026)\tPrec@1 79.688 (79.688)\n",
      "Epoch: [9][55/391]\tLoss 0.5602 (0.5641)\tPrec@1 82.031 (80.539)\n",
      "Epoch: [9][110/391]\tLoss 0.6662 (0.5775)\tPrec@1 78.125 (79.863)\n",
      "Epoch: [9][165/391]\tLoss 0.5159 (0.5808)\tPrec@1 79.688 (79.735)\n",
      "Epoch: [9][220/391]\tLoss 0.6420 (0.5781)\tPrec@1 73.438 (79.829)\n",
      "Epoch: [9][275/391]\tLoss 0.5030 (0.5751)\tPrec@1 80.469 (79.852)\n",
      "Epoch: [9][330/391]\tLoss 0.5091 (0.5747)\tPrec@1 82.812 (79.997)\n",
      "Epoch: [9][385/391]\tLoss 0.7030 (0.5754)\tPrec@1 75.781 (80.092)\n",
      "Test\t  Prec@1: 76.470 (Err: 23.530 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/391]\tLoss 0.4731 (0.4731)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [10][55/391]\tLoss 0.6635 (0.5501)\tPrec@1 79.688 (80.985)\n",
      "Epoch: [10][110/391]\tLoss 0.4972 (0.5476)\tPrec@1 83.594 (80.976)\n",
      "Epoch: [10][165/391]\tLoss 0.5908 (0.5482)\tPrec@1 84.375 (80.991)\n",
      "Epoch: [10][220/391]\tLoss 0.6904 (0.5498)\tPrec@1 71.094 (80.925)\n",
      "Epoch: [10][275/391]\tLoss 0.3710 (0.5462)\tPrec@1 88.281 (81.060)\n",
      "Epoch: [10][330/391]\tLoss 0.6175 (0.5450)\tPrec@1 78.125 (81.137)\n",
      "Epoch: [10][385/391]\tLoss 0.4567 (0.5404)\tPrec@1 85.156 (81.321)\n",
      "Test\t  Prec@1: 79.950 (Err: 20.050 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/391]\tLoss 0.5511 (0.5511)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [11][55/391]\tLoss 0.5131 (0.5041)\tPrec@1 83.594 (82.157)\n",
      "Epoch: [11][110/391]\tLoss 0.5804 (0.5089)\tPrec@1 80.469 (82.066)\n",
      "Epoch: [11][165/391]\tLoss 0.4302 (0.5247)\tPrec@1 87.500 (81.824)\n",
      "Epoch: [11][220/391]\tLoss 0.6112 (0.5213)\tPrec@1 74.219 (81.794)\n",
      "Epoch: [11][275/391]\tLoss 0.5484 (0.5186)\tPrec@1 80.469 (81.819)\n",
      "Epoch: [11][330/391]\tLoss 0.4328 (0.5166)\tPrec@1 86.719 (81.991)\n",
      "Epoch: [11][385/391]\tLoss 0.5221 (0.5140)\tPrec@1 85.938 (82.082)\n",
      "Test\t  Prec@1: 79.440 (Err: 20.560 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/391]\tLoss 0.4461 (0.4461)\tPrec@1 85.156 (85.156)\n",
      "Epoch: [12][55/391]\tLoss 0.6361 (0.5079)\tPrec@1 80.469 (82.254)\n",
      "Epoch: [12][110/391]\tLoss 0.5114 (0.4972)\tPrec@1 79.688 (82.475)\n",
      "Epoch: [12][165/391]\tLoss 0.3792 (0.4957)\tPrec@1 85.938 (82.676)\n",
      "Epoch: [12][220/391]\tLoss 0.4172 (0.4941)\tPrec@1 82.031 (82.820)\n",
      "Epoch: [12][275/391]\tLoss 0.6426 (0.4951)\tPrec@1 78.906 (82.716)\n",
      "Epoch: [12][330/391]\tLoss 0.3890 (0.4961)\tPrec@1 83.594 (82.775)\n",
      "Epoch: [12][385/391]\tLoss 0.4601 (0.4923)\tPrec@1 84.375 (82.889)\n",
      "Test\t  Prec@1: 82.210 (Err: 17.790 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/391]\tLoss 0.3079 (0.3079)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [13][55/391]\tLoss 0.5727 (0.4546)\tPrec@1 79.688 (84.780)\n",
      "Epoch: [13][110/391]\tLoss 0.4999 (0.4676)\tPrec@1 84.375 (84.185)\n",
      "Epoch: [13][165/391]\tLoss 0.5060 (0.4639)\tPrec@1 85.156 (84.191)\n",
      "Epoch: [13][220/391]\tLoss 0.4865 (0.4662)\tPrec@1 82.031 (84.053)\n",
      "Epoch: [13][275/391]\tLoss 0.4619 (0.4712)\tPrec@1 82.031 (83.849)\n",
      "Epoch: [13][330/391]\tLoss 0.5025 (0.4718)\tPrec@1 83.594 (83.771)\n",
      "Epoch: [13][385/391]\tLoss 0.5953 (0.4733)\tPrec@1 78.125 (83.725)\n",
      "Test\t  Prec@1: 82.530 (Err: 17.470 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/391]\tLoss 0.5493 (0.5493)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [14][55/391]\tLoss 0.4233 (0.4417)\tPrec@1 86.719 (85.045)\n",
      "Epoch: [14][110/391]\tLoss 0.3354 (0.4268)\tPrec@1 87.500 (85.480)\n",
      "Epoch: [14][165/391]\tLoss 0.4070 (0.4356)\tPrec@1 84.375 (85.100)\n",
      "Epoch: [14][220/391]\tLoss 0.4248 (0.4340)\tPrec@1 85.938 (85.146)\n",
      "Epoch: [14][275/391]\tLoss 0.4984 (0.4384)\tPrec@1 81.250 (84.998)\n",
      "Epoch: [14][330/391]\tLoss 0.4875 (0.4445)\tPrec@1 82.031 (84.800)\n",
      "Epoch: [14][385/391]\tLoss 0.4465 (0.4408)\tPrec@1 84.375 (84.893)\n",
      "Test\t  Prec@1: 81.230 (Err: 18.770 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/391]\tLoss 0.3812 (0.3812)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [15][55/391]\tLoss 0.3964 (0.4252)\tPrec@1 85.938 (85.714)\n",
      "Epoch: [15][110/391]\tLoss 0.5824 (0.4328)\tPrec@1 80.469 (85.191)\n",
      "Epoch: [15][165/391]\tLoss 0.4125 (0.4323)\tPrec@1 85.938 (85.119)\n",
      "Epoch: [15][220/391]\tLoss 0.4029 (0.4318)\tPrec@1 89.844 (85.061)\n",
      "Epoch: [15][275/391]\tLoss 0.3164 (0.4302)\tPrec@1 90.625 (85.182)\n",
      "Epoch: [15][330/391]\tLoss 0.4015 (0.4315)\tPrec@1 83.594 (85.083)\n",
      "Epoch: [15][385/391]\tLoss 0.5330 (0.4304)\tPrec@1 82.812 (85.124)\n",
      "Test\t  Prec@1: 81.610 (Err: 18.390 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/391]\tLoss 0.4440 (0.4440)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [16][55/391]\tLoss 0.3845 (0.3914)\tPrec@1 85.938 (86.747)\n",
      "Epoch: [16][110/391]\tLoss 0.4873 (0.4040)\tPrec@1 83.594 (86.149)\n",
      "Epoch: [16][165/391]\tLoss 0.3934 (0.4013)\tPrec@1 85.156 (86.215)\n",
      "Epoch: [16][220/391]\tLoss 0.4428 (0.4011)\tPrec@1 84.375 (86.150)\n",
      "Epoch: [16][275/391]\tLoss 0.4166 (0.4059)\tPrec@1 87.500 (85.963)\n",
      "Epoch: [16][330/391]\tLoss 0.3456 (0.4051)\tPrec@1 89.062 (85.994)\n",
      "Epoch: [16][385/391]\tLoss 0.4502 (0.4059)\tPrec@1 85.156 (85.938)\n",
      "Test\t  Prec@1: 82.410 (Err: 17.590 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/391]\tLoss 0.3704 (0.3704)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [17][55/391]\tLoss 0.4392 (0.3792)\tPrec@1 82.812 (86.733)\n",
      "Epoch: [17][110/391]\tLoss 0.5388 (0.3904)\tPrec@1 82.031 (86.494)\n",
      "Epoch: [17][165/391]\tLoss 0.3279 (0.3950)\tPrec@1 88.281 (86.314)\n",
      "Epoch: [17][220/391]\tLoss 0.5338 (0.4032)\tPrec@1 82.031 (86.079)\n",
      "Epoch: [17][275/391]\tLoss 0.4251 (0.4022)\tPrec@1 84.375 (86.136)\n",
      "Epoch: [17][330/391]\tLoss 0.4225 (0.4032)\tPrec@1 85.156 (86.070)\n",
      "Epoch: [17][385/391]\tLoss 0.5422 (0.4083)\tPrec@1 82.031 (85.865)\n",
      "Test\t  Prec@1: 81.330 (Err: 18.670 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/391]\tLoss 0.3136 (0.3136)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [18][55/391]\tLoss 0.4079 (0.3652)\tPrec@1 86.719 (87.040)\n",
      "Epoch: [18][110/391]\tLoss 0.3798 (0.3696)\tPrec@1 88.281 (87.099)\n",
      "Epoch: [18][165/391]\tLoss 0.3331 (0.3761)\tPrec@1 90.625 (86.893)\n",
      "Epoch: [18][220/391]\tLoss 0.4999 (0.3831)\tPrec@1 85.156 (86.669)\n",
      "Epoch: [18][275/391]\tLoss 0.3932 (0.3896)\tPrec@1 88.281 (86.416)\n",
      "Epoch: [18][330/391]\tLoss 0.4164 (0.3894)\tPrec@1 85.156 (86.433)\n",
      "Epoch: [18][385/391]\tLoss 0.4194 (0.3904)\tPrec@1 84.375 (86.413)\n",
      "Test\t  Prec@1: 83.230 (Err: 16.770 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/391]\tLoss 0.3569 (0.3569)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [19][55/391]\tLoss 0.3628 (0.3578)\tPrec@1 87.500 (87.667)\n",
      "Epoch: [19][110/391]\tLoss 0.4142 (0.3813)\tPrec@1 85.938 (86.930)\n",
      "Epoch: [19][165/391]\tLoss 0.3147 (0.3777)\tPrec@1 89.062 (87.067)\n",
      "Epoch: [19][220/391]\tLoss 0.4201 (0.3800)\tPrec@1 85.156 (86.888)\n",
      "Epoch: [19][275/391]\tLoss 0.3146 (0.3794)\tPrec@1 89.062 (86.770)\n",
      "Epoch: [19][330/391]\tLoss 0.3692 (0.3786)\tPrec@1 83.594 (86.792)\n",
      "Epoch: [19][385/391]\tLoss 0.3148 (0.3776)\tPrec@1 89.844 (86.869)\n",
      "Test\t  Prec@1: 81.530 (Err: 18.470 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/391]\tLoss 0.4828 (0.4828)\tPrec@1 83.594 (83.594)\n",
      "Epoch: [20][55/391]\tLoss 0.2384 (0.3666)\tPrec@1 93.750 (87.570)\n",
      "Epoch: [20][110/391]\tLoss 0.2966 (0.3563)\tPrec@1 90.625 (87.669)\n",
      "Epoch: [20][165/391]\tLoss 0.4119 (0.3604)\tPrec@1 85.156 (87.467)\n",
      "Epoch: [20][220/391]\tLoss 0.3879 (0.3632)\tPrec@1 84.375 (87.422)\n",
      "Epoch: [20][275/391]\tLoss 0.3602 (0.3658)\tPrec@1 90.625 (87.291)\n",
      "Epoch: [20][330/391]\tLoss 0.3968 (0.3614)\tPrec@1 86.719 (87.408)\n",
      "Epoch: [20][385/391]\tLoss 0.3521 (0.3629)\tPrec@1 89.062 (87.314)\n",
      "Test\t  Prec@1: 83.480 (Err: 16.520 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/391]\tLoss 0.3119 (0.3119)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [21][55/391]\tLoss 0.3030 (0.3417)\tPrec@1 86.719 (87.974)\n",
      "Epoch: [21][110/391]\tLoss 0.4586 (0.3410)\tPrec@1 84.375 (88.373)\n",
      "Epoch: [21][165/391]\tLoss 0.3401 (0.3397)\tPrec@1 85.938 (88.484)\n",
      "Epoch: [21][220/391]\tLoss 0.3217 (0.3490)\tPrec@1 86.719 (88.080)\n",
      "Epoch: [21][275/391]\tLoss 0.3499 (0.3493)\tPrec@1 87.500 (88.021)\n",
      "Epoch: [21][330/391]\tLoss 0.3841 (0.3531)\tPrec@1 85.156 (87.750)\n",
      "Epoch: [21][385/391]\tLoss 0.3285 (0.3554)\tPrec@1 84.375 (87.696)\n",
      "Test\t  Prec@1: 84.370 (Err: 15.630 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/391]\tLoss 0.4911 (0.4911)\tPrec@1 82.812 (82.812)\n",
      "Epoch: [22][55/391]\tLoss 0.3350 (0.3315)\tPrec@1 87.500 (88.574)\n",
      "Epoch: [22][110/391]\tLoss 0.3429 (0.3466)\tPrec@1 88.281 (88.007)\n",
      "Epoch: [22][165/391]\tLoss 0.3285 (0.3455)\tPrec@1 84.375 (88.027)\n",
      "Epoch: [22][220/391]\tLoss 0.3210 (0.3430)\tPrec@1 84.375 (88.175)\n",
      "Epoch: [22][275/391]\tLoss 0.2877 (0.3456)\tPrec@1 90.625 (88.089)\n",
      "Epoch: [22][330/391]\tLoss 0.2940 (0.3445)\tPrec@1 89.062 (88.130)\n",
      "Epoch: [22][385/391]\tLoss 0.3270 (0.3453)\tPrec@1 91.406 (88.063)\n",
      "Test\t  Prec@1: 84.110 (Err: 15.890 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/391]\tLoss 0.2910 (0.2910)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [23][55/391]\tLoss 0.2453 (0.3148)\tPrec@1 91.406 (88.909)\n",
      "Epoch: [23][110/391]\tLoss 0.3553 (0.3279)\tPrec@1 88.281 (88.535)\n",
      "Epoch: [23][165/391]\tLoss 0.3606 (0.3299)\tPrec@1 89.062 (88.493)\n",
      "Epoch: [23][220/391]\tLoss 0.3355 (0.3335)\tPrec@1 86.719 (88.317)\n",
      "Epoch: [23][275/391]\tLoss 0.3249 (0.3369)\tPrec@1 85.938 (88.210)\n",
      "Epoch: [23][330/391]\tLoss 0.3920 (0.3354)\tPrec@1 85.938 (88.213)\n",
      "Epoch: [23][385/391]\tLoss 0.3609 (0.3353)\tPrec@1 89.062 (88.235)\n",
      "Test\t  Prec@1: 83.520 (Err: 16.480 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/391]\tLoss 0.3274 (0.3274)\tPrec@1 86.719 (86.719)\n",
      "Epoch: [24][55/391]\tLoss 0.3416 (0.3162)\tPrec@1 91.406 (88.714)\n",
      "Epoch: [24][110/391]\tLoss 0.5240 (0.3259)\tPrec@1 80.469 (88.563)\n",
      "Epoch: [24][165/391]\tLoss 0.3695 (0.3328)\tPrec@1 85.156 (88.375)\n",
      "Epoch: [24][220/391]\tLoss 0.2634 (0.3352)\tPrec@1 90.625 (88.292)\n",
      "Epoch: [24][275/391]\tLoss 0.4297 (0.3320)\tPrec@1 85.156 (88.414)\n",
      "Epoch: [24][330/391]\tLoss 0.3863 (0.3323)\tPrec@1 86.719 (88.397)\n",
      "Epoch: [24][385/391]\tLoss 0.4023 (0.3333)\tPrec@1 84.375 (88.356)\n",
      "Test\t  Prec@1: 83.150 (Err: 16.850 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/391]\tLoss 0.2022 (0.2022)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [25][55/391]\tLoss 0.2767 (0.2991)\tPrec@1 91.406 (89.314)\n",
      "Epoch: [25][110/391]\tLoss 0.3873 (0.3045)\tPrec@1 85.938 (89.281)\n",
      "Epoch: [25][165/391]\tLoss 0.4021 (0.3100)\tPrec@1 85.156 (89.157)\n",
      "Epoch: [25][220/391]\tLoss 0.3469 (0.3118)\tPrec@1 87.500 (89.165)\n",
      "Epoch: [25][275/391]\tLoss 0.2640 (0.3146)\tPrec@1 92.188 (89.091)\n",
      "Epoch: [25][330/391]\tLoss 0.3315 (0.3161)\tPrec@1 86.719 (89.041)\n",
      "Epoch: [25][385/391]\tLoss 0.2192 (0.3181)\tPrec@1 91.406 (88.935)\n",
      "Test\t  Prec@1: 83.850 (Err: 16.150 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/391]\tLoss 0.2978 (0.2978)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [26][55/391]\tLoss 0.2602 (0.3088)\tPrec@1 90.625 (89.342)\n",
      "Epoch: [26][110/391]\tLoss 0.2274 (0.3103)\tPrec@1 92.969 (89.231)\n",
      "Epoch: [26][165/391]\tLoss 0.1980 (0.3153)\tPrec@1 93.750 (89.128)\n",
      "Epoch: [26][220/391]\tLoss 0.2593 (0.3172)\tPrec@1 88.281 (89.098)\n",
      "Epoch: [26][275/391]\tLoss 0.1910 (0.3139)\tPrec@1 90.625 (89.196)\n",
      "Epoch: [26][330/391]\tLoss 0.2412 (0.3134)\tPrec@1 92.188 (89.230)\n",
      "Epoch: [26][385/391]\tLoss 0.2703 (0.3148)\tPrec@1 90.625 (89.127)\n",
      "Test\t  Prec@1: 79.790 (Err: 20.210 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/391]\tLoss 0.3580 (0.3580)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [27][55/391]\tLoss 0.2285 (0.2994)\tPrec@1 91.406 (89.523)\n",
      "Epoch: [27][110/391]\tLoss 0.3484 (0.3057)\tPrec@1 88.281 (89.288)\n",
      "Epoch: [27][165/391]\tLoss 0.2730 (0.3058)\tPrec@1 92.188 (89.326)\n",
      "Epoch: [27][220/391]\tLoss 0.2112 (0.3011)\tPrec@1 90.625 (89.483)\n",
      "Epoch: [27][275/391]\tLoss 0.2024 (0.2985)\tPrec@1 89.844 (89.558)\n",
      "Epoch: [27][330/391]\tLoss 0.2742 (0.3041)\tPrec@1 89.844 (89.332)\n",
      "Epoch: [27][385/391]\tLoss 0.4223 (0.3090)\tPrec@1 85.156 (89.208)\n",
      "Test\t  Prec@1: 84.420 (Err: 15.580 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/391]\tLoss 0.3696 (0.3696)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [28][55/391]\tLoss 0.3902 (0.2957)\tPrec@1 86.719 (89.872)\n",
      "Epoch: [28][110/391]\tLoss 0.2573 (0.3001)\tPrec@1 89.844 (89.513)\n",
      "Epoch: [28][165/391]\tLoss 0.2385 (0.2980)\tPrec@1 89.062 (89.674)\n",
      "Epoch: [28][220/391]\tLoss 0.2826 (0.3018)\tPrec@1 88.281 (89.480)\n",
      "Epoch: [28][275/391]\tLoss 0.2968 (0.3019)\tPrec@1 88.281 (89.456)\n",
      "Epoch: [28][330/391]\tLoss 0.3234 (0.3008)\tPrec@1 91.406 (89.516)\n",
      "Epoch: [28][385/391]\tLoss 0.3207 (0.3030)\tPrec@1 87.500 (89.388)\n",
      "Test\t  Prec@1: 85.880 (Err: 14.120 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/391]\tLoss 0.2903 (0.2903)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [29][55/391]\tLoss 0.3043 (0.2586)\tPrec@1 92.188 (91.239)\n",
      "Epoch: [29][110/391]\tLoss 0.3402 (0.2717)\tPrec@1 87.500 (90.632)\n",
      "Epoch: [29][165/391]\tLoss 0.2162 (0.2858)\tPrec@1 92.188 (90.027)\n",
      "Epoch: [29][220/391]\tLoss 0.3920 (0.2893)\tPrec@1 86.719 (89.748)\n",
      "Epoch: [29][275/391]\tLoss 0.2152 (0.2909)\tPrec@1 91.406 (89.821)\n",
      "Epoch: [29][330/391]\tLoss 0.3467 (0.2926)\tPrec@1 86.719 (89.782)\n",
      "Epoch: [29][385/391]\tLoss 0.2695 (0.2944)\tPrec@1 92.188 (89.803)\n",
      "Test\t  Prec@1: 84.650 (Err: 15.350 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/391]\tLoss 0.2862 (0.2862)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [30][55/391]\tLoss 0.2371 (0.2830)\tPrec@1 93.750 (90.011)\n",
      "Epoch: [30][110/391]\tLoss 0.4465 (0.2961)\tPrec@1 83.594 (89.661)\n",
      "Epoch: [30][165/391]\tLoss 0.3302 (0.2918)\tPrec@1 89.844 (89.891)\n",
      "Epoch: [30][220/391]\tLoss 0.2434 (0.2903)\tPrec@1 92.969 (89.964)\n",
      "Epoch: [30][275/391]\tLoss 0.2468 (0.2897)\tPrec@1 89.844 (89.946)\n",
      "Epoch: [30][330/391]\tLoss 0.4015 (0.2941)\tPrec@1 85.156 (89.768)\n",
      "Epoch: [30][385/391]\tLoss 0.2204 (0.2952)\tPrec@1 93.750 (89.741)\n",
      "Test\t  Prec@1: 87.020 (Err: 12.980 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/391]\tLoss 0.2135 (0.2135)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [31][55/391]\tLoss 0.2891 (0.2560)\tPrec@1 92.188 (91.225)\n",
      "Epoch: [31][110/391]\tLoss 0.2366 (0.2631)\tPrec@1 92.969 (90.878)\n",
      "Epoch: [31][165/391]\tLoss 0.2897 (0.2660)\tPrec@1 86.719 (90.705)\n",
      "Epoch: [31][220/391]\tLoss 0.2224 (0.2792)\tPrec@1 92.188 (90.321)\n",
      "Epoch: [31][275/391]\tLoss 0.2420 (0.2784)\tPrec@1 90.625 (90.299)\n",
      "Epoch: [31][330/391]\tLoss 0.2534 (0.2803)\tPrec@1 92.188 (90.252)\n",
      "Epoch: [31][385/391]\tLoss 0.3658 (0.2828)\tPrec@1 89.062 (90.174)\n",
      "Test\t  Prec@1: 83.980 (Err: 16.020 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/391]\tLoss 0.4635 (0.4635)\tPrec@1 82.031 (82.031)\n",
      "Epoch: [32][55/391]\tLoss 0.2637 (0.2718)\tPrec@1 90.625 (90.778)\n",
      "Epoch: [32][110/391]\tLoss 0.1655 (0.2683)\tPrec@1 96.094 (90.878)\n",
      "Epoch: [32][165/391]\tLoss 0.2225 (0.2698)\tPrec@1 94.531 (90.639)\n",
      "Epoch: [32][220/391]\tLoss 0.1996 (0.2743)\tPrec@1 92.969 (90.537)\n",
      "Epoch: [32][275/391]\tLoss 0.2112 (0.2779)\tPrec@1 93.750 (90.331)\n",
      "Epoch: [32][330/391]\tLoss 0.2509 (0.2777)\tPrec@1 91.406 (90.290)\n",
      "Epoch: [32][385/391]\tLoss 0.2203 (0.2796)\tPrec@1 92.188 (90.234)\n",
      "Test\t  Prec@1: 86.680 (Err: 13.320 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/391]\tLoss 0.1484 (0.1484)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [33][55/391]\tLoss 0.2844 (0.2655)\tPrec@1 90.625 (90.932)\n",
      "Epoch: [33][110/391]\tLoss 0.3613 (0.2741)\tPrec@1 90.625 (90.660)\n",
      "Epoch: [33][165/391]\tLoss 0.3556 (0.2766)\tPrec@1 88.281 (90.489)\n",
      "Epoch: [33][220/391]\tLoss 0.2823 (0.2759)\tPrec@1 92.188 (90.466)\n",
      "Epoch: [33][275/391]\tLoss 0.3150 (0.2756)\tPrec@1 89.062 (90.427)\n",
      "Epoch: [33][330/391]\tLoss 0.4234 (0.2786)\tPrec@1 85.938 (90.306)\n",
      "Epoch: [33][385/391]\tLoss 0.2424 (0.2812)\tPrec@1 92.969 (90.289)\n",
      "Test\t  Prec@1: 85.780 (Err: 14.220 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/391]\tLoss 0.2180 (0.2180)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [34][55/391]\tLoss 0.3191 (0.2592)\tPrec@1 86.719 (90.946)\n",
      "Epoch: [34][110/391]\tLoss 0.2200 (0.2568)\tPrec@1 91.406 (91.054)\n",
      "Epoch: [34][165/391]\tLoss 0.1931 (0.2635)\tPrec@1 92.188 (90.705)\n",
      "Epoch: [34][220/391]\tLoss 0.3533 (0.2651)\tPrec@1 88.281 (90.632)\n",
      "Epoch: [34][275/391]\tLoss 0.2180 (0.2674)\tPrec@1 91.406 (90.517)\n",
      "Epoch: [34][330/391]\tLoss 0.2502 (0.2694)\tPrec@1 92.188 (90.453)\n",
      "Epoch: [34][385/391]\tLoss 0.1712 (0.2698)\tPrec@1 94.531 (90.427)\n",
      "Test\t  Prec@1: 86.670 (Err: 13.330 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/391]\tLoss 0.2243 (0.2243)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [35][55/391]\tLoss 0.2538 (0.2641)\tPrec@1 91.406 (91.044)\n",
      "Epoch: [35][110/391]\tLoss 0.2114 (0.2645)\tPrec@1 92.969 (90.857)\n",
      "Epoch: [35][165/391]\tLoss 0.2784 (0.2674)\tPrec@1 89.062 (90.804)\n",
      "Epoch: [35][220/391]\tLoss 0.2669 (0.2691)\tPrec@1 88.281 (90.629)\n",
      "Epoch: [35][275/391]\tLoss 0.2114 (0.2705)\tPrec@1 92.188 (90.568)\n",
      "Epoch: [35][330/391]\tLoss 0.2434 (0.2702)\tPrec@1 94.531 (90.585)\n",
      "Epoch: [35][385/391]\tLoss 0.2676 (0.2672)\tPrec@1 92.188 (90.694)\n",
      "Test\t  Prec@1: 86.730 (Err: 13.270 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/391]\tLoss 0.1755 (0.1755)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [36][55/391]\tLoss 0.4178 (0.2539)\tPrec@1 86.719 (91.030)\n",
      "Epoch: [36][110/391]\tLoss 0.2096 (0.2606)\tPrec@1 91.406 (90.738)\n",
      "Epoch: [36][165/391]\tLoss 0.3331 (0.2630)\tPrec@1 91.406 (90.738)\n",
      "Epoch: [36][220/391]\tLoss 0.3188 (0.2654)\tPrec@1 88.281 (90.696)\n",
      "Epoch: [36][275/391]\tLoss 0.2397 (0.2652)\tPrec@1 92.969 (90.724)\n",
      "Epoch: [36][330/391]\tLoss 0.3260 (0.2676)\tPrec@1 89.062 (90.670)\n",
      "Epoch: [36][385/391]\tLoss 0.3994 (0.2685)\tPrec@1 86.719 (90.694)\n",
      "Test\t  Prec@1: 84.160 (Err: 15.840 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/391]\tLoss 0.3893 (0.3893)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [37][55/391]\tLoss 0.2894 (0.2533)\tPrec@1 88.281 (91.183)\n",
      "Epoch: [37][110/391]\tLoss 0.2683 (0.2601)\tPrec@1 89.844 (90.885)\n",
      "Epoch: [37][165/391]\tLoss 0.4002 (0.2602)\tPrec@1 89.062 (90.846)\n",
      "Epoch: [37][220/391]\tLoss 0.2438 (0.2588)\tPrec@1 90.625 (90.971)\n",
      "Epoch: [37][275/391]\tLoss 0.1663 (0.2608)\tPrec@1 92.969 (90.942)\n",
      "Epoch: [37][330/391]\tLoss 0.2260 (0.2601)\tPrec@1 91.406 (90.984)\n",
      "Epoch: [37][385/391]\tLoss 0.2875 (0.2590)\tPrec@1 90.625 (90.969)\n",
      "Test\t  Prec@1: 85.610 (Err: 14.390 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/391]\tLoss 0.2462 (0.2462)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [38][55/391]\tLoss 0.2495 (0.2347)\tPrec@1 92.188 (91.518)\n",
      "Epoch: [38][110/391]\tLoss 0.3847 (0.2419)\tPrec@1 87.500 (91.265)\n",
      "Epoch: [38][165/391]\tLoss 0.1542 (0.2486)\tPrec@1 92.969 (91.091)\n",
      "Epoch: [38][220/391]\tLoss 0.2115 (0.2488)\tPrec@1 90.625 (91.162)\n",
      "Epoch: [38][275/391]\tLoss 0.3012 (0.2495)\tPrec@1 88.281 (91.118)\n",
      "Epoch: [38][330/391]\tLoss 0.3167 (0.2542)\tPrec@1 87.500 (91.005)\n",
      "Epoch: [38][385/391]\tLoss 0.2836 (0.2566)\tPrec@1 89.844 (90.977)\n",
      "Test\t  Prec@1: 86.080 (Err: 13.920 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/391]\tLoss 0.3494 (0.3494)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [39][55/391]\tLoss 0.2898 (0.2451)\tPrec@1 89.844 (91.588)\n",
      "Epoch: [39][110/391]\tLoss 0.2327 (0.2538)\tPrec@1 92.969 (91.118)\n",
      "Epoch: [39][165/391]\tLoss 0.1876 (0.2516)\tPrec@1 92.188 (91.086)\n",
      "Epoch: [39][220/391]\tLoss 0.1738 (0.2514)\tPrec@1 93.750 (91.127)\n",
      "Epoch: [39][275/391]\tLoss 0.2140 (0.2525)\tPrec@1 91.406 (91.078)\n",
      "Epoch: [39][330/391]\tLoss 0.2352 (0.2544)\tPrec@1 93.750 (91.024)\n",
      "Epoch: [39][385/391]\tLoss 0.2137 (0.2553)\tPrec@1 91.406 (91.044)\n",
      "Test\t  Prec@1: 83.590 (Err: 16.410 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/391]\tLoss 0.2273 (0.2273)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [40][55/391]\tLoss 0.2011 (0.2237)\tPrec@1 92.188 (92.160)\n",
      "Epoch: [40][110/391]\tLoss 0.2609 (0.2372)\tPrec@1 91.406 (91.653)\n",
      "Epoch: [40][165/391]\tLoss 0.1776 (0.2358)\tPrec@1 92.969 (91.693)\n",
      "Epoch: [40][220/391]\tLoss 0.1551 (0.2364)\tPrec@1 94.531 (91.717)\n",
      "Epoch: [40][275/391]\tLoss 0.2891 (0.2420)\tPrec@1 88.281 (91.483)\n",
      "Epoch: [40][330/391]\tLoss 0.3334 (0.2439)\tPrec@1 90.625 (91.465)\n",
      "Epoch: [40][385/391]\tLoss 0.2652 (0.2463)\tPrec@1 90.625 (91.386)\n",
      "Test\t  Prec@1: 86.170 (Err: 13.830 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/391]\tLoss 0.2309 (0.2309)\tPrec@1 91.406 (91.406)\n",
      "Epoch: [41][55/391]\tLoss 0.2012 (0.2188)\tPrec@1 92.969 (92.243)\n",
      "Epoch: [41][110/391]\tLoss 0.2272 (0.2180)\tPrec@1 94.531 (92.413)\n",
      "Epoch: [41][165/391]\tLoss 0.2339 (0.2225)\tPrec@1 92.188 (92.258)\n",
      "Epoch: [41][220/391]\tLoss 0.2057 (0.2314)\tPrec@1 92.969 (91.982)\n",
      "Epoch: [41][275/391]\tLoss 0.3226 (0.2371)\tPrec@1 85.938 (91.766)\n",
      "Epoch: [41][330/391]\tLoss 0.1728 (0.2410)\tPrec@1 92.969 (91.605)\n",
      "Epoch: [41][385/391]\tLoss 0.2766 (0.2420)\tPrec@1 89.844 (91.546)\n",
      "Test\t  Prec@1: 85.270 (Err: 14.730 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/391]\tLoss 0.2486 (0.2486)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [42][55/391]\tLoss 0.2482 (0.2322)\tPrec@1 92.188 (91.602)\n",
      "Epoch: [42][110/391]\tLoss 0.2158 (0.2284)\tPrec@1 92.188 (91.899)\n",
      "Epoch: [42][165/391]\tLoss 0.4241 (0.2376)\tPrec@1 85.938 (91.703)\n",
      "Epoch: [42][220/391]\tLoss 0.1558 (0.2383)\tPrec@1 94.531 (91.565)\n",
      "Epoch: [42][275/391]\tLoss 0.2594 (0.2426)\tPrec@1 90.625 (91.435)\n",
      "Epoch: [42][330/391]\tLoss 0.3396 (0.2465)\tPrec@1 87.500 (91.276)\n",
      "Epoch: [42][385/391]\tLoss 0.2036 (0.2452)\tPrec@1 89.844 (91.301)\n",
      "Test\t  Prec@1: 86.890 (Err: 13.110 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/391]\tLoss 0.1121 (0.1121)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [43][55/391]\tLoss 0.1885 (0.2265)\tPrec@1 92.969 (92.174)\n",
      "Epoch: [43][110/391]\tLoss 0.1696 (0.2214)\tPrec@1 95.312 (92.603)\n",
      "Epoch: [43][165/391]\tLoss 0.3336 (0.2284)\tPrec@1 90.625 (92.211)\n",
      "Epoch: [43][220/391]\tLoss 0.2446 (0.2280)\tPrec@1 89.844 (92.159)\n",
      "Epoch: [43][275/391]\tLoss 0.1316 (0.2323)\tPrec@1 96.094 (91.984)\n",
      "Epoch: [43][330/391]\tLoss 0.1694 (0.2330)\tPrec@1 94.531 (91.902)\n",
      "Epoch: [43][385/391]\tLoss 0.4356 (0.2378)\tPrec@1 82.031 (91.696)\n",
      "Test\t  Prec@1: 84.750 (Err: 15.250 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/391]\tLoss 0.3332 (0.3332)\tPrec@1 89.844 (89.844)\n",
      "Epoch: [44][55/391]\tLoss 0.1871 (0.2280)\tPrec@1 93.750 (91.922)\n",
      "Epoch: [44][110/391]\tLoss 0.2235 (0.2315)\tPrec@1 92.188 (92.012)\n",
      "Epoch: [44][165/391]\tLoss 0.3569 (0.2333)\tPrec@1 88.281 (91.924)\n",
      "Epoch: [44][220/391]\tLoss 0.2221 (0.2332)\tPrec@1 90.625 (91.876)\n",
      "Epoch: [44][275/391]\tLoss 0.1703 (0.2370)\tPrec@1 92.969 (91.715)\n",
      "Epoch: [44][330/391]\tLoss 0.2096 (0.2397)\tPrec@1 92.188 (91.614)\n",
      "Epoch: [44][385/391]\tLoss 0.2217 (0.2413)\tPrec@1 95.312 (91.611)\n",
      "Test\t  Prec@1: 84.330 (Err: 15.670 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/391]\tLoss 0.1467 (0.1467)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [45][55/391]\tLoss 0.1628 (0.2116)\tPrec@1 92.969 (92.425)\n",
      "Epoch: [45][110/391]\tLoss 0.3321 (0.2246)\tPrec@1 87.500 (92.152)\n",
      "Epoch: [45][165/391]\tLoss 0.2332 (0.2274)\tPrec@1 93.750 (92.060)\n",
      "Epoch: [45][220/391]\tLoss 0.2949 (0.2310)\tPrec@1 91.406 (91.905)\n",
      "Epoch: [45][275/391]\tLoss 0.2046 (0.2336)\tPrec@1 93.750 (91.890)\n",
      "Epoch: [45][330/391]\tLoss 0.1743 (0.2358)\tPrec@1 94.531 (91.784)\n",
      "Epoch: [45][385/391]\tLoss 0.2780 (0.2368)\tPrec@1 90.625 (91.728)\n",
      "Test\t  Prec@1: 83.100 (Err: 16.900 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/391]\tLoss 0.2347 (0.2347)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [46][55/391]\tLoss 0.1536 (0.2107)\tPrec@1 94.531 (92.662)\n",
      "Epoch: [46][110/391]\tLoss 0.2311 (0.2134)\tPrec@1 92.969 (92.645)\n",
      "Epoch: [46][165/391]\tLoss 0.2474 (0.2121)\tPrec@1 90.625 (92.540)\n",
      "Epoch: [46][220/391]\tLoss 0.2484 (0.2218)\tPrec@1 92.188 (92.226)\n",
      "Epoch: [46][275/391]\tLoss 0.2719 (0.2254)\tPrec@1 87.500 (92.094)\n",
      "Epoch: [46][330/391]\tLoss 0.2917 (0.2258)\tPrec@1 90.625 (92.065)\n",
      "Epoch: [46][385/391]\tLoss 0.2317 (0.2296)\tPrec@1 92.969 (91.987)\n",
      "Test\t  Prec@1: 85.970 (Err: 14.030 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/391]\tLoss 0.2296 (0.2296)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [47][55/391]\tLoss 0.2158 (0.2101)\tPrec@1 91.406 (92.550)\n",
      "Epoch: [47][110/391]\tLoss 0.1970 (0.2200)\tPrec@1 92.969 (92.300)\n",
      "Epoch: [47][165/391]\tLoss 0.2626 (0.2305)\tPrec@1 88.281 (91.919)\n",
      "Epoch: [47][220/391]\tLoss 0.2046 (0.2293)\tPrec@1 92.969 (91.997)\n",
      "Epoch: [47][275/391]\tLoss 0.2391 (0.2321)\tPrec@1 91.406 (91.859)\n",
      "Epoch: [47][330/391]\tLoss 0.2430 (0.2307)\tPrec@1 94.531 (91.907)\n",
      "Epoch: [47][385/391]\tLoss 0.2272 (0.2318)\tPrec@1 89.844 (91.845)\n",
      "Test\t  Prec@1: 83.710 (Err: 16.290 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/391]\tLoss 0.1050 (0.1050)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [48][55/391]\tLoss 0.1379 (0.2183)\tPrec@1 95.312 (92.201)\n",
      "Epoch: [48][110/391]\tLoss 0.3210 (0.2224)\tPrec@1 89.844 (92.202)\n",
      "Epoch: [48][165/391]\tLoss 0.1869 (0.2189)\tPrec@1 92.188 (92.249)\n",
      "Epoch: [48][220/391]\tLoss 0.2612 (0.2220)\tPrec@1 90.625 (92.149)\n",
      "Epoch: [48][275/391]\tLoss 0.1911 (0.2255)\tPrec@1 93.750 (92.052)\n",
      "Epoch: [48][330/391]\tLoss 0.1849 (0.2260)\tPrec@1 93.750 (92.003)\n",
      "Epoch: [48][385/391]\tLoss 0.2088 (0.2303)\tPrec@1 89.844 (91.866)\n",
      "Test\t  Prec@1: 85.110 (Err: 14.890 )\n",
      "\n",
      "Training resnet56 model\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/391]\tLoss 0.2315 (0.2315)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [49][55/391]\tLoss 0.1877 (0.2150)\tPrec@1 93.750 (92.759)\n",
      "Epoch: [49][110/391]\tLoss 0.2490 (0.2166)\tPrec@1 90.625 (92.511)\n",
      "Epoch: [49][165/391]\tLoss 0.1914 (0.2152)\tPrec@1 92.969 (92.536)\n",
      "Epoch: [49][220/391]\tLoss 0.2514 (0.2231)\tPrec@1 90.625 (92.276)\n",
      "Epoch: [49][275/391]\tLoss 0.2017 (0.2233)\tPrec@1 95.312 (92.278)\n",
      "Epoch: [49][330/391]\tLoss 0.2225 (0.2258)\tPrec@1 90.625 (92.128)\n",
      "Epoch: [49][385/391]\tLoss 0.1472 (0.2270)\tPrec@1 93.750 (92.058)\n",
      "Test\t  Prec@1: 86.720 (Err: 13.280 )\n",
      "\n",
      "The lowest error from resnet44 model after 50 epochs is 12.980\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    best_prec1 = main()\n",
    "    args.arch = 'resnet56';\n",
    "    print('The lowest error from {} model after {} epochs is {error:.3f}'.format( args.arch,args.epochs,error=100-best_prec1)) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ba9ca613c00912bf2bb7336c6f7b766b0be232b7fbb6881178983a86316f18c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
